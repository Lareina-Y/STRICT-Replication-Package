M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.scoring.ScoringFilters:passScoreAfterParsing(org.apache.hadoop.io.Text,org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:<init>(java.lang.String,int) (O)java.lang.Object:<init>()
M:org.apache.nutch.indexer.NutchField:add(java.lang.Object) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.crawl.FetchSchedule:calculateLastFetchTime(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.nutch.scoring.webgraph.LinkRank:getConf()
M:org.apache.nutch.scoring.webgraph.Node:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.Server:addConnector(org.mortbay.jetty.Connector)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)org.apache.nutch.crawl.LinkDb:getConf()
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.metadata.Nutch:<clinit>() (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (I)java.util.ListIterator:next()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (M)java.util.ArrayList:size()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapFile$Writer:close()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.parse.ParseText:<init>(java.lang.String)
M:org.apache.nutch.util.ObjectCache:getObject(java.lang.String) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.Node:toString() (M)org.apache.nutch.metadata.Metadata:toString()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParsePluginList:getPluginList(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDb:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.metadata.SpellCheckedMetadata:get(java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.parse.ParseResult:filter() (I)java.util.Iterator:remove()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (S)org.apache.nutch.fetcher.OldFetcher:access$400(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (S)com.google.common.io.Files:toByteArray(java.io.File)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (O)org.apache.nutch.crawl.MapWritable:addIdEntry(byte,java.lang.Class)
M:org.apache.nutch.tools.proxy.FakeHandler:<init>() (O)java.util.Random:<init>(long)
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.util.TreeMap:remove(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)java.util.regex.Pattern:compile(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)org.apache.hadoop.util.GenericOptionsParser:getRemainingArgs()
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$602(org.apache.nutch.crawl.MapWritable$KeyValueEntry,byte)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:setSignature(byte[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)org.apache.nutch.crawl.Inlink:getFromUrl()
M:org.apache.nutch.crawl.CrawlDbReducer:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.crawl.InlinkPriorityQueue:<init>(int)
M:org.apache.nutch.net.protocols.HttpDateFormat:toString(java.util.Calendar) (M)java.text.SimpleDateFormat:format(java.util.Date)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItemQueue(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.nutch.parse.ParseData:getContentMeta()
M:org.apache.nutch.indexer.IndexerMapReduce:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:setScore(float)
M:org.apache.nutch.tools.ResolveUrls:<init>(java.lang.String) (O)org.apache.nutch.tools.ResolveUrls:<init>(java.lang.String,int)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (M)java.lang.Class:getName()
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.metadata.Metadata:clear() (I)java.util.Map:clear()
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.net.URL:getPath()
M:org.apache.nutch.protocol.Content:toString() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.nutch.tools.arc.ArcSegmentCreator:getConf()
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkDatum:write(java.io.DataOutput) (I)java.io.DataOutput:writeLong(long)
M:org.apache.nutch.crawl.MapWritable:containsValue(org.apache.hadoop.io.Writable) (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (S)java.lang.reflect.Array:newInstance(java.lang.Class,int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:main(java.lang.String[]) (O)org.apache.nutch.tools.arc.ArcSegmentCreator:<init>()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.apache.nutch.tools.proxy.SegmentHandler:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:addFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.NutchDocument:add(java.lang.String,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getNumInlinks()
M:org.apache.nutch.segment.SegmentReader:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.segment.SegmentReader:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$Selector:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.indexer.IndexWriters:describe() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.util.Iterator:next()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.util.GZIPUtils:unzip(byte[]) (O)java.util.zip.GZIPInputStream:<init>(java.io.InputStream)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:reduce(org.apache.hadoop.io.ByteWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:incrCounter(java.lang.String,java.lang.String,long)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (O)org.apache.nutch.segment.SegmentReader$1:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader)
M:org.apache.nutch.util.URLUtil:<clinit>() (S)java.util.regex.Pattern:compile(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)java.util.Iterator:next()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.lang.StringBuilder:append(char)
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.FreeGenerator:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (M)java.io.BufferedReader:close()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.io.PrintWriter:<init>(java.io.Writer)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.HashSet:<init>()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (O)org.apache.nutch.parse.ParserFactory:getExtension(org.apache.nutch.plugin.Extension[],java.lang.String,java.lang.String)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (O)java.io.ByteArrayOutputStream:<init>()
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.ScoringFilters:injectedScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (I)org.apache.nutch.scoring.ScoringFilter:injectedScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.LinkDb:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setCombinerClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setNumReduceTasks(int)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (M)java.text.SimpleDateFormat:format(java.util.Date)
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:getModifiedTime()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int) (M)java.io.ByteArrayOutputStream:write(byte[],int,int)
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (O)java.util.Date:<init>(long)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.segment.SegmentPart:toString()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.util.URLUtil:getPage(java.lang.String) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.tools.Benchmark:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (O)org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setPartitionerClass(java.lang.Class)
M:org.apache.nutch.util.CommandRunner:exec() (M)java.lang.Process:exitValue()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader$1:run() (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (O)org.apache.nutch.parse.ParseCallable:<init>(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)java.util.ArrayList:size()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule$AdaptiveRate:<init>(org.apache.nutch.crawl.MimeAdaptiveFetchSchedule,java.lang.Float,java.lang.Float) (M)java.lang.Float:floatValue()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.MimeUtil:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.arc.ArcRecordReader:createValue() (M)org.apache.nutch.tools.arc.ArcRecordReader:createValue()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.tools.arc.ArcSegmentCreator:getConf()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (S)org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.Inlinks:<init>() (O)java.util.HashSet:<init>(int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:configure(org.apache.hadoop.mapred.JobConf) (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.fetcher.Fetcher:updateStatus(int) (M)java.util.concurrent.atomic.AtomicLong:addAndGet(long)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.lang.Class:getClassLoader()
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (M)org.apache.nutch.util.CommandRunner$PumperThread:isInterrupted()
M:org.apache.nutch.segment.SegmentReader$3:run() (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.util.FSUtils:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.fetcher.OldFetcher:getConf()
M:org.apache.nutch.scoring.webgraph.LinkDatum:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:getFetchTime(org.apache.nutch.parse.ParseData) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(int)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.parse.ParseSegment:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtensionPoints(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (O)org.apache.nutch.plugin.ExtensionPoint:<init>(java.lang.String,java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:setInlinkScore(float)
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (M)java.lang.String:getBytes()
M:org.apache.nutch.tools.arc.ArcRecordReader:createKey() (M)org.apache.nutch.tools.arc.ArcRecordReader:createKey()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getPluginDescriptor(java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (S)java.util.concurrent.Executors:newFixedThreadPool(int)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseResult:filter() (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)org.apache.nutch.crawl.CrawlDatum:getModifiedTime()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.parse.ParsePluginList:setAliases(java.util.Map)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.util.MimeUtil:forName(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.crawl.Generator$DecreasingFloatComparator:compare(byte[],int,int,byte[],int,int) (O)org.apache.hadoop.io.FloatWritable$Comparator:compare(byte[],int,int,byte[],int,int)
M:org.apache.nutch.util.URLUtil:getHost(java.lang.String) (M)java.net.URL:getHost()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getQueueSize()
M:org.apache.nutch.fetcher.OldFetcher:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.fetcher.OldFetcher:isParsing(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map) (O)java.util.HashMap:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$200(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.Generator$PartitionReducer:<init>() (O)org.apache.hadoop.mapred.MapReduceBase:<init>()
M:org.apache.nutch.crawl.MapWritable:putAll(org.apache.nutch.crawl.MapWritable) (M)org.apache.nutch.crawl.MapWritable:keySet()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (I)org.apache.commons.logging.Log:warn(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.Extension:getId()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.parse.ParseResult:createParseResult(java.lang.String,org.apache.nutch.parse.Parse) (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.nutch.scoring.webgraph.Loops:getConf()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (O)javax.xml.transform.stream.StreamResult:<init>(java.io.OutputStream)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.fs.Path:getName()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.segment.SegmentReader:<init>(org.apache.hadoop.conf.Configuration,boolean,boolean,boolean,boolean,boolean,boolean) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.DeduplicationJob:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.Inlinks:getAnchors() (O)java.util.HashSet:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.protocol.Content:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.String:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.nutch.parse.ParseSegment:getConf()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.util.GZIPUtils:unzip(byte[]) (M)java.io.ByteArrayOutputStream:close()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.indexer.IndexerMapReduce:filterUrl(java.lang.String) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.metadata.MetaWrapper:getMetaValues(java.lang.String) (M)org.apache.nutch.metadata.Metadata:getValues(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.net.URL:getHost()
M:org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.parse.ParseStatus:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.util.URLUtil:main(java.lang.String[]) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setScore(float)
M:org.apache.nutch.util.StringUtil:fromHexString(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.util.PrefixStringMatcher:matches(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.util.HashSet:add(java.lang.Object)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (M)org.apache.nutch.util.GenericWritableConfigurable:getTypes()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:next(java.lang.Object,java.lang.Object) (M)org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.Generator$GeneratorOutputFormat:generateFileNameForKeyValue(org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator$SelectorEntry,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.ObjectWritable:<init>()
M:org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String) (M)java.lang.String:indexOf(java.lang.String)
M:org.apache.nutch.crawl.LinkDbFilter:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.protocol.ProtocolStatus:read(java.io.DataInput) (M)org.apache.nutch.protocol.ProtocolStatus:readFields(java.io.DataInput)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:error(org.xml.sax.SAXParseException) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)java.util.HashSet:size()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (M)org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseSegment:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.net.URLFilterException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:toString() (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.crawl.Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$100(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.util.TimingUtil:elapsedTime(long,long) (S)java.text.NumberFormat:getInstance()
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (M)java.lang.Object:getClass()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.crawl.MapWritable$KeyValueEntry:equals(java.lang.Object) (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginManifestParser:parseManifestFile(java.lang.String) (O)org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (M)java.io.OutputStream:flush()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.Outlink:getAnchor()
M:org.apache.nutch.crawl.URLPartitioner:configure(org.apache.hadoop.mapred.JobConf) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[]) (S)org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.LongWritable:<init>(long)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getTotalSize() (M)java.util.concurrent.atomic.AtomicInteger:get()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (M)java.lang.String:trim()
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:write(java.io.DataOutput) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem() (I)java.util.Set:size()
M:org.apache.nutch.metadata.MetaWrapper:readFields(java.io.DataInput) (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.util.GZIPUtils:unzip(byte[]) (M)java.io.ByteArrayOutputStream:toByteArray()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.parse.Parse:isCanonical()
M:org.apache.nutch.protocol.Content:<init>() (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (O)org.apache.nutch.crawl.AdaptiveFetchSchedule:<init>()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$LoopSet:getLoopSet()
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.fetcher.FetcherOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (O)java.io.InputStreamReader:<init>(java.io.InputStream)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (S)org.apache.nutch.parse.Outlink:read(java.io.DataInput)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.scoring.ScoringFilterException:getMessage()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)java.util.List:size()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)java.lang.String:endsWith(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (I)java.util.Map:values()
M:org.apache.nutch.crawl.LinkDb:getHost(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.fetcher.OldFetcher:access$900(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:toString() (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.NutchDocument:setWeight(float)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator$HashComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable) (M)org.apache.hadoop.io.Text:getBytes()
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)java.lang.String:length()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator$SelectorEntry:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getModifiedTime()
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (M)org.apache.nutch.parse.ParserNotFound:getMessage()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.FloatWritable:<init>(float)
M:org.apache.nutch.util.MimeUtil:getMimeType(java.io.File) (M)org.apache.tika.Tika:detect(java.io.File)
M:org.apache.nutch.metadata.MetaWrapper:setMeta(java.lang.String,java.lang.String) (M)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexerOutputFormat$1:write(java.lang.Object,java.lang.Object) (M)org.apache.nutch.indexer.IndexerOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.indexer.NutchIndexAction)
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (O)org.apache.nutch.net.URLNormalizerChecker:checkAll(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)java.util.HashSet:add(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (M)java.lang.Object:toString()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getCrawlDatum(org.apache.hadoop.io.Text) (O)org.apache.nutch.tools.proxy.SegmentHandler$Segment:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (I)java.util.List:size()
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (I)java.io.DataOutput:writeByte(int)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$500(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.parse.ParseStatus:<init>(java.lang.Throwable) (O)org.apache.nutch.parse.ParseStatus:<init>(int,int,java.lang.String[])
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$202(org.apache.nutch.fetcher.OldFetcher,long)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:substring(int)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.protocol.ProtocolStatus:<init>(int) (O)org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.String[])
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilters:updateDbScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,java.util.List)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)org.apache.hadoop.io.MD5Hash:digest(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:main(java.lang.String[]) (O)org.apache.nutch.crawl.LinkDb:<init>()
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (O)java.util.StringTokenizer:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (O)org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:clone() (O)java.lang.RuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (I)java.util.List:toArray(java.lang.Object[])
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:emptyQueue()
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String,java.lang.String)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (S)java.util.concurrent.Executors:newCachedThreadPool(java.util.concurrent.ThreadFactory)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.plugin.PluginRuntimeException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (I)java.util.List:toArray(java.lang.Object[])
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.InlinkPriorityQueue:lessThan(java.lang.Object,java.lang.Object) (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.segment.SegmentPart:get(org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.mapred.FileSplit:getPath()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:setHandled(boolean)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)
M:org.apache.nutch.indexer.IndexerMapReduce:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.indexer.IndexerMapReduce:filterUrl(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseData:setOutlinks(org.apache.nutch.parse.Outlink[])
M:org.apache.nutch.util.PrefixStringMatcher:<init>(java.util.Collection) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReducer:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.LinkDbReader:getAnchors(org.apache.hadoop.io.Text) (M)org.apache.nutch.crawl.Inlinks:getAnchors()
M:org.apache.nutch.indexer.IndexerOutputFormat:<init>() (O)org.apache.hadoop.mapred.FileOutputFormat:<init>()
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseResult:filter() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setOutputKeyComparatorClass(java.lang.Class)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.parse.Outlink:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectReducer:configure(org.apache.hadoop.mapred.JobConf) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.scoring.ScoringFilters:initialScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (S)javax.xml.transform.TransformerFactory:newInstance()
M:org.apache.nutch.parse.ParseData:<init>(org.apache.nutch.parse.ParseStatus,java.lang.String,org.apache.nutch.parse.Outlink[],org.apache.nutch.metadata.Metadata) (O)org.apache.nutch.parse.ParseData:<init>(org.apache.nutch.parse.ParseStatus,java.lang.String,org.apache.nutch.parse.Outlink[],org.apache.nutch.metadata.Metadata,org.apache.nutch.metadata.Metadata)
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (O)org.apache.nutch.segment.SegmentMerger:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.parse.ParsePluginsReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (M)org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$000(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (O)org.apache.nutch.crawl.MapWritable:findEntryByKey(org.apache.hadoop.io.Writable)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (O)org.apache.nutch.parse.ParseOutputFormat:<init>()
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.parse.ParseStatus:getArgs()
M:org.apache.nutch.plugin.PluginRepository:<clinit>() (O)java.util.WeakHashMap:<init>()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:toString() (M)java.lang.StringBuilder:substring(int,int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.protocol.Content:getContent()
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$700(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.parse.Outlink:toString() (M)org.apache.hadoop.io.MapWritable:isEmpty()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.io.SequenceFile$Reader:close()
M:org.apache.nutch.parse.ParseImpl:write(java.io.DataOutput) (M)org.apache.nutch.parse.ParseText:write(java.io.DataOutput)
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (I)java.util.Set:iterator()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexingFilter:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:setStatus(int)
M:org.apache.nutch.crawl.Generator$PartitionReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getNumReduceTasks()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (O)org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.crawl.Injector$InjectMapper:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.util.MimeUtil:cleanMimeType(java.lang.String) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (O)java.lang.Float:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:toString() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.CrawlDb:main(java.lang.String[]) (O)org.apache.nutch.crawl.CrawlDb:<init>()
M:org.apache.nutch.parse.ParseStatus:getEmptyParseResult(java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.parse.ParseStatus:getEmptyParse(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (S)java.util.Collections:emptyList()
M:org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.segment.SegmentPart:<init>(java.lang.String,java.lang.String) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (S)java.lang.Math:round(double)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.scoring.ScoringFilters:passScoreBeforeParsing(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:addExtensionPoint(org.apache.nutch.plugin.ExtensionPoint) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.util.DeflateUtils:deflate(byte[]) (O)java.io.ByteArrayOutputStream:<init>(int)
M:org.apache.nutch.parse.HtmlParseFilter:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.Exception:getMessage()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)org.apache.nutch.crawl.LinkDbReader:getConf()
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (O)java.lang.StringBuffer:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.util.TrieStringMatcher:addPatternForward(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean)
M:org.apache.nutch.util.PrefixStringMatcher:matches(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:isTerminal()
M:org.apache.nutch.crawl.URLPartitioner:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.Parse) (O)org.apache.nutch.parse.ParseText:<init>(java.lang.String)
M:org.apache.nutch.plugin.Extension:<init>(org.apache.nutch.plugin.PluginDescriptor,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository) (O)java.util.HashMap:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.util.EncodingDetector:clearClues() (I)java.util.List:clear()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput) (I)java.io.DataOutput:writeBoolean(boolean)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.net.URLNormalizerChecker:checkAll(java.lang.String) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.util.MimeUtil:getMimeType(java.io.File) (M)java.lang.Exception:getMessage()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.metadata.MetaWrapper:<init>(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.parse.ParseStatus:<init>()
M:org.apache.nutch.crawl.CrawlDatum:<clinit>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.crawl.LinkDb:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.crawl.LinkDb:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable)
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (S)org.apache.nutch.util.CommandRunner:access$002(org.apache.nutch.util.CommandRunner,java.lang.Throwable)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.SequenceFile$Writer:close()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.util.CommandRunner:exec() (M)org.apache.nutch.util.CommandRunner$PullerThread:interrupt()
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (M)org.apache.nutch.plugin.PluginDescriptor:getDependencies()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.util.SuffixStringMatcher:shortestMatch(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.util.URLUtil:fixEmbeddedParams(java.net.URL,java.lang.String) (M)java.lang.String:indexOf(int)
M:org.apache.nutch.crawl.Generator$SelectorEntry:write(java.io.DataOutput) (M)org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (O)java.io.InputStreamReader:<init>(java.io.InputStream)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.nutch.indexer.IndexWriters:describe()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.indexer.NutchIndexAction:write(java.io.DataOutput) (M)org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput)
M:org.apache.nutch.crawl.TextProfileSignature$Token:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getReaders(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbReader:get(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.Generator$DecreasingFloatComparator:<init>() (O)org.apache.hadoop.io.FloatWritable$Comparator:<init>()
M:org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum) (I)java.util.Set:iterator()
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.util.URLUtil:getPage(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.crawl.Generator$SelectorEntry:readFields(java.io.DataInput) (M)org.apache.hadoop.io.IntWritable:readFields(java.io.DataInput)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinksJDK5Impl(java.lang.String) (O)java.lang.UnsupportedOperationException:<init>(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int)
M:org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:<init>() (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:writeOutAsDuplicate(org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:incrCounter(java.lang.String,java.lang.String,long)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getInlinkScore()
M:org.apache.nutch.parse.ParseImpl:read(java.io.DataInput) (O)org.apache.nutch.parse.ParseImpl:<init>()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.FreeGenerator$FG:<init>() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)org.apache.nutch.protocol.Content:getContentType()
M:org.apache.nutch.util.CommandRunner:exec() (M)org.apache.nutch.util.CommandRunner$PullerThread:start()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:warning(org.xml.sax.SAXParseException) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.NodeWalker:nextNode() (I)org.w3c.dom.Node:getChildNodes()
M:org.apache.nutch.parse.ParseOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.SequenceFile$Writer:close()
M:org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String) (M)java.io.File:toURI()
M:org.apache.nutch.util.StringUtil:toHexString(byte[],java.lang.String,int) (M)java.lang.StringBuffer:append(char)
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:setInlinkScore(float)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL) (M)java.lang.String:substring(int)
M:org.apache.nutch.util.DomUtil:getDom(java.io.InputStream) (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (S)org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String)
M:org.apache.nutch.util.StringUtil:fromHexString(java.lang.String) (S)org.apache.nutch.util.StringUtil:charToNibble(char)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.net.URL:getHost()
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (O)org.apache.nutch.parse.ParseData:<init>()
M:org.apache.nutch.crawl.AbstractFetchSchedule:setPageGoneSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(float)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL) (M)org.apache.nutch.util.domain.DomainSuffixes:get(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)java.lang.Object:toString()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getUrl()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.scoring.ScoringFilters:passScoreAfterParsing(org.apache.hadoop.io.Text,org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:<init>(org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode[]) (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.Outlink:read(java.io.DataInput) (O)org.apache.nutch.parse.Outlink:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.scoring.ScoringFilters:passScoreAfterParsing(org.apache.hadoop.io.Text,org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.parse.ParseSegment:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:createValue() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getPage(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.mapred.SequenceFileRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)
M:org.apache.nutch.scoring.webgraph.NodeDumper:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.NodeDumper:<init>()
M:org.apache.nutch.segment.SegmentReader:<init>() (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.apache.nutch.protocol.Protocol:getProtocolOutput(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.URLPartitioner:configure(org.apache.hadoop.mapred.JobConf) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParseStatus:<clinit>() (O)org.apache.nutch.parse.ParseStatus:<init>(int)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)org.apache.hadoop.mapred.JobConf:setInt(java.lang.String,int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (O)org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String)
M:org.apache.nutch.parse.ParseStatus:<init>(int,java.lang.String[]) (O)org.apache.nutch.parse.ParseStatus:<init>(int,int,java.lang.String[])
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.Character:isLetter(char)
M:org.apache.nutch.crawl.LinkDbMerger:main(java.lang.String[]) (O)org.apache.nutch.crawl.LinkDbMerger:<init>()
M:org.apache.nutch.fetcher.Fetcher$InputFormat:<init>() (O)org.apache.hadoop.mapred.SequenceFileInputFormat:<init>()
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (M)java.util.Properties:entrySet()
M:org.apache.nutch.fetcher.OldFetcher:isStoringContent(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItem:getQueueID()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:next(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.nutch.metadata.MetaWrapper:setMeta(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.util.HashSet:add(java.lang.Object)
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:setHandled(boolean)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.LongWritable:<init>(long)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:normalizeUrl(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.IndexerOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.indexer.NutchIndexAction) (M)org.apache.nutch.indexer.IndexWriters:delete(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)java.net.URL:getHost()
M:org.apache.nutch.tools.proxy.LogDebugHandler:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain) (I)javax.servlet.http.HttpServletResponse:sendError(int,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Route:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.segment.SegmentReader$5:run() (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.util.SuffixStringMatcher:longestMatch(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)org.apache.nutch.util.CommandRunner:evaluate()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getTotalSize()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.crawl.CrawlDbReader:<init>()
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.NodeReader:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.TreeMap:<init>()
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.scoring.ScoringFilters:initialScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)java.lang.Throwable:toString()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (I)org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (O)org.apache.nutch.plugin.PluginClassLoader:<init>(java.net.URL[],java.lang.ClassLoader)
M:org.apache.nutch.parse.ParseUtil:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.lang.Throwable:toString()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:writeOutAsDuplicate(org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.DeduplicationJob:access$000()
M:org.apache.nutch.parse.ParseException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.io.File:getParentFile()
M:org.apache.nutch.tools.proxy.SegmentHandler:<clinit>() (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:setContentType(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (M)com.google.common.util.concurrent.ThreadFactoryBuilder:setNameFormat(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.crawl.CrawlDatum:getMetaData() (O)org.apache.hadoop.io.MapWritable:<init>()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.Inlink:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (O)org.apache.nutch.plugin.CircularDependencyException:<init>(java.lang.String)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher:getConf()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$102(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.crawl.CrawlDatum:setSignature(byte[])
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:setModifiedTime(long)
M:org.apache.nutch.util.DomUtil:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.MapFile$Reader:getKeyClass()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (M)java.util.regex.Pattern:matcher(java.lang.CharSequence)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:warning(org.xml.sax.SAXParseException) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.util.concurrent.atomic.AtomicInteger:incrementAndGet()
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.crawl.Injector$InjectReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDatum:<clinit>() (O)org.apache.nutch.crawl.CrawlDatum$Comparator:<init>()
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.handler.HandlerList:addHandler(org.mortbay.jetty.Handler)
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (M)java.io.OutputStream:flush()
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.plugin.Plugin:<init>(org.apache.nutch.plugin.PluginDescriptor,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.Plugin:setDescriptor(org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (O)java.io.OutputStreamWriter:<init>(java.io.OutputStream,java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (S)java.lang.Math:min(int,int)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)org.apache.nutch.scoring.webgraph.NodeReader:getConf()
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.MapFile$Reader:getValueClass()
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getOutputCompressionType(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.fetcher.OldFetcher:reportStatus()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:values() (M)java.util.LinkedList:add(java.lang.Object)
M:org.apache.nutch.parse.Outlink:skip(java.io.DataInput) (M)org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput)
M:org.apache.nutch.plugin.PluginDescriptor:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.fetcher.OldFetcher:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.fetcher.OldFetcher:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexWriters:describe() (I)org.apache.nutch.indexer.IndexWriter:describe()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.plugin.PluginManifestParser:parseRequires(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.NodeList:getLength()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.plugin.PluginRepository:<clinit>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String)
M:org.apache.nutch.parse.ParseResult:isSuccess() (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.parse.ParseText:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:contains(java.lang.Object)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.scoring.webgraph.Loops$Looper:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.URLUtil:getHostSegments(java.net.URL) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.apache.nutch.protocol.Protocol:getRobotRules(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (O)org.apache.nutch.util.PrefixStringMatcher:<init>(java.lang.String[])
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setCombinerClass(java.lang.Class)
M:org.apache.nutch.crawl.Inlinks:toString() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.Generator:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$500(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.parse.ParseResult:iterator()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.ProtocolStatus:isSuccess()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.nutch.util.FSUtils:closeReaders(org.apache.hadoop.io.MapFile$Reader[])
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.net.URI:toURL()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)java.util.List:iterator()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.metadata.MetaWrapper:write(java.io.DataOutput) (O)org.apache.nutch.crawl.NutchWritable:write(java.io.DataOutput)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat:<init>() (O)org.apache.hadoop.mapred.FileOutputFormat:<init>()
M:org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.Object,long) (S)java.lang.String:valueOf(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.io.BufferedWriter:<init>(java.io.Writer)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (M)java.lang.Class:getName()
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.parse.ParseData:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:getUri()
M:org.apache.nutch.metadata.SpellCheckedMetadata:getValues(java.lang.String) (O)org.apache.nutch.metadata.Metadata:getValues(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (O)java.io.ByteArrayOutputStream:<init>()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (I)java.util.List:size()
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (M)org.apache.nutch.protocol.Content:getUrl()
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.SequenceFile$Reader:getKeyClass()
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (S)java.nio.charset.Charset:forName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)org.apache.nutch.util.EncodingDetector$EncodingClue:meetsThreshold()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.NodeDumper:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginDescriptor:setPath(java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:toString()
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (O)org.apache.tika.Tika:<init>(org.apache.tika.config.TikaConfig)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)java.lang.Integer:valueOf(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (I)java.util.concurrent.ExecutorService:shutdownNow()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.util.URLUtil:getHost(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseResult:filter() (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (M)org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.parse.ParserChecker:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (M)org.apache.nutch.plugin.CircularDependencyException:getMessage()
M:org.apache.nutch.util.CommandRunner:exec() (O)java.util.concurrent.CyclicBarrier:<init>(int)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (O)org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map)
M:org.apache.nutch.plugin.PluginDescriptor:getDependencyLibs() (O)java.util.ArrayList:<init>()
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (O)org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.NodeDumper:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.parse.Outlink:toString() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.net.URLFilterChecker:checkAll() (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readLong()
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (M)java.lang.String:substring(int)
M:org.apache.nutch.metadata.Metadata:equals(java.lang.Object) (O)org.apache.nutch.metadata.Metadata:_getValues(java.lang.String)
M:org.apache.nutch.parse.HTMLMetaTags:reset() (M)java.util.Properties:clear()
M:org.apache.nutch.tools.proxy.LogDebugHandler:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (O)java.lang.String:<init>(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.TextProfileSignature$Token:<init>(int,java.lang.String) (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.protocol.Content:getMetadata()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:entrySet()
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (O)org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput)
M:org.apache.nutch.crawl.MapWritable:get(org.apache.hadoop.io.Writable) (O)org.apache.nutch.crawl.MapWritable:findEntryByKey(org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.FloatWritable:get()
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (I)java.util.List:size()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.Extension:addAttribute(java.lang.String,java.lang.String)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)java.io.BufferedInputStream:read(byte[])
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.fetcher.OldFetcher$FetcherThread:setName(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.protocol.Content:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)java.util.HashSet:addAll(java.util.Collection)
M:org.apache.nutch.protocol.Content:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (M)org.apache.nutch.protocol.Content:getBaseUrl()
M:org.apache.nutch.crawl.CrawlDbFilter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:iterator()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.OldFetcher:getConf()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:warning(org.xml.sax.SAXParseException) (M)org.xml.sax.SAXParseException:getMessage()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:<init>(org.apache.nutch.tools.DmozParser,org.xml.sax.XMLReader,int,boolean,int,java.util.regex.Pattern) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseStatus:getEmptyParse(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(java.lang.Object,java.lang.Object) (M)org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$300(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.String:trim()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.util.GZIPUtils:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.URLUtil:main(java.lang.String[]) (M)java.net.MalformedURLException:printStackTrace()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseResult:filter() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (O)java.util.StringTokenizer:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.parse.ParseStatus:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$102(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.Object:getClass()
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text) (M)org.apache.hadoop.mapred.SequenceFileRecordReader:next(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.Server:addHandler(org.mortbay.jetty.Handler)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:close(org.apache.hadoop.mapred.Reporter) (M)java.io.DataOutputStream:close()
M:org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (O)org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.util.PrefixStringMatcher:longestMatch(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.util.NutchConfiguration:setUUID(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.io.LongWritable:get()
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:addHeader(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setInt(java.lang.String,int)
M:org.apache.nutch.util.DeflateUtils:inflate(byte[]) (M)java.io.ByteArrayOutputStream:toByteArray()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.apache.nutch.parse.Parse:isCanonical()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDbReader:<clinit>() (O)org.apache.hadoop.mapred.lib.HashPartitioner:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:write(java.io.DataOutput) (I)java.util.Iterator:next()
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map) (O)org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (S)org.apache.nutch.fetcher.Fetcher:access$500(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.LinkDbReader:init(org.apache.hadoop.fs.Path) (M)org.apache.nutch.crawl.LinkDbReader:getConf()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem() (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.nutch.crawl.Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean)
M:org.apache.nutch.util.EncodingDetector:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.ParserFactory:getExtension(org.apache.nutch.plugin.Extension[],java.lang.String) (M)org.apache.nutch.plugin.Extension:getId()
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (O)com.google.common.util.concurrent.ThreadFactoryBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.parse.ParseCallable:<init>(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.HtmlParseFilters:filter(org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseResult,org.apache.nutch.parse.HTMLMetaTags,org.w3c.dom.DocumentFragment) (M)org.apache.nutch.parse.ParseResult:isSuccess()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (O)java.util.Random:<init>()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.nutch.segment.SegmentMerger:getConf()
M:org.apache.nutch.crawl.MapWritable:putAll(org.apache.nutch.crawl.MapWritable) (M)org.apache.nutch.crawl.MapWritable:size()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:close() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseData:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (M)javax.xml.transform.TransformerFactory:newTransformer()
M:org.apache.nutch.util.StringUtil:toHexString(byte[]) (S)org.apache.nutch.util.StringUtil:toHexString(byte[],java.lang.String,int)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)org.apache.nutch.plugin.Extension:getDescriptor()
M:org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:<init>(java.lang.String,org.apache.nutch.scoring.webgraph.Node)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseSegment:run(java.lang.String[]) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (O)org.apache.nutch.protocol.ProtocolNotFound:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$500(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:setSignature(byte[])
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.indexer.IndexingJob:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String)
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (S)java.lang.Character:isLetter(char)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.Class:getSimpleName()
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String,int)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.tika.Tika:<init>()
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.DeduplicationJob:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setMapRunnerClass(java.lang.Class)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL) (M)java.lang.String:indexOf(int)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:contains(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.net.URLFilters:filter(java.lang.String) (I)org.apache.nutch.net.URLFilter:filter(java.lang.String)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (O)org.xml.sax.InputSource:<init>(java.io.InputStream)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (O)org.apache.nutch.tools.ResolveUrls:<init>(java.lang.String,int)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (I)java.util.List:size()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:write(java.io.DataOutput) (M)org.apache.nutch.scoring.webgraph.Node:write(java.io.DataOutput)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexerOutputFormat$1:<init>(org.apache.nutch.indexer.IndexerOutputFormat,org.apache.nutch.indexer.IndexWriters) (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$3:run() (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.oro.text.regex.PatternMatcherInput:<init>(java.lang.String)
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (M)org.apache.tika.mime.MimeTypes:forName(java.lang.String)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:error(org.xml.sax.SAXParseException) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(java.lang.Object,java.lang.Object) (M)org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.CommandRunner:exec() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:<init>(java.io.DataOutputStream)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:setEndTime(long,boolean) (M)java.util.concurrent.atomic.AtomicLong:set(long)
M:org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (O)org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.parse.ParserFactory:getExtensions(java.lang.String) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)java.lang.Class:forName(java.lang.String)
M:org.apache.nutch.parse.ParseStatus:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.parse.ParseData:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.metadata.MetaWrapper:get()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.crawl.AbstractFetchSchedule:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.crawl.FetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseResult:get(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (M)org.apache.nutch.fetcher.Fetcher:getConf()
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.tools.proxy.NotFoundHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (M)java.lang.reflect.Field:getType()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:access$200(org.apache.nutch.parse.ParseOutputFormat)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char) (M)java.util.LinkedList:toArray(java.lang.Object[])
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)org.w3c.dom.Element:getAttribute(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)java.util.HashMap:<init>()
M:org.apache.nutch.crawl.AbstractFetchSchedule:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MapWritable$KeyValueEntry:hashCode() (M)org.apache.nutch.crawl.MapWritable$KeyValueEntry:toString()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (M)javax.xml.parsers.SAXParserFactory:newSAXParser()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (M)java.lang.Exception:toString()
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:<init>(org.apache.hadoop.io.Text,java.net.URL,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentPart:parse(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.parse.ParseResult:iterator() (I)java.util.Set:iterator()
M:org.apache.nutch.util.URLUtil:fixEmbeddedParams(java.net.URL,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.indexer.NutchDocument:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (M)java.lang.Float:floatValue()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (I)java.io.DataOutput:writeFloat(float)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.protocol.RobotRulesParser:parseRules(java.lang.String,byte[],java.lang.String,java.lang.String) (M)crawlercommons.robots.SimpleRobotRulesParser:parseContent(java.lang.String,byte[],java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.net.URLFilterException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.fs.FileSystem:getFileStatus(org.apache.hadoop.fs.Path)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:readFields(java.io.DataInput) (O)org.apache.nutch.scoring.webgraph.Node:<init>()
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:addTiming(java.lang.String,java.lang.String,long) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (M)java.util.HashMap:values()
M:org.apache.nutch.metadata.Metadata:equals(java.lang.Object) (M)org.apache.nutch.metadata.Metadata:size()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String)
M:org.apache.nutch.parse.ParseStatus:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (I)org.apache.hadoop.io.Writable:write(java.io.DataOutput)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:setEndTime(long) (O)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:setEndTime(long,boolean)
M:org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$600(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (O)java.util.Random:<init>()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:length()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.InlinkPriorityQueue:size()
M:org.apache.nutch.parse.Outlink:write(java.io.DataOutput) (M)org.apache.hadoop.io.MapWritable:write(java.io.DataOutput)
M:org.apache.nutch.net.URLFilter:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (I)java.util.Set:contains(java.lang.Object)
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(int)
M:org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getExtenstionPoints() (M)java.util.ArrayList:size()
M:org.apache.nutch.protocol.ProtocolFactory:findExtension(java.lang.String) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.EncodingDetector:<clinit>() (O)java.util.HashSet:<init>()
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.TrieStringMatcher:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.nutch.crawl.LinkDbMerger:getConf()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:getProgress() (M)org.apache.hadoop.mapred.SequenceFileRecordReader:getProgress()
M:org.apache.nutch.tools.proxy.FakeHandler:<init>() (O)org.apache.nutch.tools.proxy.AbstractTestbedHandler:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$LoopSet:getLoopSet()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:configure(org.apache.hadoop.mapred.JobConf) (S)java.util.regex.Pattern:compile(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.Content:equals(java.lang.Object) (M)org.apache.nutch.protocol.Content:getContent()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Node:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (O)org.apache.nutch.plugin.MissingDependencyException:<init>(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (S)org.apache.nutch.util.EncodingDetector$EncodingClue:access$100(org.apache.nutch.util.EncodingDetector$EncodingClue)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.crawl.CrawlDb:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.indexer.NutchField:toString() (M)java.lang.Object:toString()
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (O)org.apache.nutch.protocol.Content:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,org.apache.nutch.metadata.Metadata,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:get()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.plugin.ExtensionPoint:<init>(java.lang.String,java.lang.String,java.lang.String) (O)java.lang.Object:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)java.io.File:listFiles()
M:org.apache.nutch.protocol.ProtocolStatus:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (O)org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.segment.SegmentReader$3:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map) (O)java.lang.Thread:<init>()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (I)org.apache.nutch.crawl.FetchSchedule:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.io.LongWritable:set(long)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$400(org.apache.nutch.fetcher.Fetcher,int)
M:org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$InputCompatMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)org.slf4j.Logger:isTraceEnabled()
M:org.apache.nutch.util.DomUtil:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.Injector:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:setFound(boolean)
M:org.apache.nutch.util.GZIPUtils:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getFloat(java.lang.String,float)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.indexer.IndexerMapReduce:filterUrl(java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (O)org.apache.nutch.tools.proxy.SegmentHandler$Segment:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.NutchConfiguration:setUUID(org.apache.hadoop.conf.Configuration) (M)java.util.UUID:toString()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (O)java.io.FileReader:<init>(java.io.File)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.NodeReader:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable) (M)org.apache.nutch.crawl.NutchWritable:get()
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.tools.ResolveUrls:access$100()
M:org.apache.nutch.parse.ParseStatus:read(java.io.DataInput) (O)org.apache.nutch.parse.ParseStatus:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.crawl.MapWritable:equals(java.lang.Object) (O)java.util.HashSet:<init>()
M:org.apache.nutch.metadata.Metadata:remove(java.lang.String) (I)java.util.Map:remove(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:toString() (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:read() (M)java.io.Reader:mark(int)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.scoring.webgraph.Loops$Initializer:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.ProtocolStatus:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator$Selector:<init>() (O)org.apache.nutch.crawl.Generator$SelectorEntry:<init>()
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:iterator()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.ObjectCache:<clinit>() (O)java.util.WeakHashMap:<init>()
M:org.apache.nutch.util.CommandRunner$PusherThread:<init>(org.apache.nutch.util.CommandRunner,java.lang.String,java.io.InputStream,java.io.OutputStream) (O)org.apache.nutch.util.CommandRunner$PumperThread:<init>(org.apache.nutch.util.CommandRunner,java.lang.String,java.io.InputStream,java.io.OutputStream,boolean)
M:org.apache.nutch.parse.ParseResult:get(java.lang.String) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.indexer.NutchField:clone() (O)java.lang.Object:clone()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:next(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (I)org.apache.commons.logging.Log:debug(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (O)java.lang.RuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int) (O)java.util.zip.GZIPInputStream:<init>(java.io.InputStream)
M:org.apache.nutch.util.SuffixStringMatcher:longestMatch(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:isTerminal()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.InlinkPriorityQueue:insert(java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:containsKey(org.apache.hadoop.io.Writable) (O)org.apache.nutch.crawl.MapWritable:findEntryByKey(org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setPartitionerClass(java.lang.Class)
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (O)org.apache.nutch.protocol.ProtocolNotFound:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.metadata.MetaWrapper:<init>(org.apache.nutch.metadata.Metadata,org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.metadata.MetaWrapper:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.indexer.NutchField:getValues()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (I)java.util.Set:remove(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)java.util.concurrent.atomic.AtomicInteger:get()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseResult:get(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.CrawlDbMerger:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (I)java.util.Set:iterator()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.lang.ClassLoader:getResource(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseResult:isEmpty()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.io.OutputStream:write(byte[],int,int)
M:org.apache.nutch.util.TrieStringMatcher:matchChar(org.apache.nutch.util.TrieStringMatcher$TrieNode,java.lang.String,int) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (O)org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:addTiming(java.lang.String,java.lang.String,long) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.util.DeflateUtils:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.NodeReader:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:equals(java.lang.Object) (M)java.util.HashSet:equals(java.lang.Object)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.lang.String:length()
M:org.apache.nutch.crawl.LinkDbReader:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (M)java.util.concurrent.atomic.AtomicInteger:decrementAndGet()
M:org.apache.nutch.plugin.Extension:addAttribute(java.lang.String,java.lang.String) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper$NameType:valueOf(java.lang.String) (S)java.lang.Enum:valueOf(java.lang.Class,java.lang.String)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.Class:newInstance()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (I)java.util.Collection:iterator()
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (O)org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$702(org.apache.nutch.crawl.MapWritable$KeyValueEntry,byte)
M:org.apache.nutch.crawl.Inlink:skip(java.io.DataInput) (S)org.apache.hadoop.io.Text:skip(java.io.DataInput)
M:org.apache.nutch.parse.OutlinkExtractor:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.indexer.NutchField:<init>(java.lang.Object) (O)org.apache.nutch.indexer.NutchField:<init>(java.lang.Object,float)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDbFilter:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.crawl.FetchSchedule:initializeSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.protocol.RobotRulesParser:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilters:initialScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.metadata.MetaWrapper:write(java.io.DataOutput) (M)org.apache.nutch.metadata.Metadata:write(java.io.DataOutput)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:<init>(org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.util.Progressable) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Inlinks:getAnchors() (I)java.util.Set:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.FloatWritable:get()
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.MapWritable:values() (O)java.util.LinkedList:<init>()
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getTotalSize()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.crawl.CrawlDbReducer:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (M)java.io.BufferedReader:close()
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (I)java.util.Map:size()
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.Loops$LoopSet:<init>()
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.net.URL:getFile()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.NutchDocument:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)org.apache.nutch.crawl.TextProfileSignature:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.Plugin:<init>(org.apache.nutch.plugin.PluginDescriptor,org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:error(org.xml.sax.SAXParseException) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.fetcher.FetcherOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)java.lang.String:length()
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.io.LongWritable:set(long)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.metadata.SpellCheckedMetadata:remove(java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.lang.String) (S)org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.net.URL)
M:org.apache.nutch.crawl.Inlink:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (I)java.util.Collection:toArray(java.lang.Object[])
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)org.apache.nutch.crawl.Inlinks:iterator()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (I)java.util.Set:iterator()
M:org.apache.nutch.indexer.IndexWriters:describe() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$700(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:<init>() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (M)org.apache.nutch.parse.ParseStatus:getEmptyParseResult(java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration) (M)java.util.WeakHashMap:get(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.MapWritable$KeyValueEntry:<init>(org.apache.nutch.crawl.MapWritable,org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (O)java.lang.Object:<init>()
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.DeflateUtils:inflate(byte[]) (M)java.io.ByteArrayOutputStream:write(byte[],int,int)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (M)java.lang.Class:newInstance()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (M)java.io.BufferedReader:close()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)org.apache.hadoop.mapred.lib.HashPartitioner:<init>()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.CleaningJob$DeleterReducer:reduce(org.apache.hadoop.io.ByteWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.util.URLUtil:getProtocol(java.lang.String) (S)org.apache.nutch.util.URLUtil:getProtocol(java.net.URL)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.parse.ParseResult:filter() (M)org.apache.nutch.parse.ParseResult:iterator()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.util.StringUtil:fromHexString(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.segment.SegmentPart:<init>()
M:org.apache.nutch.crawl.AbstractFetchSchedule:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:<init>(java.lang.String) (O)java.lang.Thread:<init>()
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (I)java.util.Iterator:next()
M:org.apache.nutch.util.NodeWalker:hasNext() (M)java.util.Stack:size()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (I)java.util.Iterator:next()
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.FreeGenerator$FG:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (I)java.util.Iterator:next()
M:org.apache.nutch.segment.SegmentReader:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.IndexerMapReduce:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.Outlink:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getElementsByTagName(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (O)org.apache.nutch.fetcher.Fetcher$FetchItem:<init>(org.apache.hadoop.io.Text,java.net.URL,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)org.apache.nutch.protocol.Content:getContentType()
M:org.apache.nutch.plugin.PluginDescriptor:getNotExportedLibUrls() (M)java.util.ArrayList:size()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:emptyQueue()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (O)org.apache.hadoop.io.MapWritable:<init>()
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer)
M:org.apache.nutch.crawl.AbstractFetchSchedule:shouldFetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (O)org.apache.nutch.segment.SegmentReader$5:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)org.apache.nutch.plugin.PluginDescriptor:getClassLoader()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (O)java.util.ArrayList:<init>(java.util.Collection)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:getCounter(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:addExtension(org.apache.nutch.plugin.Extension)
M:org.apache.nutch.fetcher.OldFetcher:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (M)org.apache.nutch.tools.DmozParser$XMLCharFilter:close()
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.tools.arc.ArcRecordReader:getPos() (M)org.apache.hadoop.fs.FSDataInputStream:getPos()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (I)org.apache.commons.logging.Log:isInfoEnabled()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (I)java.util.Iterator:next()
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (M)java.io.OutputStream:write(byte[],int,int)
M:org.apache.nutch.scoring.webgraph.LinkRank:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.util.NodeWalker:skipChildren() (I)org.w3c.dom.NodeList:getLength()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.scoring.ScoringFilters:generatorSortValue(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,float) (I)org.apache.nutch.scoring.ScoringFilter:generatorSortValue(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,float)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.protocol.Content:equals(java.lang.Object) (S)java.util.Arrays:equals(byte[],byte[])
M:org.apache.nutch.crawl.Inlinks:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (I)java.io.DataOutput:writeBoolean(boolean)
M:org.apache.nutch.crawl.AbstractFetchSchedule:calculateLastFetchTime(org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.tools.DmozParser:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Route:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.Outlink:toString() (M)org.apache.hadoop.io.MapWritable:entrySet()
M:org.apache.nutch.protocol.ProtocolFactory:findExtension(java.lang.String) (M)org.apache.nutch.protocol.ProtocolFactory:contains(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.tools.Benchmark:getConf()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.WebGraph:getConf()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (M)java.lang.Class:newInstance()
M:org.apache.nutch.util.GZIPUtils:zip(byte[]) (O)java.io.ByteArrayOutputStream:<init>(int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.parse.ParseData:write(java.io.DataOutput) (M)org.apache.nutch.metadata.Metadata:write(java.io.DataOutput)
M:org.apache.nutch.crawl.LinkDbMerger:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.util.GZIPUtils:zip(byte[]) (O)java.util.zip.GZIPOutputStream:<init>(java.io.OutputStream)
M:org.apache.nutch.crawl.Inlink:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (O)org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:normalizeUrl(java.lang.String) (M)java.lang.String:trim()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:contains(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.nutch.util.FSUtils:closeReaders(org.apache.hadoop.io.MapFile$Reader[])
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (M)java.util.concurrent.atomic.AtomicInteger:decrementAndGet()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (M)org.apache.nutch.plugin.Extension:getAttribute(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:<init>(org.apache.nutch.segment.SegmentMerger$ObjectInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.SequenceFileRecordReader,org.apache.hadoop.io.Writable,java.lang.String) (O)org.apache.hadoop.mapred.SequenceFileRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.CommandRunner:exec() (M)java.lang.Process:destroy()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:next(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.hadoop.mapred.SequenceFileRecordReader:next(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:next(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.nutch.metadata.MetaWrapper:set(org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (S)java.nio.charset.Charset:isSupported(java.lang.String)
M:org.apache.nutch.parse.ParserChecker:main(java.lang.String[]) (O)org.apache.nutch.parse.ParserChecker:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.net.URL:getHost()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)java.net.MalformedURLException:toString()
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.Loops$Route:<init>()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:incrCounter(java.lang.String,java.lang.String,long)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.util.HashSet:iterator()
M:org.apache.nutch.crawl.MD5Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.nutch.protocol.Content:getContent()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (S)org.apache.nutch.util.StringUtil:isEmpty(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.ProtocolStatus:<clinit>() (O)org.apache.nutch.protocol.ProtocolStatus:<init>(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)
M:org.apache.nutch.segment.SegmentPart:get(org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char) (S)java.util.Arrays:sort(java.lang.Object[])
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (M)java.util.ArrayList:size()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.segment.SegmentPart:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:<init>(org.apache.hadoop.mapred.RecordReader,org.apache.nutch.fetcher.Fetcher$FetchItemQueues,int) (M)org.apache.nutch.fetcher.Fetcher$QueueFeeder:setName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.String:endsWith(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.Class:getName()
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.lang.Exception:printStackTrace()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseData:getContentMeta()
M:org.apache.nutch.segment.SegmentReader$4:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map) (O)java.lang.Thread:<init>()
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:equals(java.lang.Object) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.LinkDbFilter:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:toString() (M)org.apache.nutch.crawl.MapWritable$KeyValueEntry:toString()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasDbStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)java.lang.Object:toString()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:get(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endDocument() (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.crawl.CrawlDatum:<clinit>() (S)java.lang.Byte:valueOf(byte)
M:org.apache.nutch.util.PrefixStringMatcher:longestMatch(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:write(java.lang.Object,java.lang.Object) (M)org.apache.nutch.fetcher.FetcherOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (O)org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.CrawlDatum$Comparator:<init>() (O)org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.util.MimeUtil:getMimeType(java.io.File) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getChildNodes()
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)java.util.HashMap:containsKey(java.lang.Object)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseResult:filter() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:<init>(org.apache.nutch.tools.DmozParser,org.xml.sax.XMLReader,int,boolean,int,java.util.regex.Pattern) (O)org.xml.sax.helpers.DefaultHandler:<init>()
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:createKey() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:close()
M:org.apache.nutch.crawl.Injector:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.tools.proxy.TestbedProxy:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Generator$HashComparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.Generator$HashComparator:hash(byte[],int,int)
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.net.protocols.HttpDateFormat:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.mapred.SequenceFileRecordReader:createKey()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseStatus:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher$FetcherThread:start()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (I)java.util.concurrent.ExecutorService:awaitTermination(long,java.util.concurrent.TimeUnit)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:getRobotRulesSet(org.apache.nutch.protocol.Protocol,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (S)java.lang.Math:abs(int)
M:org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String) (M)java.lang.String:trim()
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (S)java.net.IDN:toASCII(java.lang.String)
M:org.apache.nutch.util.EncodingDetector$EncodingClue:<init>(org.apache.nutch.util.EncodingDetector,java.lang.String,java.lang.String,int) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.crawl.MD5Signature:<init>() (O)org.apache.nutch.crawl.Signature:<init>()
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (M)org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.StringUtil:leftPad(java.lang.String,int) (M)java.lang.String:length()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (I)java.util.List:size()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump()
M:org.apache.nutch.crawl.LinkDbMerger:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.crawl.LinkDbMerger:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRuntimeException:getMessage()
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.net.protocols.HttpDateFormat:<clinit>() (M)java.text.SimpleDateFormat:setTimeZone(java.util.TimeZone)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.arc.ArcRecordReader:getProgress() (M)org.apache.nutch.tools.arc.ArcRecordReader:getPos()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.EncodingDetector:<clinit>() (M)java.util.HashSet:add(java.lang.Object)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.ProtocolStatus:isSuccess()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.fetcher.Fetcher:reportStatus(int,int)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)java.util.HashMap:values()
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:setLookingFor(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.tools.proxy.SegmentHandler$SegmentPathFilter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (O)org.apache.nutch.crawl.CrawlDatum:<init>(int,int,float)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (S)org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.Outlink:setUrl(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Generator$SelectorEntry:write(java.io.DataOutput) (M)org.apache.hadoop.io.IntWritable:write(java.io.DataOutput)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)java.lang.String:trim()
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (S)org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.util.Map:size()
M:org.apache.nutch.scoring.webgraph.LinkDatum:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (I)java.util.Iterator:next()
M:org.apache.nutch.plugin.PluginRepository:getCachedClass(org.apache.nutch.plugin.PluginDescriptor,java.lang.String) (O)java.util.HashMap:<init>()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:iterator() (I)java.util.Set:iterator()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.protocol.Content:getUrl()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDatum:equals(java.lang.Object) (S)org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (O)org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.crawl.LinkDb:getHost(java.lang.String) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map)
M:org.apache.nutch.util.NutchConfiguration:create() (O)org.apache.hadoop.conf.Configuration:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.protocol.Content:getContent()
M:org.apache.nutch.scoring.webgraph.LinkDatum:write(java.io.DataOutput) (I)java.io.DataOutput:writeByte(int)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (O)org.apache.nutch.parse.ParsePluginsReader:<init>()
M:org.apache.nutch.parse.ParseResult:<init>(java.lang.String) (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getInlinkScore()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginRepository:getPluginDescriptors() (I)java.util.List:toArray(java.lang.Object[])
M:org.apache.nutch.protocol.ProtocolNotFound:<init>(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:getFetchTime(org.apache.nutch.parse.ParseData) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.NutchField:<init>(java.lang.Object,float) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.segment.SegmentReader$5:run() (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.crawl.FetchSchedule:setPageRetrySchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (M)java.lang.Object:getClass()
M:org.apache.nutch.parse.ParseStatus:<init>(int,int) (O)org.apache.nutch.parse.ParseStatus:<init>(int,int,java.lang.String[])
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)javax.xml.parsers.ParserConfigurationException:toString()
M:org.apache.nutch.util.URLUtil:getHost(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$InputCompatMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:writeOutAsDuplicate(org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:remove(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (M)java.util.regex.Pattern:matcher(java.lang.CharSequence)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseStatus:getEmptyParse(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (M)java.io.ByteArrayOutputStream:toByteArray()
M:org.apache.nutch.parse.ParseImpl:readFields(java.io.DataInput) (O)org.apache.nutch.parse.ParseData:<init>()
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlinks:add(org.apache.nutch.crawl.Inlink)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)java.util.Iterator:next()
M:org.apache.nutch.indexer.NutchDocument:removeField(java.lang.String) (I)java.util.Map:remove(java.lang.Object)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.BooleanWritable:<init>(boolean)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:readFields(java.io.DataInput) (I)java.util.Set:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$300(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.protocol.Content:getUrl()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:<init>() (O)org.apache.nutch.crawl.Generator$SelectorEntry:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:readFields(java.io.DataInput) (I)java.io.DataInput:readUTF()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:generateSegmentName() (M)java.text.SimpleDateFormat:format(java.util.Date)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseStatus:getMajorCode()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (O)org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValues(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (O)org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$InputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int) (M)org.apache.nutch.fetcher.OldFetcher$InputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)java.io.FileInputStream:<init>(java.io.File)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:handle(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,int) (M)org.apache.nutch.tools.proxy.AbstractTestbedHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.parse.ParseData:write(java.io.DataOutput) (M)org.apache.nutch.parse.Outlink:write(java.io.DataOutput)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (O)org.apache.nutch.indexer.IndexingFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:setOutlinkUrl(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (O)java.lang.Object:<init>()
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (I)java.util.List:get(int)
M:org.apache.nutch.crawl.MapWritable:toString() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:<init>(org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (I)java.util.List:addAll(java.util.Collection)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.Object:getClass()
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseStatus:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getUrl()
M:org.apache.nutch.crawl.AbstractFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LoopReader:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.plugin.Plugin:finalize() (M)org.apache.nutch.plugin.Plugin:shutDown()
M:org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler$SegmentPathFilter:accept(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.Path:getName()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:emptyQueue() (I)java.util.List:clear()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.parse.ParseResult:isSuccess() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.Class:getName()
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (O)org.apache.nutch.plugin.PluginDescriptor:getDependencyLibs()
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.plugin.ExtensionPoint:getExtensions() (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:normalizeUrl(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:getAnchors(org.apache.hadoop.io.Text) (M)org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.util.URLUtil:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.indexer.IndexWriters:delete(java.lang.String) (I)org.apache.nutch.indexer.IndexWriter:delete(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:read() (S)org.apache.xerces.util.XMLChar:isValid(int)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.crawl.CrawlDatum:toString()
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setReduceSpeculativeExecution(boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.protocol.Content:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (I)java.util.Iterator:next()
M:org.apache.nutch.util.URLUtil:isSameDomainName(java.net.URL,java.net.URL) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.crawl.Generator$HashComparator:<init>() (O)org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (O)java.io.FileInputStream:<init>(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:main(java.lang.String[]) (O)org.apache.nutch.crawl.DeduplicationJob:<init>()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.nutch.scoring.webgraph.WebGraph:getConf()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getQueueCount() (I)java.util.Map:size()
M:org.apache.nutch.segment.SegmentReader:access$100(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (O)org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:<init>(org.apache.hadoop.conf.Configuration) (O)com.ibm.icu.text.CharsetDetector:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseStatus:getMajorCode()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:addInProgressFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem) (I)java.util.Set:add(java.lang.Object)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.oro.text.regex.Perl5Compiler:<init>()
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseImpl:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.CrawlDatum:setSignature(byte[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (S)org.apache.nutch.util.EncodingDetector$EncodingClue:access$100(org.apache.nutch.util.EncodingDetector$EncodingClue)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Loops$Route,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:isFound()
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.metadata.Metadata:setAll(java.util.Properties) (I)java.util.Enumeration:nextElement()
M:org.apache.nutch.crawl.CrawlDbReader:openReaders(java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (I)java.util.Iterator:next()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getNumInlinks()
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)org.apache.hadoop.io.MapWritable:entrySet()
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.apache.oro.text.regex.PatternMatcher:getMatch()
M:org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)crawlercommons.robots.BaseRobotRules:isAllowed(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:readUrl(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.io.PrintStream:println(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:size()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.RecordWriter:close(org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.crawl.LinkDbFilter:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:getFetchTime(org.apache.nutch.parse.ParseData) (M)org.apache.nutch.parse.ParseData:getContentMeta()
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (O)org.apache.hadoop.mapred.SequenceFileRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)
M:org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.net.URL) (M)java.lang.String:lastIndexOf(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.parse.ParseSegment:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.mapred.Counters$Counter:increment(long)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:<clinit>() (O)java.util.concurrent.atomic.AtomicLong:<init>(long)
M:org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.mapred.JobConf:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)
M:org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginPath()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:warning(org.xml.sax.SAXParseException) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.util.URLUtil:fixEmbeddedParams(java.net.URL,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexingException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.net.URLFilterChecker:checkAll() (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setLong(java.lang.String,long)
M:org.apache.nutch.crawl.Generator$Selector:<init>() (O)org.apache.hadoop.io.LongWritable:<init>(long)
M:org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.Object,long) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Inlinks:readFields(java.io.DataInput) (M)org.apache.nutch.crawl.Inlinks:add(org.apache.nutch.crawl.Inlink)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:reduce(org.apache.hadoop.io.ByteWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)java.lang.Float:valueOf(float)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:contains(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.net.URLNormalizers:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArgs()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseCallable:call() (I)org.apache.nutch.parse.Parser:getParse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String) (M)java.io.File:toURI()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.net.InetAddress:getHostAddress()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.crawl.FetchSchedule:shouldFetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endElement(java.lang.String,java.lang.String,java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (I)java.util.Set:iterator()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.SequenceFile$Reader:close()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$InputFormat:<init>() (O)org.apache.hadoop.mapred.SequenceFileInputFormat:<init>()
M:org.apache.nutch.metadata.Metadata:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.util.Random:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:updateStatus(int) (M)java.util.concurrent.atomic.AtomicInteger:incrementAndGet()
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseImpl:write(java.io.DataOutput) (I)java.io.DataOutput:writeBoolean(boolean)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text) (O)java.lang.String:<init>(byte[])
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (M)java.lang.Class:getName()
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:set(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.parse.ParseSegment:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.Math:min(int,int)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (M)java.lang.StringBuffer:append(char)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.io.BufferedReader:close()
M:org.apache.nutch.crawl.Inlinks:add(org.apache.nutch.crawl.Inlink) (M)java.util.HashSet:add(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseStatus:<init>(int,java.lang.String) (O)org.apache.nutch.parse.ParseStatus:<init>(int,int,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:setFound(boolean)
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)org.w3c.dom.NodeList:getLength()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.LinkDbMerger:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.parse.ParseResult:get(org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.GenericWritableConfigurable:<init>() (O)org.apache.hadoop.io.GenericWritable:<init>()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.parse.ParseResult:put(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData) (M)org.apache.nutch.parse.ParseResult:put(java.lang.String,org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.util.GZIPUtils:zip(byte[]) (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.Generator:generateSegmentName() (M)java.text.SimpleDateFormat:format(java.util.Date)
M:org.apache.nutch.crawl.Inlinks:toString() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content)
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:getUri()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.WebGraph:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)javax.xml.parsers.DocumentBuilderFactory:newDocumentBuilder()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.crawl.MapWritable:keySet() (O)java.util.HashSet:<init>()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:<init>(org.apache.nutch.tools.DmozParser,org.xml.sax.XMLReader,int,boolean,int,java.util.regex.Pattern) (O)java.util.Random:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.parse.ParseStatus:<init>()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (O)java.util.ArrayList:<init>()
M:org.apache.nutch.util.CommandRunner:exec() (M)org.apache.nutch.util.CommandRunner$PusherThread:setDaemon(boolean)
M:org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)org.apache.nutch.plugin.Extension:getId()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern)
M:org.apache.nutch.crawl.CrawlDatum:<init>(int,int) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (O)java.lang.IllegalArgumentException:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (O)java.util.concurrent.atomic.AtomicLong:<init>()
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:getOutlinkUrl()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String) (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.TimingUtil:elapsedTime(long,long) (M)java.text.NumberFormat:setMinimumIntegerDigits(int)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.util.HashSet:size()
M:org.apache.nutch.indexer.NutchDocument:getFieldValue(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseData:getContentMeta()
M:org.apache.nutch.net.URLFilterChecker:checkAll() (O)java.io.InputStreamReader:<init>(java.io.InputStream)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.protocol.ProtocolStatus:toString()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.ObjectWritable:<init>()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getLong(java.lang.String,long)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getTotalSize()
M:org.apache.nutch.net.URLNormalizerChecker:checkAll(java.lang.String) (O)java.io.InputStreamReader:<init>(java.io.InputStream)
M:org.apache.nutch.plugin.PluginClassLoader:equals(java.lang.Object) (S)java.util.Arrays:equals(java.lang.Object[],java.lang.Object[])
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.fetcher.OldFetcher:access$700(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.indexer.CleaningJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.protocol.Content:equals(java.lang.Object) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)java.util.HashSet:<init>()
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (S)org.apache.nutch.crawl.Generator:generateSegmentName()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (S)java.util.ResourceBundle:getBundle(java.lang.String,java.util.Locale,java.lang.ClassLoader)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolStatus:getCode()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:<init>(org.apache.nutch.tools.DmozParser,org.xml.sax.XMLReader,int,boolean,int,java.util.regex.Pattern) (M)java.util.Random:nextInt()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.metadata.Metadata:write(java.io.DataOutput) (M)org.apache.nutch.metadata.Metadata:names()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (O)org.apache.nutch.tools.ResolveUrls$ResolverThread:<init>(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)java.io.IOException:toString()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.fetcher.OldFetcher$FetcherThread:setDaemon(boolean)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)java.lang.String:length()
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (I)org.slf4j.Logger:trace(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getSignature()
M:org.apache.nutch.tools.Benchmark:getDate() (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.crawl.FetchScheduleFactory:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (O)org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.Long:toString(long)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (O)java.util.HashMap:<init>()
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)org.apache.nutch.plugin.PluginDescriptor:getName()
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (M)java.net.InetAddress:getHostAddress()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector)
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.net.URL:getUserInfo()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$SelectorEntry:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseImpl:<init>(java.lang.String,org.apache.nutch.parse.ParseData) (O)org.apache.nutch.parse.ParseText:<init>(java.lang.String)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.parse.ParseOutputFormat:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.plugin.ExtensionPoint:getExtensions() (M)java.util.ArrayList:size()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getUrl()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.metadata.Metadata:isMultiValued(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String,java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.parse.ParserChecker:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (O)org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (M)java.io.PrintWriter:println(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int) (M)java.io.ByteArrayOutputStream:close()
M:org.apache.nutch.util.NodeWalker:<init>(org.w3c.dom.Node) (M)java.util.Stack:add(java.lang.Object)
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)java.io.ByteArrayOutputStream:toByteArray()
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getFloat(java.lang.String,float)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.plugin.PluginManifestParser:parseManifestFile(java.lang.String) (M)java.net.URI:toURL()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItemQueue(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.protocol.ProtocolNotFound:<init>(java.lang.String,java.lang.String) (O)org.apache.nutch.protocol.ProtocolException:<init>(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$6:run() (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.util.DomUtil:getDom(java.io.InputStream) (O)org.apache.xerces.parsers.DOMParser:<init>()
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getOutputCompressionType(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)org.apache.nutch.parse.ParseStatus:getEmptyParseResult(java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parseManifestFile(java.lang.String) (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean)
M:org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[]) (M)org.apache.hadoop.fs.FileStatus:getPath()
M:org.apache.nutch.segment.SegmentReader$2:run() (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkTimelimit() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Finalizer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Loops$Route,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum) (M)org.apache.hadoop.io.MapWritable:entrySet()
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setMetaData(org.apache.hadoop.io.MapWritable)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (I)java.util.List:size()
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.io.DataOutput:writeByte(int)
M:org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.TreeMap:<init>()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getUrl()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.net.URLFilterException:<init>(java.lang.String,java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.net.protocols.ProtocolException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (I)java.util.List:size()
M:org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.URLUtil:fixEmbeddedParams(java.net.URL,java.lang.String) (O)java.net.URL:<init>(java.net.URL,java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object) (S)org.apache.nutch.crawl.SignatureComparator:_compare(byte[],int,int,byte[],int,int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)org.apache.nutch.crawl.CrawlDatum:<init>(int,int)
M:org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.io.File:listFiles()
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature$TokenComparator:<init>(org.apache.nutch.crawl.TextProfileSignature$1) (O)org.apache.nutch.crawl.TextProfileSignature$TokenComparator:<init>()
M:org.apache.nutch.protocol.ProtocolStatus:getName() (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.util.TrieStringMatcher:addPatternBackward(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:size()
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:createValue() (M)org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:createValue()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.parse.ParseSegment:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDbReader:close() (M)org.apache.hadoop.io.MapFile$Reader:close()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.parse.ParseOutputFormat$1:<init>(org.apache.nutch.parse.ParseOutputFormat,org.apache.hadoop.io.MapFile$Writer,org.apache.hadoop.io.SequenceFile$Writer,java.lang.String[],int,boolean,boolean,int,org.apache.hadoop.io.MapFile$Writer)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.io.MD5Hash:getDigest()
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.util.NutchConfiguration:setUUID(org.apache.hadoop.conf.Configuration) (S)java.util.UUID:randomUUID()
M:org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (M)java.lang.String:trim()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.io.IOException:getMessage()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (S)org.apache.hadoop.io.MD5Hash:digest(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.Generator$SelectorInverseMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Generator$SelectorInverseMapper:map(org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator$SelectorEntry,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.ParsePluginsReader:<init>()
M:org.apache.nutch.indexer.NutchField:<init>() (O)java.util.ArrayList:<init>()
M:org.apache.nutch.util.SuffixStringMatcher:longestMatch(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char)
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (M)java.lang.Exception:toString()
M:org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (M)org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (M)org.apache.nutch.parse.ParseResult:isEmpty()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (O)java.util.LinkedList:<init>()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getReaders(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.TrieStringMatcher:matchChar(org.apache.nutch.util.TrieStringMatcher$TrieNode,java.lang.String,int) (M)java.lang.String:charAt(int)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)org.apache.nutch.plugin.Extension:getId()
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:<init>() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.crawl.Inlink:<init>(java.lang.String,java.lang.String) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.MapWritable:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:sendError(int,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.parse.ParseData:toString()
M:org.apache.nutch.indexer.CleaningJob:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.MapFile$Reader:close()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Long:toString(long)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:add(java.lang.Object)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.parse.ParseStatus:equals(java.lang.Object) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.segment.SegmentPart:get(org.apache.hadoop.mapred.FileSplit) (S)org.apache.nutch.segment.SegmentPart:get(java.lang.String)
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.arc.ArcRecordReader:createValue() (S)org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginDescriptor:getNotExportedLibUrls() (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem()
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)java.lang.String:indexOf(int)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:handle(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,int) (M)java.lang.Object:getClass()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:<init>() (O)org.apache.hadoop.io.LongWritable:<init>(long)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.fetcher.FetcherOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDatum$Comparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.SignatureComparator:_compare(byte[],int,int,byte[],int,int)
M:org.apache.nutch.util.SuffixStringMatcher:shortestMatch(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:isTerminal()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.parse.ParseData:toString() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.crawl.MapWritable:keySet() (M)java.util.HashSet:add(java.lang.Object)
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (O)java.io.FileInputStream:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpURI:toString()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher:addPatternBackward(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.Class:getName()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParserChecker:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.net.URL:getHost()
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader$1:run() (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.util.CommandRunner:exec() (S)java.lang.Thread:interrupted()
M:org.apache.nutch.segment.SegmentPart:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:<init>() (O)org.mortbay.jetty.handler.AbstractHandler:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.Outlink:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getNumOutlinks()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String,java.lang.String) (O)org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String,java.lang.String,long)
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.MapWritable$KeyValueEntry:toString() (M)java.lang.Object:toString()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)java.util.Iterator:next()
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (O)java.util.HashMap:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (M)java.util.concurrent.atomic.AtomicInteger:decrementAndGet()
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:readFields(java.io.DataInput) (O)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:<init>()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDatum:setFetchInterval(float) (S)java.lang.Math:round(float)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)java.lang.Exception:toString()
M:org.apache.nutch.crawl.Generator$SelectorInverseMapper:map(org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator$SelectorEntry,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Inlink:equals(java.lang.Object) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (M)org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:get(int)
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.util.ArrayList:iterator()
M:org.apache.nutch.crawl.MapWritable:containsValue(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.MapWritable:keySet() (M)org.apache.nutch.crawl.MapWritable:isEmpty()
M:org.apache.nutch.metadata.MetaWrapper:<init>(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.metadata.MetaWrapper:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilters:generatorSortValue(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,float)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.TrieStringMatcher:<init>() (O)org.apache.nutch.util.TrieStringMatcher$TrieNode:<init>(org.apache.nutch.util.TrieStringMatcher,char,boolean)
M:org.apache.nutch.crawl.CrawlDbReducer:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)java.util.List:size()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.TreeMap:isEmpty()
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobClient:getFs()
M:org.apache.nutch.fetcher.FetcherOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.indexer.IndexingException:<init>(java.lang.String,java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.scoring.webgraph.LinkDatum:write(java.io.DataOutput) (I)java.io.DataOutput:writeFloat(float)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(float)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.protocol.ProtocolStatus:getCode()
M:org.apache.nutch.segment.SegmentMergeFilters:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (M)com.google.common.util.concurrent.ThreadFactoryBuilder:setDaemon(boolean)
M:org.apache.nutch.scoring.ScoringFilters:passScoreBeforeParsing(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content) (I)org.apache.nutch.scoring.ScoringFilter:passScoreBeforeParsing(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.LongWritable:get()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)org.apache.nutch.util.EncodingDetector$EncodingClue:isEmpty()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String) (M)org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.util.CommandRunner:evaluate() (M)org.apache.nutch.util.CommandRunner:exec()
M:org.apache.nutch.plugin.Plugin:finalize() (O)java.lang.Object:finalize()
M:org.apache.nutch.util.NodeWalker:skipChildren() (M)java.util.Stack:pop()
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:setHandled(boolean)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (I)java.util.Iterator:next()
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Inlink:read(java.io.DataInput) (O)org.apache.nutch.crawl.Inlink:<init>()
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexerOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.indexer.NutchIndexAction) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.crawl.CrawlDatum:setSignature(byte[])
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.parse.ParseText:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpConnection:getRequestFields()
M:org.apache.nutch.indexer.NutchDocument:toString() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (O)org.apache.nutch.plugin.PluginManifestParser:parseLibraries(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.protocol.RobotRulesParser:getRobotRulesSet(org.apache.nutch.protocol.Protocol,org.apache.hadoop.io.Text) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setLong(java.lang.String,long)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.LinkDb:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.protocol.Content:toString() (O)java.lang.String:<init>(byte[])
M:org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String,int) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (O)java.lang.Thread:<init>()
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int) (O)java.io.ByteArrayInputStream:<init>(byte[])
M:org.apache.nutch.crawl.Inlinks:write(java.io.DataOutput) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.ContentAsTextInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.plugin.PluginManifestParser:parseLibraries(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getElementsByTagName(java.lang.String)
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (I)org.apache.hadoop.io.Writable:readFields(java.io.DataInput)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (I)org.apache.nutch.net.URLFilter:filter(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.util.HashMap:values()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.util.ArrayList:size()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (O)java.util.HashMap:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:toString() (M)java.lang.StringBuilder:length()
M:org.apache.nutch.scoring.webgraph.LinkRank:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.net.protocols.HttpDateFormat:toLong(java.lang.String) (M)java.util.Date:getTime()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.segment.SegmentPart:get(org.apache.hadoop.mapred.FileSplit)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:setContentLength(int)
M:org.apache.nutch.util.EncodingDetector:<clinit>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:filterUrl(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:shutDownActivatedPlugins() (M)java.util.HashMap:values()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:error(org.xml.sax.SAXParseException) (M)org.xml.sax.SAXParseException:getMessage()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.Math:floor(double)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.util.DomUtil:getDom(java.io.InputStream) (I)org.w3c.dom.Document:getChildNodes()
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.Injector$InjectReducer:configure(org.apache.hadoop.mapred.JobConf) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)java.lang.Integer:intValue()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:close() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)javax.xml.parsers.DocumentBuilder:parse(org.xml.sax.InputSource)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpURI:toString()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers) (M)java.net.URL:getHost()
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (M)java.io.ByteArrayOutputStream:close()
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.WebGraph:<init>()
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.tools.proxy.LogDebugHandler:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain) (I)javax.servlet.http.HttpServletResponse:addHeader(java.lang.String,java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseOutputFormat$SimpleEntry:getKey() (M)org.apache.nutch.parse.ParseOutputFormat$SimpleEntry:getKey()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:emptyQueue() (I)java.util.List:size()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getReaders(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)java.util.Random:<init>()
M:org.apache.nutch.util.URLUtil:isSameDomainName(java.lang.String,java.lang.String) (S)org.apache.nutch.util.URLUtil:isSameDomainName(java.net.URL,java.net.URL)
M:org.apache.nutch.protocol.ProtocolStatus:readFields(java.io.DataInput) (I)java.io.DataInput:readLong()
M:org.apache.nutch.parse.ParseStatus:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getLong(java.lang.String,long)
M:org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String,java.lang.String) (O)org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String,java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.parse.HTMLMetaTags:reset() (M)org.apache.nutch.metadata.Metadata:clear()
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (I)java.util.concurrent.ExecutorService:submit(java.util.concurrent.Callable)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.tools.proxy.DelayHandler:<init>(int) (O)org.apache.nutch.tools.proxy.AbstractTestbedHandler:<init>()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:writeOutAsDuplicate(org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)crawlercommons.robots.BaseRobotRules:getCrawlDelay()
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.Outlink:toString() (I)java.util.Set:iterator()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (I)java.util.List:size()
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:<init>(org.apache.nutch.util.TrieStringMatcher,char,boolean) (O)java.lang.Object:<init>()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:toString() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.protocol.ProtocolStatus:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getDomainName(java.lang.String)
M:org.apache.nutch.util.CommandRunner:exec() (M)org.apache.nutch.util.CommandRunner$PullerThread:setDaemon(boolean)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (I)java.util.Map:keySet()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (M)java.util.concurrent.atomic.AtomicInteger:incrementAndGet()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.LongWritable:set(long)
M:org.apache.nutch.util.DeflateUtils:inflate(byte[]) (M)java.util.zip.InflaterInputStream:read(byte[])
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.ObjectWritable:<init>()
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (M)java.lang.Boolean:booleanValue()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (S)org.apache.commons.lang.StringUtils:isNotBlank(java.lang.String)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (O)org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (M)java.lang.StringBuilder:append(char)
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:<init>() (O)org.apache.hadoop.mapred.FileOutputFormat:<init>()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (O)org.apache.nutch.crawl.CrawlDatum:<init>(int,int,float)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)java.lang.String:length()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (M)java.util.LinkedList:listIterator()
M:org.apache.nutch.indexer.CleaningJob$DBFilter:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.CleaningJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.util.DeflateUtils:inflate(byte[]) (O)java.util.zip.InflaterInputStream:<init>(java.io.InputStream)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)java.util.ArrayList:<init>(int)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (S)java.lang.Byte:valueOf(byte)
M:org.apache.nutch.crawl.MapWritable:putAll(org.apache.nutch.crawl.MapWritable) (M)org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (I)org.xml.sax.XMLReader:setErrorHandler(org.xml.sax.ErrorHandler)
M:org.apache.nutch.crawl.LinkDbReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.nutch.crawl.LinkDbReader:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:<init>() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.fetcher.Fetcher$FetcherThread:setName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:write(java.io.DataOutput) (I)java.util.Set:iterator()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (I)java.util.Collection:iterator()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)org.apache.nutch.crawl.LinkDb:getConf()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.ProtocolOutput:getStatus()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (O)java.lang.RuntimeException:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getLong(java.lang.String,long)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseStatus:getMinorCode()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (I)java.util.concurrent.ExecutorService:execute(java.lang.Runnable)
M:org.apache.nutch.tools.FreeGenerator$FG:<init>() (O)org.apache.hadoop.mapred.MapReduceBase:<init>()
M:org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String) (O)org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (I)java.util.concurrent.ExecutorService:shutdown()
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)java.util.Random:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:containsKey(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)org.apache.nutch.parse.ParseStatus:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:incrCounter(java.lang.String,java.lang.String,long)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (O)org.apache.nutch.segment.SegmentReader$2:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:close() (M)org.apache.nutch.indexer.IndexWriters:close()
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)java.io.File:getAbsolutePath()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpURI:getHost()
M:org.apache.nutch.parse.ParseData:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.parse.ParseOutputFormat$SimpleEntry:getValue() (M)org.apache.nutch.parse.ParseOutputFormat$SimpleEntry:getValue()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (O)java.lang.RuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (M)org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$LoopSet:getLoopSet()
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (O)org.apache.nutch.plugin.PluginRuntimeException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)java.util.HashSet:<init>(int)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseData:getOutlinks()
M:org.apache.nutch.util.DeflateUtils:inflate(byte[]) (O)java.io.ByteArrayOutputStream:<init>(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkTimelimit() (M)java.util.concurrent.atomic.AtomicInteger:set(int)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text) (M)org.apache.nutch.protocol.Content:getContent()
M:org.apache.nutch.crawl.Injector$InjectMapper:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbMerger:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:remove(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:reduce(org.apache.hadoop.io.ByteWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.CrawlDatum:<init>() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseData:equals(java.lang.Object) (M)org.apache.nutch.parse.ParseStatus:equals(java.lang.Object)
M:org.apache.nutch.crawl.Injector$InjectReducer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.net.URL:getHost()
M:org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)org.apache.nutch.crawl.CrawlDb:getConf()
M:org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput) (I)java.io.DataOutput:write(byte[])
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.util.ArrayList:addAll(java.util.Collection)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.crawl.Generator:getConf()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (I)java.util.Set:size()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.util.ArrayList:addAll(java.util.Collection)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:containsValue(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.indexer.IndexerMapReduce:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.indexer.IndexerMapReduce:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.TreeMap:lastKey()
M:org.apache.nutch.util.PrefixStringMatcher:matches(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char)
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (S)org.apache.nutch.util.NutchConfiguration:setUUID(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.FetcherOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getNumReduceTasks()
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (O)org.apache.nutch.crawl.CrawlDatum:<init>(int,int,float)
M:org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput) (M)org.apache.hadoop.io.MapWritable:write(java.io.DataOutput)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.protocol.ProtocolStatus:<init>(int,long) (O)org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.String[],long)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String) (M)java.net.URI:toURL()
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (M)org.apache.hadoop.io.DataOutputBuffer:getData()
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)org.apache.nutch.plugin.PluginDescriptor:getPluginPath()
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.DefaultFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getLong(java.lang.String,long)
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexerMapReduce:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)java.io.IOException:toString()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (O)org.apache.nutch.segment.SegmentReader:<init>(org.apache.hadoop.conf.Configuration,boolean,boolean,boolean,boolean,boolean,boolean)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (O)java.util.Random:<init>()
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$DumpType:<init>(java.lang.String,int) (O)java.lang.Enum:<init>(java.lang.String,int)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[])
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.hadoop.io.MapWritable:putAll(java.util.Map)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginRepository:shutDownActivatedPlugins() (I)java.util.Collection:iterator()
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (M)org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.fs.FileStatus:getLen()
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (O)java.util.HashMap:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.lang.String:trim()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)org.w3c.dom.NodeList:getLength()
M:org.apache.nutch.fetcher.Fetcher$InputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int) (M)org.apache.nutch.fetcher.Fetcher$InputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.io.File:isAbsolute()
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.segment.SegmentReader$4:run() (S)org.apache.nutch.segment.SegmentReader:access$100(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.AbstractFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.util.TreeMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:<init>(org.apache.hadoop.conf.Configuration,boolean,boolean,boolean,boolean,boolean,boolean) (M)org.apache.nutch.segment.SegmentReader:getConf()
M:org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String) (S)org.apache.commons.lang.StringUtils:getLevenshteinDistance(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.Generator$PartitionReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParserChecker:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.indexer.IndexingJob:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.Generator$SelectorEntry:<init>() (O)org.apache.hadoop.io.IntWritable:<init>(int)
M:org.apache.nutch.parse.ParseResult:put(java.lang.String,org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRuntimeException:toString()
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (M)org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:<clinit>() (O)crawlercommons.robots.SimpleRobotRules:<init>(crawlercommons.robots.SimpleRobotRules$RobotRulesMode)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (O)org.apache.nutch.plugin.PluginManifestParser:parseExtensionPoints(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.plugin.PluginRuntimeException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.util.PrefixStringMatcher:longestMatch(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char)
M:org.apache.nutch.plugin.PluginClassLoader:hashCode() (M)java.lang.Object:hashCode()
M:org.apache.nutch.crawl.CrawlDb:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.crawl.FetchSchedule:setPageGoneSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long)
M:org.apache.nutch.protocol.ProtocolFactory:contains(java.lang.String,java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.util.TimingUtil:elapsedTime(long,long) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.protocol.ProtocolFactory:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$300(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.parse.ParseSegment:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.CleaningJob:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:iterator() (I)java.util.Map:entrySet()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.parse.ParserFactory:getExtensionFromAlias(org.apache.nutch.plugin.Extension[],java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.metadata.SpellCheckedMetadata:add(java.lang.String,java.lang.String) (O)org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String)
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkRank$Inverter:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setJobName(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseImpl:readFields(java.io.DataInput) (M)org.apache.nutch.parse.ParseData:readFields(java.io.DataInput)
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParseResult:createParseResult(java.lang.String,org.apache.nutch.parse.Parse) (O)org.apache.nutch.parse.ParseText:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (O)java.util.ArrayList:<init>(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:add(java.lang.Object)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (O)org.apache.nutch.parse.ParseException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolStatus:getMessage()
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)org.apache.nutch.parse.ParseResult:isEmpty()
M:org.apache.nutch.util.NodeWalker:skipChildren() (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (M)org.apache.nutch.plugin.Extension:getId()
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (M)java.util.LinkedList:addAll(java.util.Collection)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseData:getMeta(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:next(java.lang.Object,java.lang.Object) (M)org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:next(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)java.net.URL:getHost()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)java.util.List:size()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)java.net.URL:openStream()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (S)java.lang.System:arraycopy(java.lang.Object,int,java.lang.Object,int,int)
M:org.apache.nutch.plugin.Extension:<init>(org.apache.nutch.plugin.PluginDescriptor,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository) (M)org.apache.nutch.plugin.Extension:setId(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (I)java.util.List:iterator()
M:org.apache.nutch.plugin.PluginManifestParser:parseRequires(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getElementsByTagName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)org.apache.nutch.scoring.webgraph.Node:<init>()
M:org.apache.nutch.protocol.Content:toString() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.ScoringFilters:updateDbScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,java.util.List) (I)org.apache.nutch.scoring.ScoringFilter:updateDbScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,java.util.List)
M:org.apache.nutch.plugin.Extension:<init>(org.apache.nutch.plugin.PluginDescriptor,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository) (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (O)java.io.BufferedInputStream:<init>(java.io.InputStream)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)java.util.List:size()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)org.apache.hadoop.fs.FSDataInputStream:getPos()
M:org.apache.nutch.parse.ParseSegment:run(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (I)java.util.ListIterator:previous()
M:org.apache.nutch.crawl.Inlinks:write(java.io.DataOutput) (M)java.util.HashSet:iterator()
M:org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (M)javax.xml.transform.Transformer:transform(javax.xml.transform.Source,javax.xml.transform.Result)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setScore(float)
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (S)org.apache.nutch.util.MimeUtil:cleanMimeType(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentReader:createJobConf() (M)org.apache.nutch.segment.SegmentReader:getConf()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (M)java.lang.Class:getName()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.parse.ParseText:<init>(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpURI:toString()
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.CircularDependencyException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.indexer.IndexWriters:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getAttribute(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Node:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.crawl.URLPartitioner:configure(org.apache.hadoop.mapred.JobConf) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.NutchDocument:add(java.lang.String,java.lang.Object) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:write(java.io.DataOutput) (S)org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.Generator$GeneratorOutputFormat:generateFileNameForKeyValue(java.lang.Object,java.lang.Object,java.lang.String) (M)org.apache.nutch.crawl.Generator$GeneratorOutputFormat:generateFileNameForKeyValue(org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator$SelectorEntry,java.lang.String)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration) (O)java.lang.Thread:<init>()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:close()
M:org.apache.nutch.crawl.InlinkPriorityQueue:<init>(int) (M)org.apache.nutch.crawl.InlinkPriorityQueue:initialize(int)
M:org.apache.nutch.crawl.MapWritable:putAll(org.apache.nutch.crawl.MapWritable) (I)java.util.Iterator:next()
M:org.apache.nutch.segment.SegmentReader:usage() (M)java.io.PrintStream:println()
M:org.apache.nutch.indexer.IndexingFiltersChecker:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)java.lang.Float:toString(float)
M:org.apache.nutch.segment.ContentAsTextInputFormat:<init>() (O)org.apache.hadoop.mapred.SequenceFileInputFormat:<init>()
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:<init>(org.apache.nutch.util.TrieStringMatcher,char,boolean) (O)java.util.LinkedList:<init>()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.parse.ParseOutputFormat$1:<init>(org.apache.nutch.parse.ParseOutputFormat,org.apache.hadoop.io.MapFile$Writer,org.apache.hadoop.io.SequenceFile$Writer,java.lang.String[],int,boolean,boolean,int,org.apache.hadoop.io.MapFile$Writer) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator$SelectorEntry:<init>() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (M)java.net.URL:getUserInfo()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.proxy.NotFoundHandler:<init>() (O)org.apache.nutch.tools.proxy.AbstractTestbedHandler:<init>()
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getDependencies() (M)java.util.ArrayList:size()
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (I)org.slf4j.Logger:isTraceEnabled()
M:org.apache.nutch.util.SuffixStringMatcher:matches(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.util.StringUtil:fromHexString(java.lang.String) (M)java.lang.String:trim()
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (I)java.util.List:iterator()
M:org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:setRetriesSinceFetch(int)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.lang.StringBuffer:length()
M:org.apache.nutch.fetcher.FetcherOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getOutputCompressionType(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (M)java.lang.String:replace(char,char)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)org.apache.hadoop.io.BytesWritable:set(byte[],int,int)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.lang.String:endsWith(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.plugin.PluginManifestParser:parseLibraries(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.NodeList:getLength()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.crawl.Generator$PartitionReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Generator$PartitionReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.parse.ParseStatus:getMinorCode()
M:org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:set(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.TextProfileSignature:<init>() (O)org.apache.nutch.crawl.Signature:<init>()
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String)
M:org.apache.nutch.segment.SegmentReader$1:run() (S)org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.nutch.indexer.IndexingJob:getConf()
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)java.lang.Integer:valueOf(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.io.OutputStreamWriter:<init>(java.io.OutputStream)
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (M)java.net.URL:getHost()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.indexer.IndexWriters:open(org.apache.hadoop.mapred.JobConf,java.lang.String)
M:org.apache.nutch.parse.ParseImpl:write(java.io.DataOutput) (M)org.apache.nutch.parse.ParseData:write(java.io.DataOutput)
M:org.apache.nutch.util.CommandRunner:exec() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (I)org.xml.sax.XMLReader:setContentHandler(org.xml.sax.ContentHandler)
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.Node:<init>()
M:org.apache.nutch.util.StringUtil:main(java.lang.String[]) (S)org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String)
M:org.apache.nutch.crawl.FetchScheduleFactory:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.NodeWalker:skipChildren() (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.crawl.Inlink:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (M)java.net.URL:getPath()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (M)java.util.concurrent.atomic.AtomicInteger:incrementAndGet()
M:org.apache.nutch.protocol.ProtocolStatus:<clinit>() (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItemQueue(java.lang.String) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseData:getTitle()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.Generator:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.MD5Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)org.apache.hadoop.io.MD5Hash:digest(byte[])
M:org.apache.nutch.indexer.CleaningJob:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)java.lang.Class:getName()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (S)org.apache.nutch.segment.SegmentPart:parse(java.lang.String)
M:org.apache.nutch.parse.ParseException:<init>() (O)java.lang.Exception:<init>()
M:org.apache.nutch.util.EncodingDetector$EncodingClue:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.io.File:listFiles()
M:org.apache.nutch.scoring.webgraph.LinkDumper:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.LongWritable:get()
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (O)java.util.LinkedList:<init>()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.Server:join()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.tools.Benchmark$BenchmarkResults:addTiming(java.lang.String,java.lang.String,long)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getOutputCompressionType(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.net.protocols.HttpDateFormat:toString(long) (O)java.util.Date:<init>(long)
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)org.apache.nutch.scoring.webgraph.Loops$LoopSet:getLoopSet()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:length()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (O)java.io.FileReader:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.Inlink:hashCode() (M)java.lang.String:hashCode()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.apache.oro.text.regex.MatchResult:group(int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean)
M:org.apache.nutch.indexer.NutchIndexAction:readFields(java.io.DataInput) (O)org.apache.nutch.indexer.NutchDocument:<init>()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.String:getBytes(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseData:getContentMeta()
M:org.apache.nutch.parse.ParseException:<init>(java.lang.String,java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginPath()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:normalizeUrl(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:warning(org.xml.sax.SAXParseException) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.crawl.Generator$SelectorEntry:toString() (M)org.apache.nutch.crawl.CrawlDatum:toString()
M:org.apache.nutch.parse.ParseText:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.lang.Thread:isAlive()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlink:getFromUrl()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.util.Set:iterator()
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.protocol.Content:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,org.apache.nutch.metadata.Metadata,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.protocol.Content:getContentType(java.lang.String,java.lang.String,byte[])
M:org.apache.nutch.crawl.LinkDbFilter:<init>() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseText:read(java.io.DataInput) (M)org.apache.nutch.parse.ParseText:readFields(java.io.DataInput)
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:iterator()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule$AdaptiveRate:<init>(org.apache.nutch.crawl.MimeAdaptiveFetchSchedule,java.lang.Float,java.lang.Float) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (O)org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (O)java.io.ByteArrayInputStream:<init>(byte[])
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (I)java.util.Set:iterator()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (S)org.apache.nutch.fetcher.Fetcher:isStoringContent(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.URLPartitioner:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.util.EncodingDetector:<clinit>() (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem() (I)java.util.List:size()
M:org.apache.nutch.parse.ParseSegment:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.arc.ArcRecordReader:getProgress() (S)java.lang.Math:min(float,float)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (M)java.util.concurrent.atomic.AtomicLong:addAndGet(long)
M:org.apache.nutch.util.MimeUtil:getMimeType(java.io.File) (M)java.io.File:getPath()
M:org.apache.nutch.indexer.IndexWriters:open(org.apache.hadoop.mapred.JobConf,java.lang.String) (I)org.apache.nutch.indexer.IndexWriter:open(org.apache.hadoop.mapred.JobConf,java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.LinkDumper:<init>()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Float:toString(float)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:access$000(org.apache.nutch.parse.ParseOutputFormat)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (S)java.lang.Character:toLowerCase(char)
M:org.apache.nutch.tools.proxy.LogDebugHandler:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain) (I)javax.servlet.FilterChain:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (S)java.net.InetAddress:getByName(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.DeduplicationJob:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.scoring.webgraph.Node:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.DefaultFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(int)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher:isParsing(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.parse.HTMLMetaTags:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map:containsKey(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (I)java.util.List:size()
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean)
M:org.apache.nutch.parse.ParseStatus:getEmptyParseResult(java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.parse.ParseResult:createParseResult(java.lang.String,org.apache.nutch.parse.Parse)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.plugin.MissingDependencyException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.util.SuffixStringMatcher:shortestMatch(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginDescriptor:setPluginClass(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.mortbay.jetty.bio.SocketConnector:<init>()
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.protocol.Content:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,org.apache.nutch.metadata.Metadata,org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:getFileStatus(org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.protocol.RobotRulesParser:getRobotRulesSet(org.apache.nutch.protocol.Protocol,org.apache.hadoop.io.Text) (M)org.apache.nutch.protocol.RobotRulesParser:getRobotRulesSet(org.apache.nutch.protocol.Protocol,java.net.URL)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.fs.Path:getName()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.StringUtil:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginRepository:getCachedClass(org.apache.nutch.plugin.PluginDescriptor,java.lang.String) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.NodeWalker:<init>(org.w3c.dom.Node) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getNumReduceTasks()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.metadata.Metadata:setAll(java.util.Properties) (M)java.util.Properties:getProperty(java.lang.String)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.mapred.SequenceFileRecordReader:createValue()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.MapWritable:<init>()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseResult:get(java.lang.String)
M:org.apache.nutch.parse.ParseData:read(java.io.DataInput) (O)org.apache.nutch.parse.ParseData:<init>()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:setUrl(java.lang.String)
M:org.apache.nutch.indexer.IndexWriters:update(org.apache.nutch.indexer.NutchDocument) (I)org.apache.nutch.indexer.IndexWriter:update(org.apache.nutch.indexer.NutchDocument)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (M)java.lang.String:hashCode()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.Inlinks:toString() (M)java.lang.StringBuffer:append(java.lang.Object)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (M)javax.xml.parsers.SAXParser:getXMLReader()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.io.OutputStream:write(byte[])
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.io.LineNumberReader:close()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.TreeMap:get(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseResult:iterator()
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (I)org.apache.nutch.parse.Parser:getParse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readFloat()
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$GeneratorOutputFormat:generateFileNameForKeyValue(org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator$SelectorEntry,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.metadata.SpellCheckedMetadata:getValues(java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:addTiming(java.lang.String,java.lang.String,long) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginDescriptor:setName(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:setSignature(byte[])
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (I)java.io.DataInput:readFloat()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.protocol.Content:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)java.util.Random:nextInt()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.io.File:toString()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (I)org.apache.nutch.crawl.FetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int)
M:org.apache.nutch.util.PrefixStringMatcher:<init>(java.lang.String[]) (M)org.apache.nutch.util.PrefixStringMatcher:addPatternForward(java.lang.String)
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.indexer.IndexerMapReduce:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)java.util.ArrayList:addAll(java.util.Collection)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)java.lang.Object:getClass()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)org.apache.nutch.parse.ParseText:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.Path:getParent()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int)
M:org.apache.nutch.parse.Outlink:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:closeReaders() (M)org.apache.hadoop.io.MapFile$Reader:close()
M:org.apache.nutch.net.URLNormalizerChecker:checkAll(java.lang.String) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.Parse) (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.scoring.webgraph.Node:toString() (M)org.apache.nutch.scoring.webgraph.Node:getOutlinkScore()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.lang.StringBuffer:append(char)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:handle(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,int) (M)java.lang.Class:getSimpleName()
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (M)java.lang.Exception:toString()
M:org.apache.nutch.indexer.NutchDocument:getFieldValue(java.lang.String) (I)java.util.List:get(int)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable) (I)org.apache.hadoop.mapred.RecordWriter:write(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.util.TimingUtil:elapsedTime(long,long) (M)java.text.NumberFormat:format(long)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.HashSet:<init>()
M:org.apache.nutch.crawl.Injector:main(java.lang.String[]) (O)org.apache.nutch.crawl.Injector:<init>()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.apache.nutch.protocol.Protocol:getProtocolOutput(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:keySet() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (I)java.util.concurrent.Future:get(long,java.util.concurrent.TimeUnit)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:closeReaders(org.apache.hadoop.io.MapFile$Reader[]) (M)org.apache.hadoop.io.MapFile$Reader:close()
M:org.apache.nutch.parse.ParserFactory:match(org.apache.nutch.plugin.Extension,java.lang.String,java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getRetriesSinceFetch()
M:org.apache.nutch.crawl.LinkDbReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.tika.mime.MimeTypesFactory:create(java.io.InputStream)
M:org.apache.nutch.plugin.Extension:<init>(org.apache.nutch.plugin.PluginDescriptor,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository) (M)org.apache.nutch.plugin.Extension:setClazz(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.NutchIndexAction:write(java.io.DataOutput) (I)java.io.DataOutput:write(int)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (I)java.util.Map:entrySet()
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.util.URLUtil:resolveURL(java.net.URL,java.lang.String) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getSignature()
M:org.apache.nutch.crawl.MapWritable:values() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseResult:filter() (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.util.URLUtil:fixEmbeddedParams(java.net.URL,java.lang.String) (M)java.lang.String:substring(int)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)com.ibm.icu.text.CharsetMatch:getConfidence()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.protocol.Content:getMetadata()
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetcherThread:getName()
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)org.apache.nutch.parse.ParserNotFound:getMessage()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String,java.lang.String)
M:org.apache.nutch.util.NodeWalker:nextNode() (M)org.apache.nutch.util.NodeWalker:hasNext()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.Inlinks:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.Benchmark:getDate() (O)java.util.Date:<init>(long)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getInlinkScore()
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:progress()
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtensionPoints(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getElementsByTagName(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (O)java.util.HashMap:<init>()
M:org.apache.nutch.crawl.CrawlDatum:getStatusName(byte) (S)java.lang.Byte:valueOf(byte)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.SequenceFile$Reader:close()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseData:getContentMeta()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseResult:isEmpty()
M:org.apache.nutch.plugin.PluginManifestParser:parseXML(java.net.URL) (S)javax.xml.parsers.DocumentBuilderFactory:newInstance()
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.net.URL:getPort()
M:org.apache.nutch.segment.SegmentReader:usage() (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.crawl.Injector$InjectReducer:configure(org.apache.hadoop.mapred.JobConf) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.util.CommandRunner:exec() (M)java.lang.Process:getErrorStream()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseText:toString()
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.crawl.Signature:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)java.util.Properties:keySet()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.util.NodeWalker:nextNode() (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginDescriptor:getExtenstionPoints() (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseData:write(java.io.DataOutput) (M)org.apache.nutch.parse.ParseStatus:write(java.io.DataOutput)
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)java.util.HashSet:contains(java.lang.Object)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (I)java.io.DataInput:readBoolean()
M:org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (S)java.net.URLDecoder:decode(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.Content:setContentType(java.lang.String)
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parseRequires(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.fetcher.OldFetcher:getConf()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.StringUtil:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.BytesWritable:<init>(byte[])
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)crawlercommons.robots.SimpleRobotRulesParser:parseContent(java.lang.String,byte[],java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.LoopReader:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setLong(java.lang.String,long)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.net.URLFilterChecker:checkAll() (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseStatus:getArgs()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.fetcher.Fetcher$InputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int) (M)org.apache.hadoop.fs.FileStatus:getLen()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.metadata.MetaWrapper:setMeta(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.fetcher.OldFetcher:access$600(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.net.URL:toString()
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (O)org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getExtensions() (M)java.util.ArrayList:size()
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)java.util.HashSet:add(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher:<init>() (O)java.util.concurrent.atomic.AtomicInteger:<init>(int)
M:org.apache.nutch.scoring.webgraph.NodeDumper:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.fetcher.Fetcher:isStoringContent(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (O)org.apache.hadoop.io.MapWritable:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (O)org.apache.nutch.crawl.TextProfileSignature$Token:<init>(int,java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.mortbay.jetty.handler.HandlerList:<init>()
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.hadoop.io.Text:equals(java.lang.Object)
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (M)java.io.InputStream:close()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getRetriesSinceFetch()
M:org.apache.nutch.util.URLUtil:resolveURL(java.net.URL,java.lang.String) (S)org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String)
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.protocol.ProtocolStatus:toString() (O)java.lang.Integer:<init>(int)
M:org.apache.nutch.util.URLUtil:fixEmbeddedParams(java.net.URL,java.lang.String) (M)java.net.URL:toString()
M:org.apache.nutch.indexer.IndexingException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.NodeDumper$DumpType:valueOf(java.lang.String) (S)java.lang.Enum:valueOf(java.lang.Class,java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:setLinkType(byte)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.StringUtil:leftPad(java.lang.String,int) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (O)java.io.InputStreamReader:<init>(java.io.InputStream,java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (O)org.apache.nutch.plugin.PluginManifestParser:parseManifestFile(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String) (M)java.lang.String:endsWith(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.GZIPUtils:zip(byte[]) (M)java.util.zip.GZIPOutputStream:close()
M:org.apache.nutch.segment.SegmentReader$6:run() (S)org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.URLUtil:getHostSegments(java.net.URL) (M)java.net.URL:getHost()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:<init>(org.apache.hadoop.conf.Configuration) (O)java.util.concurrent.atomic.AtomicInteger:<init>(int)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDatum:toString() (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (M)org.apache.nutch.util.SuffixStringMatcher:longestMatch(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.StringUtil:toHexString(byte[],java.lang.String,int) (O)java.lang.StringBuffer:<init>(int)
M:org.apache.nutch.segment.SegmentMerger:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (I)org.xml.sax.XMLReader:parse(org.xml.sax.InputSource)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (I)java.util.Map:containsKey(java.lang.Object)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.net.URL:getRef()
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:entrySet()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:getFetchTime(org.apache.nutch.parse.ParseData) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setInt(java.lang.String,int)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)org.apache.nutch.plugin.PluginRuntimeException:printStackTrace()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:getOutputStream()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.scoring.ScoringFilters:passScoreBeforeParsing(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginRepository:getPluginDescriptor(java.lang.String) (I)java.util.List:iterator()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (M)org.apache.nutch.plugin.ExtensionPoint:addExtension(org.apache.nutch.plugin.Extension)
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (M)java.lang.StringBuilder:append(double)
M:org.apache.nutch.util.GZIPUtils:zip(byte[]) (M)java.util.zip.GZIPOutputStream:write(byte[])
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:iterator()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.ProtocolOutput:getContent()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.hadoop.io.MapWritable:containsKey(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.Loops:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (M)java.io.InputStream:read(byte[])
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:readFields(java.io.DataInput) (O)java.util.HashSet:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.CleaningJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:<clinit>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:get()
M:org.apache.nutch.util.StringUtil:rightPad(java.lang.String,int) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.util.CommandRunner:exec() (M)java.lang.Process:getInputStream()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.Node:toString()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (M)java.util.concurrent.atomic.AtomicLong:get()
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getLinkType()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$800(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)org.apache.nutch.parse.ParserFactory:getExtensions(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)org.apache.nutch.scoring.webgraph.Node:getNumOutlinks()
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuffer:append(long)
M:org.apache.nutch.indexer.IndexerOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.nutch.indexer.IndexWriters:open(org.apache.hadoop.mapred.JobConf,java.lang.String)
M:org.apache.nutch.parse.ParseResult:createParseResult(java.lang.String,org.apache.nutch.parse.Parse) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.util.CommandRunner:exec() (M)java.util.concurrent.CyclicBarrier:await()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.mapred.FileSplit:getPath()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.util.EncodingDetector$EncodingClue:isEmpty() (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.ParseResult:filter() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Generator$SelectorEntry:toString() (M)org.apache.hadoop.io.IntWritable:toString()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (S)java.net.InetAddress:getByName(java.lang.String)
M:org.apache.nutch.crawl.DefaultFetchSchedule:<init>() (O)org.apache.nutch.crawl.AbstractFetchSchedule:<init>()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilterException:getMessage()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:<init>(java.io.DataOutputStream) (M)java.io.DataOutputStream:writeBytes(java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:write(java.io.DataOutput) (I)java.io.DataOutput:writeLong(long)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:metadataEquals(org.apache.hadoop.io.MapWritable) (M)org.apache.hadoop.io.MapWritable:size()
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.parse.ParseResult:size() (I)java.util.Map:size()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.parse.ParserFactory:getExtensions(java.lang.String) (S)org.apache.nutch.util.MimeUtil:cleanMimeType(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:contains(java.lang.Object)
M:org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.util.ArrayList:get(int)
M:org.apache.nutch.parse.Outlink:toString() (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.util.HadoopFSUtil:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)java.util.UUID:randomUUID()
M:org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable) (O)org.apache.nutch.util.GenericWritableConfigurable:<init>()
M:org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getNumReduceTasks()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setNumReduceTasks(int)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolOutput:getContent()
M:org.apache.nutch.metadata.Metadata:toString() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDatum:compareTo(org.apache.nutch.crawl.CrawlDatum) (S)org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$400(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:incrementExceptionCounter() (M)java.util.concurrent.atomic.AtomicInteger:incrementAndGet()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.FloatWritable:<init>(float)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.nutch.crawl.Generator:generateSegmentName()
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.crawl.NutchWritable:<init>() (O)org.apache.nutch.util.GenericWritableConfigurable:<init>()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (O)org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:warning(org.xml.sax.SAXParseException) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.URLPartitioner:configure(org.apache.hadoop.mapred.JobConf) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (S)org.apache.nutch.parse.ParseStatus:read(java.io.DataInput)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.Outlink:skip(java.io.DataInput) (I)java.io.DataInput:readBoolean()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.LinkDb:getHost(java.lang.String)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.io.Writer:write(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (O)org.xml.sax.InputSource:<init>(java.io.Reader)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (O)org.apache.nutch.crawl.MapWritable$KeyValueEntry:<init>(org.apache.nutch.crawl.MapWritable,org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.crawl.TextProfileSignature$TokenComparator:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.protocol.Content:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,org.apache.nutch.metadata.Metadata,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.plugin.ExtensionPoint:<init>(java.lang.String,java.lang.String,java.lang.String) (O)org.apache.nutch.plugin.ExtensionPoint:setName(java.lang.String)
M:org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginRepository:getCachedClass(org.apache.nutch.plugin.PluginDescriptor,java.lang.String) (M)org.apache.nutch.plugin.PluginClassLoader:loadClass(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setNumReduceTasks(int)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.protocol.Content:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseResult:iterator()
M:org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.util.EncodingDetector:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.ScoringFilter:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.util.NutchConfiguration:create() (S)org.apache.nutch.util.NutchConfiguration:setUUID(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.util.Random:nextInt()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.net.URL:toExternalForm()
M:org.apache.nutch.tools.arc.ArcInputFormat:<init>() (O)org.apache.hadoop.mapred.FileInputFormat:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtensionPoints(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.NodeList:getLength()
M:org.apache.nutch.scoring.webgraph.NodeDumper$AggrType:values() (M)org.apache.nutch.scoring.webgraph.NodeDumper$AggrType[]:clone()
M:org.apache.nutch.indexer.NutchField:<init>(java.lang.Object,float) (I)java.util.List:addAll(java.util.Collection)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$002(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.hadoop.io.Writable)
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (M)java.lang.String:lastIndexOf(java.lang.String)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Node:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.util.DeflateUtils:deflate(byte[]) (M)java.util.zip.DeflaterOutputStream:write(byte[])
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.ScoringFilterException:<init>() (O)java.lang.Exception:<init>()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)java.util.Random:<init>()
M:org.apache.nutch.crawl.Inlinks:toString() (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.LinkDbReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (O)org.apache.nutch.crawl.MapWritable:getClass(byte)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)org.apache.nutch.parse.ParseData:<init>(org.apache.nutch.parse.ParseStatus,java.lang.String,org.apache.nutch.parse.Outlink[],org.apache.nutch.metadata.Metadata,org.apache.nutch.metadata.Metadata)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:getLookingFor()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.tools.ResolveUrls:access$300()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parseLibraries(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String)
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)org.w3c.dom.Element:getElementsByTagName(java.lang.String)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.Inlinks:toString() (M)java.util.HashSet:iterator()
M:org.apache.nutch.util.GZIPUtils:unzip(byte[]) (O)java.io.ByteArrayOutputStream:<init>(int)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)org.apache.nutch.parse.ParseImpl:<init>(java.lang.String,org.apache.nutch.parse.ParseData)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)com.ibm.icu.text.CharsetDetector:setText(byte[])
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)org.apache.nutch.protocol.Protocol:getProtocolOutput(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.parse.ParseData:toString() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (O)org.apache.nutch.util.TrieStringMatcher$TrieNode:<init>(org.apache.nutch.util.TrieStringMatcher,char,boolean)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)java.lang.Integer:intValue()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.plugin.ExtensionPoint:<init>(java.lang.String,java.lang.String,java.lang.String) (O)org.apache.nutch.plugin.ExtensionPoint:setSchema(java.lang.String)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpURI:toString()
M:org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)java.util.HashMap:containsKey(java.lang.Object)
M:org.apache.nutch.parse.ParseOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (M)java.io.OutputStream:close()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (M)java.io.IOException:toString()
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.util.HashMap:containsKey(java.lang.Object)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long)
M:org.apache.nutch.fetcher.OldFetcher:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.fetcher.OldFetcher:isStoringContent(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getContent(org.apache.hadoop.io.Text) (O)org.apache.nutch.protocol.Content:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (I)java.io.DataOutput:writeLong(long)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput)
M:org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String) (M)org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String,int)
M:org.apache.nutch.parse.HtmlParseFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (S)org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.Object:getClass()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.util.FSUtils:closeReaders(org.apache.hadoop.io.MapFile$Reader[]) (M)org.apache.hadoop.io.MapFile$Reader:close()
M:org.apache.nutch.scoring.webgraph.LoopReader:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseImpl:<init>(java.lang.String,org.apache.nutch.parse.ParseData) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:flushBuffer()
M:org.apache.nutch.fetcher.OldFetcher:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.nutch.crawl.Generator:generateSegmentName()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.URLPartitioner:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.net.protocols.ProtocolException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.fetcher.Fetcher$FetcherThread:setDaemon(boolean)
M:org.apache.nutch.parse.ParseResult:iterator() (I)java.util.Map:entrySet()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)org.apache.hadoop.fs.FSDataInputStream:seek(long)
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValues(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.protocol.ProtocolException:<init>() (O)java.lang.Exception:<init>()
M:org.apache.nutch.indexer.IndexWriters:close() (I)org.apache.nutch.indexer.IndexWriter:close()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endDocument() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (M)java.net.URL:getProtocol()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (O)org.apache.nutch.crawl.MapWritable:addIdEntry(byte,java.lang.Class)
M:org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text) (M)org.apache.nutch.crawl.LinkDbReader:getConf()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.lang.Thread:start()
M:org.apache.nutch.parse.ParseOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.MapWritable$KeyValueEntry:hashCode() (M)java.lang.String:hashCode()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.protocol.Content:getContent()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:getLinks()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.MapWritable:keySet() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)java.util.concurrent.atomic.AtomicLong:get()
M:org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.net.URL) (S)org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL)
M:org.apache.nutch.fetcher.Fetcher:<init>() (O)java.util.concurrent.atomic.AtomicLong:<init>(long)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.NutchWritable:get()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)java.io.DataOutputStream:writeBytes(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseImpl:getText() (M)org.apache.nutch.parse.ParseText:getText()
M:org.apache.nutch.crawl.AbstractFetchSchedule:initializeSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(int)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.Object:toString()
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:addHeader(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.tools.proxy.SegmentHandler$Segment:getContent(org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:<init>(org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.util.Progressable) (O)java.util.HashMap:<init>()
M:org.apache.nutch.tools.Benchmark:getDate() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (I)org.apache.commons.logging.Log:fatal(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.Exception:getMessage()
M:org.apache.nutch.util.URLUtil:fixEmbeddedParams(java.net.URL,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.Long:toString(long)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:warning(org.xml.sax.SAXParseException) (M)org.xml.sax.SAXParseException:toString()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:writeOutAsDuplicate(org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.parse.ParseOutputFormat$SimpleEntry:setValue(java.lang.Object) (M)org.apache.nutch.parse.ParseOutputFormat$SimpleEntry:setValue(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.util.Vector:size()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.BytesWritable:getBytes()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)java.lang.String:length()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseImpl:readFields(java.io.DataInput) (M)org.apache.nutch.parse.ParseText:readFields(java.io.DataInput)
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:hashCode() (M)org.apache.hadoop.io.MapWritable:entrySet()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.TreeMap:lastEntry()
M:org.apache.nutch.protocol.RobotRulesParser:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:shutDownActivatedPlugins() (I)java.util.Iterator:next()
M:org.apache.nutch.tools.arc.ArcInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit)
M:org.apache.nutch.util.EncodingDetector$EncodingClue:meetsThreshold() (S)org.apache.nutch.util.EncodingDetector:access$000(org.apache.nutch.util.EncodingDetector)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getQueueSize()
M:org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String) (O)org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String,java.lang.String,long)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (I)org.w3c.dom.Element:getAttribute(java.lang.String)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:close() (M)org.apache.hadoop.mapred.SequenceFileRecordReader:close()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.fetcher.Fetcher:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.SequenceFile$Reader:getValueClass()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.TextProfileSignature$Token:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.Class:getName()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)org.apache.nutch.scoring.webgraph.Loops$LoopSet:<init>()
M:org.apache.nutch.protocol.ProtocolStatus:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:addHeader(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (O)org.apache.hadoop.io.LongWritable:<init>()
M:org.apache.nutch.util.NodeWalker:skipChildren() (M)java.util.Stack:peek()
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:<init>(int,int,float) (O)org.apache.nutch.crawl.CrawlDatum:<init>(int,int)
M:org.apache.nutch.parse.HtmlParseFilters:filter(org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseResult,org.apache.nutch.parse.HTMLMetaTags,org.w3c.dom.DocumentFragment) (M)org.apache.nutch.parse.ParseResult:filter()
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.io.File:listFiles()
M:org.apache.nutch.scoring.webgraph.NodeDumper$NameType:<clinit>() (O)org.apache.nutch.scoring.webgraph.NodeDumper$NameType:<init>(java.lang.String,int)
M:org.apache.nutch.crawl.Generator$Selector:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.LinkDbReader:main(java.lang.String[]) (O)org.apache.nutch.crawl.LinkDbReader:<init>()
M:org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.util.PrefixStringMatcher:shortestMatch(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:<clinit>() (O)java.util.concurrent.atomic.AtomicInteger:<init>(int)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.util.GZIPUtils:unzip(byte[]) (O)java.io.ByteArrayInputStream:<init>(byte[])
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.AbstractFetchSchedule:shouldFetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (M)org.apache.hadoop.io.DataOutputBuffer:getLength()
M:org.apache.nutch.util.URLUtil:getProtocol(java.net.URL) (M)java.net.URL:getProtocol()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)java.util.List:add(int,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.fetcher.OldFetcher:main(java.lang.String[]) (O)org.apache.nutch.fetcher.OldFetcher:<init>()
M:org.apache.nutch.tools.FreeGenerator$FG:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)org.slf4j.Logger:trace(java.lang.String)
M:org.apache.nutch.protocol.ProtocolOutput:<init>(org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.Plugin:startUp()
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (M)org.apache.nutch.crawl.MapWritable:size()
M:org.apache.nutch.indexer.IndexerMapReduce:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.Object:toString()
M:org.apache.nutch.util.SuffixStringMatcher:<init>(java.util.Collection) (I)java.util.Iterator:next()
M:org.apache.nutch.util.EncodingDetector$EncodingClue:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)org.apache.nutch.parse.ParseText:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:setNumOutlinks(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String,java.lang.String) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Inlinks:readFields(java.io.DataInput) (S)org.apache.nutch.crawl.Inlink:read(java.io.DataInput)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.mortbay.jetty.Server:<init>()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (O)org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)java.lang.Integer:intValue()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.Generator:generateSegmentName() (O)java.util.Date:<init>(long)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:getNode()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.Extension:getExtensionInstance()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)java.util.Random:<init>()
M:org.apache.nutch.indexer.NutchDocument:getFieldNames() (I)java.util.Map:keySet()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (O)java.lang.String:<init>(byte[],int,int)
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (M)org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput)
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)org.apache.nutch.util.domain.DomainSuffixes:isDomainSuffix(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:addTiming(java.lang.String,java.lang.String,long) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (O)org.apache.nutch.tools.DmozParser:<init>()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getCrawlDatum(org.apache.hadoop.io.Text) (O)org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.plugin.ExtensionPoint:<init>(java.lang.String,java.lang.String,java.lang.String) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (M)java.util.HashMap:containsKey(java.lang.Object)
M:org.apache.nutch.util.URLUtil:getHostSegments(java.net.URL) (M)java.util.regex.Matcher:matches()
M:org.apache.nutch.indexer.IndexingException:<init>() (O)java.lang.Exception:<init>()
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (M)java.lang.reflect.Field:getModifiers()
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.apache.oro.text.regex.PatternMatcher:contains(org.apache.oro.text.regex.PatternMatcherInput,org.apache.oro.text.regex.Pattern)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (I)org.apache.nutch.protocol.Protocol:getProtocolOutput(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.protocol.Content:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,org.apache.nutch.metadata.Metadata,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.NutchWritable:get()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endDocument() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.parse.ParseSegment:main(java.lang.String[]) (O)org.apache.nutch.parse.ParseSegment:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.ObjectWritable:<init>()
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:readUrl(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:indexOf(java.lang.String)
M:org.apache.nutch.net.protocols.ProtocolException:<init>() (O)java.lang.Exception:<init>()
M:org.apache.nutch.parse.ParseStatus:<init>(java.lang.Throwable) (M)java.lang.Throwable:toString()
M:org.apache.nutch.protocol.Content:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)java.util.HashMap:<init>()
M:org.apache.nutch.net.URLNormalizerChecker:checkAll(java.lang.String) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (O)org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.nutch.tools.FreeGenerator:getConf()
M:org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.Counters$Counter:getValue()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int) (M)java.io.ByteArrayOutputStream:toByteArray()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.util.DomUtil:getDom(java.io.InputStream) (O)org.xml.sax.InputSource:<init>(java.io.InputStream)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItemQueue(java.lang.String)
M:org.apache.nutch.parse.ParseResult:filter() (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)org.apache.nutch.util.CommandRunner:setTimeout(int)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbFilter:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.plugin.PluginDescriptor:addDependency(java.lang.String) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path)
M:org.apache.nutch.metadata.MetaWrapper:<init>(org.apache.nutch.metadata.Metadata,org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (M)org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput)
M:org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:close() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.protocol.ProtocolException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.segment.SegmentMerger:<init>() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.parse.ParserChecker:getConf()
M:org.apache.nutch.crawl.MapWritable:<clinit>() (S)org.apache.nutch.crawl.MapWritable:addToMap(java.lang.Class,java.lang.Byte)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)org.slf4j.Logger:trace(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (O)java.io.LineNumberReader:<init>(java.io.Reader)
M:org.apache.nutch.util.DeflateUtils:deflate(byte[]) (O)java.util.zip.DeflaterOutputStream:<init>(java.io.OutputStream)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:generateSegmentName() (O)java.util.Date:<init>(long)
M:org.apache.nutch.parse.ParseData:<init>() (O)org.apache.hadoop.io.VersionedWritable:<init>()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.indexer.NutchDocument:getFieldValue(java.lang.String) (I)java.util.List:size()
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.String:trim()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.Generator$Selector:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.NodeList:getLength()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.OldFetcher:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:<clinit>() (O)org.apache.hadoop.mapred.lib.HashPartitioner:<init>()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.util.MimeUtil:getMimeType(java.io.File) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.servlet.ServletHandler:addFilterWithMapping(java.lang.Class,java.lang.String,int)
M:org.apache.nutch.util.URLUtil:getHostSegments(java.lang.String) (S)org.apache.nutch.util.URLUtil:getHostSegments(java.net.URL)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.util.PrefixStringMatcher:<init>(java.util.Collection) (I)java.util.Collection:iterator()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (M)java.lang.Byte:byteValue()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.Class:getName()
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (O)org.apache.nutch.crawl.MimeAdaptiveFetchSchedule$AdaptiveRate:<init>(org.apache.nutch.crawl.MimeAdaptiveFetchSchedule,java.lang.Float,java.lang.Float)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDb:getConf()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean) (M)org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String)
M:org.apache.nutch.crawl.MapWritable:addToMap(java.lang.Class,java.lang.Byte) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)crawlercommons.robots.BaseRobotRules:isAllowed(java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setPageRetrySchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long) (M)org.apache.nutch.crawl.CrawlDatum:getRetriesSinceFetch()
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)java.util.Random:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:<init>() (O)java.util.HashSet:<init>()
M:org.apache.nutch.metadata.Metadata:setAll(java.util.Properties) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.protocol.ProtocolStatus:equals(java.lang.Object) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.hadoop.fs.FileStatus:isDir()
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.ArrayList:size()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.protocol.RobotRulesParser:<clinit>() (O)java.util.Hashtable:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.metadata.Metadata:names()
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.parse.ParseStatus:toString() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.tools.DmozParser:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.mapred.FileSplit:getPath()
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:getPos() (M)org.apache.hadoop.mapred.SequenceFileRecordReader:getPos()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.crawl.CrawlDatum:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:set(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput)
M:org.apache.nutch.tools.Benchmark:getDate() (M)java.text.SimpleDateFormat:format(java.util.Date)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setScore(float)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (O)org.apache.nutch.parse.ParseException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:<clinit>() (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:createValue() (O)org.apache.nutch.metadata.MetaWrapper:<init>()
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (M)org.apache.nutch.parse.ParsePluginsReader:setFParsePluginsFile(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:hashCode() (S)java.lang.Float:floatToIntBits(float)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Generator$CrawlDbUpdater:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.tools.proxy.LogDebugHandler:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain) (M)java.lang.Throwable:toString()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.Path:getName()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.apache.nutch.tools.proxy.DelayHandler:<init>(int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseStatus:getMessage()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parseXML(java.net.URL) (M)javax.xml.parsers.DocumentBuilderFactory:newDocumentBuilder()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (S)java.util.regex.Pattern:compile(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.parse.ParserFactory:match(org.apache.nutch.plugin.Extension,java.lang.String,java.lang.String) (M)java.lang.String:matches(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.util.DomUtil:getDom(java.io.InputStream) (M)org.xml.sax.InputSource:setEncoding(java.lang.String)
M:org.apache.nutch.util.HadoopFSUtil$2:<init>(org.apache.hadoop.fs.FileSystem) (O)java.lang.Object:<init>()
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (O)org.apache.nutch.plugin.PluginRuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDbReader:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasFetchStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.Outlink:getToUrl()
M:org.apache.nutch.parse.ParseStatus:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:getConnection()
M:org.apache.nutch.segment.SegmentReader$1:run() (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.hadoop.io.FloatWritable:get()
M:org.apache.nutch.parse.ParserFactory:match(org.apache.nutch.plugin.Extension,java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.Extension:getId()
M:org.apache.nutch.parse.Outlink:skip(java.io.DataInput) (S)org.apache.hadoop.io.Text:skip(java.io.DataInput)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.plugin.PluginDescriptor:getDependencies() (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (O)org.apache.nutch.parse.ParserFactory:getExtensionFromAlias(org.apache.nutch.plugin.Extension[],java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String) (M)java.net.URL:getPath()
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.tools.ResolveUrls:access$000()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.NutchDocument:getField(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem)
M:org.apache.nutch.indexer.IndexerMapReduce:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.indexer.IndexingFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)org.apache.nutch.plugin.ExtensionPoint:getName()
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat$1:<init>(org.apache.nutch.segment.SegmentReader$TextOutputFormat,java.io.PrintStream) (O)java.lang.Object:<init>()
M:org.apache.nutch.protocol.ProtocolStatus:<init>(java.lang.Throwable) (O)org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.Object)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (M)java.util.regex.Matcher:matches()
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)org.apache.nutch.plugin.PluginRepository:getPluginDescriptor(java.lang.String)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)
M:org.apache.nutch.plugin.PluginManifestParser:<clinit>() (S)java.lang.System:getProperty(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String,int) (S)org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.parse.ParseStatus:getMinorCode()
M:org.apache.nutch.segment.SegmentMerger:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.util.MimeUtil:getMimeType(java.io.File) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.util.Random:<init>()
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (I)org.apache.nutch.net.URLNormalizer:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.regex.Pattern:matcher(java.lang.CharSequence)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.SequenceFile$Reader:getValueClass()
M:org.apache.nutch.segment.SegmentReader$6:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map) (O)java.lang.Thread:<init>()
M:org.apache.nutch.util.SuffixStringMatcher:longestMatch(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (I)org.xml.sax.Locator:getColumnNumber()
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$InputCompatMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Generator$HashComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable) (S)org.apache.nutch.crawl.Generator$HashComparator:hash(byte[],int,int)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapFile$Writer:close()
M:org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:initializeSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:setRetriesSinceFetch(int)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader$6:run() (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.MD5Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.io.MD5Hash:getDigest()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getUrl()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkRank$Counter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)java.lang.Object:getClass()
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.Character:isUpperCase(char)
M:org.apache.nutch.util.StringUtil:rightPad(java.lang.String,int) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNMapper:<clinit>() (O)org.apache.hadoop.io.FloatWritable:<init>()
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.String:charAt(int)
M:org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput) (I)java.io.DataOutput:writeFloat(float)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (M)java.lang.Class:newInstance()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Looper:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.protocol.Content:getMetadata()
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.SegmentPart:parse(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:clear()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:<init>(org.apache.hadoop.conf.Configuration) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput) (I)java.io.DataOutput:writeByte(int)
M:org.apache.nutch.parse.ParseResult:<init>(java.lang.String) (O)java.util.HashMap:<init>()
M:org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:toString() (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.metadata.MetaWrapper:<init>(org.apache.nutch.metadata.Metadata,org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.metadata.Metadata:write(java.io.DataOutput) (O)org.apache.nutch.metadata.Metadata:_getValues(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinksJakartaRegexpImpl(java.lang.String) (O)java.lang.UnsupportedOperationException:<init>(java.lang.String)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpURI:getPath()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (O)org.apache.nutch.crawl.CrawlDbReader:<init>()
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String) (M)java.lang.Class:getSimpleName()
M:org.apache.nutch.util.URLUtil:getPage(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)org.apache.nutch.fetcher.OldFetcher:getConf()
M:org.apache.nutch.parse.ParseResult:createParseResult(java.lang.String,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseResult:put(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (O)org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.HtmlParseFilters:filter(org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseResult,org.apache.nutch.parse.HTMLMetaTags,org.w3c.dom.DocumentFragment) (I)org.apache.nutch.parse.HtmlParseFilter:filter(org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseResult,org.apache.nutch.parse.HTMLMetaTags,org.w3c.dom.DocumentFragment)
M:org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:toString() (I)java.util.Set:iterator()
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.fs.FSDataInputStream:seek(long)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)org.w3c.dom.Element:getAttribute(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)org.apache.nutch.protocol.Content:getContent()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (O)org.apache.nutch.tools.DmozParser$XMLCharFilter:<init>(java.io.Reader)
M:org.apache.nutch.util.URLUtil:resolveURL(java.net.URL,java.lang.String) (M)java.lang.String:trim()
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL) (M)java.util.regex.Pattern:matcher(java.lang.CharSequence)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.Counters:getGroup(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.CommandRunner:exec() (M)java.util.concurrent.CyclicBarrier:await(long,java.util.concurrent.TimeUnit)
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.util.SuffixStringMatcher:<init>(java.util.Collection) (M)org.apache.nutch.util.SuffixStringMatcher:addPatternBackward(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:putAll(org.apache.nutch.crawl.MapWritable) (I)java.util.Set:iterator()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.util.FSUtils:closeReaders(org.apache.hadoop.io.SequenceFile$Reader[]) (M)org.apache.hadoop.io.SequenceFile$Reader:close()
M:org.apache.nutch.parse.ParseResult:isSuccess() (M)org.apache.nutch.parse.ParseResult:iterator()
M:org.apache.nutch.scoring.webgraph.WebGraph:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentMergeFilter:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:getUri()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (O)org.apache.nutch.crawl.AbstractFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)java.util.ArrayList:<init>(int)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.parse.ParseData:write(java.io.DataOutput) (I)java.io.DataOutput:writeByte(int)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (O)org.apache.nutch.parse.ParserFactory:getExtension(org.apache.nutch.plugin.Extension[],java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.apache.nutch.parse.Parse:isCanonical()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.metadata.MetaWrapper:readFields(java.io.DataInput) (O)org.apache.nutch.crawl.NutchWritable:readFields(java.io.DataInput)
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)java.util.HashMap:size()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)org.apache.nutch.util.CommandRunner:setInputStream(java.io.InputStream)
M:org.apache.nutch.util.DomUtil:getDom(java.io.InputStream) (M)org.apache.xerces.parsers.DOMParser:parse(org.xml.sax.InputSource)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.crawl.Injector:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.oro.text.regex.Perl5Matcher:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:<clinit>() (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.hadoop.io.MapFile$Reader:close()
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.Signature:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseData:<init>(org.apache.nutch.parse.ParseStatus,java.lang.String,org.apache.nutch.parse.Outlink[],org.apache.nutch.metadata.Metadata) (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.lang.String:charAt(int)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)com.ibm.icu.text.CharsetDetector:enableInputFilter(boolean)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.tools.proxy.SegmentHandler:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.parse.ParseStatus$EmptyParseImpl:<init>(org.apache.nutch.parse.ParseStatus,org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.metadata.Metadata:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.parse.ParserFactory:getExtension(org.apache.nutch.plugin.Extension[],java.lang.String,java.lang.String) (O)org.apache.nutch.parse.ParserFactory:match(org.apache.nutch.plugin.Extension,java.lang.String,java.lang.String)
M:org.apache.nutch.util.CommandRunner:exec() (O)org.apache.nutch.util.CommandRunner$PusherThread:<init>(org.apache.nutch.util.CommandRunner,java.lang.String,java.io.InputStream,java.io.OutputStream)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseStatus:getEmptyParse(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(java.lang.Object,java.lang.Object) (M)org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolOutput:getStatus()
M:org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean) (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.OldFetcher:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.fetcher.OldFetcher:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.Loops:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getHost(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.scoring.ScoringFilterException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (O)org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.Generator:generateSegmentName() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.ObjectCache:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler$SegmentPathFilter:accept(org.apache.hadoop.fs.Path) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.parse.ParseResult:put(java.lang.String,org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endDocument() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.util.MimeUtil:getMimeType(java.lang.String) (M)org.apache.tika.Tika:detect(java.lang.String)
M:org.apache.nutch.crawl.Generator:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.crawl.Generator:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.CleaningJob:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)org.apache.nutch.scoring.webgraph.Node:getNumInlinks()
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$300(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (M)java.lang.Exception:printStackTrace()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.TimingUtil:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.segment.SegmentPart:parse(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:write(java.io.DataOutput) (M)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:write(java.io.DataOutput)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setSpeculativeExecution(boolean)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:incrCounter(java.lang.String,java.lang.String,long)
M:org.apache.nutch.tools.Benchmark:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.crawl.Generator:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.HadoopFSUtil:getPassAllFilter() (O)org.apache.nutch.util.HadoopFSUtil$1:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(java.lang.Object,java.lang.Object) (M)org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (S)java.lang.Math:round(float)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (S)java.lang.Class:forName(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (O)org.apache.nutch.parse.ParseData:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String) (M)java.lang.String:substring(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem() (M)java.util.concurrent.atomic.AtomicLong:get()
M:org.apache.nutch.parse.ParseStatus:<init>(int,int,java.lang.String[]) (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)java.util.ArrayList:get(int)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (S)java.lang.Math:round(float)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler$SegmentPathFilter:<clinit>() (O)org.apache.nutch.tools.proxy.SegmentHandler$SegmentPathFilter:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.metadata.Metadata:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum) (I)java.util.Iterator:next()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.lang.String:concat(java.lang.String)
M:org.apache.nutch.crawl.Inlinks:write(java.io.DataOutput) (M)org.apache.nutch.crawl.Inlink:write(java.io.DataOutput)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (M)org.xml.sax.SAXParseException:getMessage()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.apache.nutch.tools.proxy.NotFoundHandler:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setPageGoneSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:size()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:setLinkType(byte)
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (O)java.io.InputStreamReader:<init>(java.io.InputStream)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.util.HashMap:remove(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.Generator$Selector:<init>() (O)org.apache.hadoop.io.FloatWritable:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:remove()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (O)org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.ParseText:readFields(java.io.DataInput) (O)org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.FloatWritable:set(float)
M:org.apache.nutch.plugin.PluginRepository:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkTimelimit()
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.Injector$InjectReducer:<init>() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.nutch.indexer.CleaningJob:getConf()
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (M)java.io.Writer:write(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.SuffixStringMatcher:<init>(java.util.Collection) (I)java.util.Collection:iterator()
M:org.apache.nutch.parse.Outlink:read(java.io.DataInput) (M)org.apache.nutch.parse.Outlink:readFields(java.io.DataInput)
M:org.apache.nutch.util.CommandRunner$PullerThread:<init>(org.apache.nutch.util.CommandRunner,java.lang.String,java.io.InputStream,java.io.OutputStream) (O)org.apache.nutch.util.CommandRunner$PumperThread:<init>(org.apache.nutch.util.CommandRunner,java.lang.String,java.io.InputStream,java.io.OutputStream,boolean)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.protocol.ProtocolFactory:contains(java.lang.String,java.lang.String) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.util.URLUtil:getDomainName(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.Class:getSimpleName()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (O)org.apache.nutch.segment.SegmentPart:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (O)org.apache.nutch.net.URLNormalizerChecker:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)org.apache.nutch.plugin.PluginRuntimeException:toString()
M:org.apache.nutch.util.CommandRunner:exec() (O)org.apache.nutch.util.CommandRunner$PullerThread:<init>(org.apache.nutch.util.CommandRunner,java.lang.String,java.io.InputStream,java.io.OutputStream)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.segment.SegmentReader$InputCompatMapper:<init>() (O)org.apache.hadoop.mapred.MapReduceBase:<init>()
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.nutch.segment.SegmentReader:createJobConf()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:getUrl()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolStatus:getName()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.io.ByteArrayOutputStream:write(byte[],int,int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:compareTo(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum$Comparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.CrawlDatum$Comparator:readLong(byte[],int)
M:org.apache.nutch.plugin.PluginClassLoader:<init>(java.net.URL[],java.lang.ClassLoader) (O)java.net.URLClassLoader:<init>(java.net.URL[],java.lang.ClassLoader)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:compareTo(java.lang.Object) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:compareTo(org.apache.nutch.util.TrieStringMatcher$TrieNode)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.protocol.Content:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (M)java.util.HashMap:containsKey(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.CrawlDbReader:readUrl(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentReader$1:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map) (O)java.lang.Thread:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.parse.ParsePluginList:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (M)java.util.LinkedList:size()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.parse.ParseResult:isSuccess() (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (M)org.apache.nutch.protocol.Content:getMetadata()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getInlinkScore()
M:org.apache.nutch.util.StringUtil:toHexString(byte[],java.lang.String,int) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.parse.ParseImpl:readFields(java.io.DataInput) (O)org.apache.nutch.parse.ParseText:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserFactory:getExtensions(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (O)java.util.HashMap:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (M)java.lang.reflect.Field:get(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:add(java.lang.Object)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)java.lang.String:indexOf(int)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setInt(java.lang.String,int)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (O)org.apache.hadoop.conf.Configuration:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.util.MimeUtil:forName(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Inlink:read(java.io.DataInput) (M)org.apache.nutch.crawl.Inlink:readFields(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.NodeDumper$NameType:<init>(java.lang.String,int) (O)java.lang.Enum:<init>(java.lang.String,int)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:trim()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (I)java.util.Collection:iterator()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:toString()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.fs.Path:getName()
M:org.apache.nutch.crawl.Generator$Selector:getPartition(java.lang.Object,java.lang.Object,int) (M)org.apache.nutch.crawl.Generator$Selector:getPartition(org.apache.hadoop.io.FloatWritable,org.apache.hadoop.io.Writable,int)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.concurrent.atomic.AtomicInteger:get()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.apache.nutch.tools.proxy.FakeHandler:<init>()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)java.util.Iterator:next()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getContent(org.apache.hadoop.io.Text) (O)org.apache.nutch.tools.proxy.SegmentHandler$Segment:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable)
M:org.apache.nutch.parse.OutlinkExtractor:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.crawl.FetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (S)java.util.Collections:synchronizedSet(java.util.Set)
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int) (M)java.util.zip.GZIPInputStream:read(byte[])
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (M)org.apache.nutch.util.SuffixStringMatcher:matches(java.lang.String)
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (M)java.lang.Exception:toString()
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (M)org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.protocol.Content:read(java.io.DataInput) (O)org.apache.nutch.protocol.Content:<init>()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (I)java.util.List:iterator()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilters:injectedScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.nutch.crawl.TextProfileSignature$Token:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)java.util.Iterator:next()
M:org.apache.nutch.plugin.PluginManifestParser:parseLibraries(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.protocol.Content:read(java.io.DataInput) (M)org.apache.nutch.protocol.Content:readFields(java.io.DataInput)
M:org.apache.nutch.segment.SegmentReader$5:run() (S)org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.parse.ParseText:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.servlet.ServletHandler:addServletWithMapping(java.lang.Class,java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasOptionalArg()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.crawl.LinkDb:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.parse.ParseSegment:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.io.Writer:flush()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.parse.ParseStatus:<init>(int) (O)org.apache.nutch.parse.ParseStatus:<init>(int,int,java.lang.String[])
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexWriter:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.tools.arc.ArcInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)java.lang.Object:toString()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.crawl.Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.protocol.ProtocolStatus:<clinit>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (S)org.apache.nutch.util.EncodingDetector$EncodingClue:access$200(org.apache.nutch.util.EncodingDetector$EncodingClue)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:incrCounter(java.lang.String,java.lang.String,long)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getOutlinkScore()
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.hadoop.io.Text:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Initializer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.indexer.NutchDocument:getField(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (O)org.apache.nutch.crawl.MapWritable$KeyValueEntry:<init>(org.apache.nutch.crawl.MapWritable,org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.protocol.Protocol:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.util.URLUtil:isSameDomainName(java.lang.String,java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.apache.hadoop.mapred.Reporter:incrCounter(java.lang.String,java.lang.String,long)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:error(org.xml.sax.SAXParseException) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.util.ArrayList:iterator()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.protocol.ProtocolStatus:getName() (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.tools.proxy.DelayHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.tools.proxy.DelayHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:<init>(org.apache.hadoop.conf.Configuration) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:read() (M)java.io.Reader:read()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.protocol.Content:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.SignatureFactory:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)org.apache.hadoop.io.ArrayFile$Reader:close()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object)
M:org.apache.nutch.metadata.Metadata:<init>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)java.util.HashMap:values()
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getModifiedTime()
M:org.apache.nutch.metadata.Metadata:names() (I)java.util.Map:keySet()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher$QueueFeeder:setTimeLimit(long)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)java.io.PrintStream:print(java.lang.String)
M:org.apache.nutch.tools.proxy.DelayHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.indexer.NutchIndexAction:<init>(org.apache.nutch.indexer.NutchDocument,byte) (O)java.lang.Object:<init>()
M:org.apache.nutch.protocol.ProtocolStatus:readFields(java.io.DataInput) (O)org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:<init>(org.apache.hadoop.mapred.RecordReader,org.apache.nutch.fetcher.Fetcher$FetchItemQueues,int) (M)org.apache.nutch.fetcher.Fetcher$QueueFeeder:setDaemon(boolean)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.parse.ParseSegment:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseSegment:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setNumReduceTasks(int)
M:org.apache.nutch.util.StringUtil:fromHexString(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getNumOutlinks()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (M)java.lang.Object:getClass()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endElement(java.lang.String,java.lang.String,java.lang.String) (M)java.lang.StringBuffer:delete(int,int)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startDocument() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)org.apache.hadoop.fs.FSDataInputStream:read(byte[])
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL) (M)java.net.URL:getHost()
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.HadoopFSUtil:getPassAllFilter()
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.NodeWalker:nextNode() (M)java.util.Stack:add(java.lang.Object)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkRank$Counter:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int)
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlinks:clear()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader$3:run() (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (M)org.apache.nutch.indexer.NutchField:write(java.io.DataOutput)
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (O)java.io.DataInputStream:<init>(java.io.InputStream)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.Content:getContentType()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (I)org.slf4j.Logger:isTraceEnabled()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.parse.ParseResult:put(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.NutchWritable) (M)org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (O)org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (S)java.lang.Math:round(float)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.SuffixStringMatcher:<init>(java.util.Collection) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (S)org.apache.nutch.util.EncodingDetector$EncodingClue:access$300(org.apache.nutch.util.EncodingDetector$EncodingClue)
M:org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseSegment:getConf()
M:org.apache.nutch.parse.ParseResult:createParseResult(java.lang.String,org.apache.nutch.parse.Parse) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.parse.ParseCallable:call() (M)org.apache.nutch.parse.ParseCallable:call()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:generateSegmentName() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginManifestParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository)
M:org.apache.nutch.crawl.LinkDb:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.util.UUID:toString()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:close()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (I)java.util.List:iterator()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getFloat(java.lang.String,float)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (I)java.util.Set:iterator()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (O)org.apache.nutch.parse.ParseStatus:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Float:parseFloat(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.parse.ParseOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (O)org.apache.hadoop.mapred.InvalidJobConfException:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.nutch.segment.SegmentReader:getConf()
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (S)org.apache.nutch.util.CommandRunner:access$100(org.apache.nutch.util.CommandRunner)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:close() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentReader:<init>(org.apache.hadoop.conf.Configuration,boolean,boolean,boolean,boolean,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:setUrl(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.fetcher.FetcherOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:createValue() (M)org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:createValue()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.protocol.Content:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,org.apache.nutch.metadata.Metadata,org.apache.hadoop.conf.Configuration) (O)java.lang.IllegalArgumentException:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String) (S)org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int)
M:org.apache.nutch.parse.ParseSegment:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (I)java.util.Iterator:next()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (O)org.apache.nutch.fetcher.Fetcher:checkConfiguration()
M:org.apache.nutch.metadata.Metadata:setAll(java.util.Properties) (I)java.util.Enumeration:hasMoreElements()
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (I)org.slf4j.Logger:info(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getRetriesSinceFetch()
M:org.apache.nutch.plugin.PluginManifestParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository) (O)java.lang.Object:<init>()
M:org.apache.nutch.protocol.Content:write(java.io.DataOutput) (I)java.io.DataOutput:write(byte[])
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String,java.lang.String,long)
M:org.apache.nutch.crawl.URLPartitioner:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginClass()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.MapWritable:values() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)org.apache.nutch.util.CommandRunner:getExitValue()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)org.w3c.dom.Element:getElementsByTagName(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String) (M)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (S)java.util.Arrays:sort(java.lang.Object[])
M:org.apache.nutch.util.GZIPUtils:unzip(byte[]) (M)java.io.ByteArrayOutputStream:write(byte[],int,int)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:<init>() (O)org.apache.hadoop.mapred.MapReduceBase:<init>()
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:normalizeUrl(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.indexer.NutchDocument:toString() (I)java.util.Map:entrySet()
M:org.apache.nutch.segment.SegmentPart:parse(java.lang.String) (M)java.lang.String:substring(int)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.io.File:exists()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.tools.Benchmark$BenchmarkResults:<init>()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:get()
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:handle(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,int) (S)org.mortbay.jetty.HttpConnection:getCurrentConnection()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.FloatWritable:get()
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (M)java.net.URI:toString()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (M)org.apache.nutch.crawl.LinkDb:getConf()
M:org.apache.nutch.plugin.ExtensionPoint:addExtension(org.apache.nutch.plugin.Extension) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:toString() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.URLPartitioner:configure(org.apache.hadoop.mapred.JobConf) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (O)org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content)
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.segment.SegmentReader:getConf()
M:org.apache.nutch.fetcher.Fetcher:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.fetcher.Fetcher:isParsing(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseStatus$EmptyParseImpl:<init>(org.apache.nutch.parse.ParseStatus,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.ParseData:<init>(org.apache.nutch.parse.ParseStatus,java.lang.String,org.apache.nutch.parse.Outlink[],org.apache.nutch.metadata.Metadata,org.apache.nutch.metadata.Metadata)
M:org.apache.nutch.crawl.Inlinks:getAnchors() (O)java.util.HashMap:<init>()
M:org.apache.nutch.parse.ParsePluginList:<init>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.scoring.webgraph.Loops:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:hashCode() (M)java.lang.Object:hashCode()
M:org.apache.nutch.util.PrefixStringMatcher:longestMatch(java.lang.String) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (I)org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getSignature()
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (I)java.util.List:iterator()
M:org.apache.nutch.segment.SegmentReader$InputCompatMapper:<init>() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)java.lang.Math:round(float)
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.Injector:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.crawl.Injector:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (I)java.util.Map:remove(java.lang.Object)
M:org.apache.nutch.crawl.Generator$GeneratorOutputFormat:generateFileNameForKeyValue(org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator$SelectorEntry,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexWriters:describe() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNMapper:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseSegment:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (O)java.io.InputStreamReader:<init>(java.io.InputStream)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.util.TrieStringMatcher:addPatternBackward(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasFetchStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.Injector$InjectReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getFloat(java.lang.String,float)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.DeduplicationJob$DedupReducer:writeOutAsDuplicate(org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlinks:iterator()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher$QueueFeeder:isAlive()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)org.apache.nutch.crawl.LinkDbReader:init(org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParserFactory:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDatum:putAllMetaData(org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:readUrl(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbFilter:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)java.net.URL:getHost()
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat$1:write(java.lang.Object,java.lang.Object) (M)org.apache.nutch.segment.SegmentReader$TextOutputFormat$1:write(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.Outlink:getToUrl()
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.DeduplicationJob:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (O)org.apache.nutch.parse.ParserFactory:getExtension(org.apache.nutch.plugin.Extension[],java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$100(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.TextProfileSignature:<init>() (O)org.apache.nutch.crawl.MD5Signature:<init>()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.crawl.Generator$SelectorEntry:write(java.io.DataOutput) (M)org.apache.hadoop.io.Text:write(java.io.DataOutput)
M:org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean) (M)java.lang.String:length()
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.CrawlDbFilter:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.fetcher.OldFetcher:<init>() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (M)java.util.regex.Matcher:matches()
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.StringUtil:toHexString(byte[],java.lang.String,int) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (M)org.apache.nutch.crawl.MapWritable$KeyValueEntry:equals(java.lang.Object)
M:org.apache.nutch.crawl.Generator$HashComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable) (M)org.apache.hadoop.io.Text:getLength()
M:org.apache.nutch.scoring.webgraph.LinkDatum:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.io.PrintWriter:close()
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.protocol.Content:getContentType()
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:write(java.io.DataOutput) (I)java.util.Set:size()
M:org.apache.nutch.tools.proxy.SegmentHandler:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.util.DomUtil:getDom(java.io.InputStream) (M)org.apache.xerces.parsers.DOMParser:getDocument()
M:org.apache.nutch.util.MimeUtil:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long) (M)org.apache.nutch.crawl.Generator:getConf()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.Integer:intValue()
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (O)org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (M)org.apache.nutch.plugin.Extension:getDescriptor()
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseData:getOutlinks()
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.util.HadoopFSUtil$2:accept(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileStatus:isDir()
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (M)org.apache.nutch.util.PrefixStringMatcher:matches(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.NodeDumper$AggrType:<clinit>() (O)org.apache.nutch.scoring.webgraph.NodeDumper$AggrType:<init>(java.lang.String,int)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text) (M)java.lang.String:replaceAll(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:length()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (S)java.util.Collections:synchronizedList(java.util.List)
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (S)org.apache.nutch.util.domain.DomainSuffixes:getInstance()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.lang.StringBuffer:length()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (S)org.apache.nutch.util.URLUtil:toASCII(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.metadata.Metadata:toString() (M)org.apache.nutch.metadata.Metadata:names()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.util.URLUtil:getHostSegments(java.net.URL) (M)java.util.regex.Pattern:matcher(java.lang.CharSequence)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.plugin.PluginManifestParser:parseRequires(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getAttribute(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.segment.SegmentReader$InputCompatMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.Object:toString()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:filterUrl(java.lang.String) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.nutch.crawl.Generator:getConf()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)org.apache.nutch.util.CommandRunner:setStdErrorStream(java.io.OutputStream)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.parse.ParseSegment:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String) (I)javax.servlet.http.HttpServletResponse:addHeader(java.lang.String,java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.nutch.metadata.MetaWrapper:get()
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (O)org.apache.nutch.segment.SegmentReader$4:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)java.lang.Math:min(int,int)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (M)org.apache.nutch.protocol.Content:getContent()
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.String[]) (O)java.lang.Object:<init>()
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.io.MapWritable:putAll(java.util.Map)
M:org.apache.nutch.crawl.URLPartitioner:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getPage(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkTimelimit() (M)java.util.concurrent.atomic.AtomicInteger:get()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.bio.SocketConnector:setPort(int)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.crawl.Injector:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.indexer.NutchDocument:<init>() (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)java.io.IOException:toString()
M:org.apache.nutch.crawl.CrawlDbFilter:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (I)java.util.Collection:iterator()
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (I)org.apache.nutch.crawl.FetchSchedule:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Inlink:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (M)com.google.common.util.concurrent.ThreadFactoryBuilder:build()
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)org.apache.nutch.scoring.webgraph.LinkRank:getConf()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.indexer.IndexerMapReduce:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:generateSegmentName() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.nutch.crawl.CrawlDbMerger:getConf()
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (O)java.util.zip.InflaterInputStream:<init>(java.io.InputStream,java.util.zip.Inflater)
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:set(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:<clinit>() (O)org.apache.hadoop.io.LongWritable:<init>(long)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (S)org.apache.tika.config.TikaConfig:getDefaultConfig()
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String,int) (O)org.apache.nutch.util.EncodingDetector$EncodingClue:<init>(org.apache.nutch.util.EncodingDetector,java.lang.String,java.lang.String,int)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlinks:add(org.apache.nutch.crawl.Inlink)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.parse.ParseResult:filter() (I)java.util.Iterator:next()
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.lang.String:trim()
M:org.apache.nutch.crawl.Inlink:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.segment.ContentAsTextInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)java.lang.Object:toString()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDbReader:readUrl(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilterException:getMessage()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.Outlink:setUrl(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.concurrent.atomic.AtomicLong:get()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkRank:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.LongWritable:get()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.fetcher.OldFetcher:access$800(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.crawl.Injector:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseData:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (I)java.util.List:iterator()
M:org.apache.nutch.indexer.IndexWriters:write(org.apache.nutch.indexer.NutchDocument) (I)org.apache.nutch.indexer.IndexWriter:write(org.apache.nutch.indexer.NutchDocument)
M:org.apache.nutch.parse.ParseSegment:run(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.parse.ParsePluginList:getSupportedMimeTypes() (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)java.net.InetAddress:getByName(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:<clinit>() (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.tools.arc.ArcRecordReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)org.apache.hadoop.io.ArrayFile$Reader:close()
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (M)java.io.OutputStream:close()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.Class:getName()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:close() (M)org.apache.hadoop.mapred.SequenceFileRecordReader:close()
M:org.apache.nutch.parse.Outlink:write(java.io.DataOutput) (I)java.io.DataOutput:writeBoolean(boolean)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getUrl()
M:org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.parse.ParseData:<init>(org.apache.nutch.parse.ParseStatus,java.lang.String,org.apache.nutch.parse.Outlink[],org.apache.nutch.metadata.Metadata,org.apache.nutch.metadata.Metadata) (O)org.apache.hadoop.io.VersionedWritable:<init>()
M:org.apache.nutch.parse.ParseImpl:read(java.io.DataInput) (M)org.apache.nutch.parse.ParseImpl:readFields(java.io.DataInput)
M:org.apache.nutch.parse.ParseOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (O)java.util.HashMap:<init>()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:setContentType(java.lang.String)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (O)org.apache.nutch.util.EncodingDetector$EncodingClue:<init>(org.apache.nutch.util.EncodingDetector,java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setPageRetrySchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long) (M)org.apache.nutch.crawl.CrawlDatum:setRetriesSinceFetch(int)
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (M)org.apache.nutch.crawl.CrawlDatum:setSignature(byte[])
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasOptionalArg()
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:getExportedLibUrls()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)java.lang.Object:toString()
M:org.apache.nutch.net.URLNormalizerChecker:checkAll(java.lang.String) (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.scoring.AbstractScoringFilter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setNumReduceTasks(int)
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.Inlink:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:toString() (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:addTiming(java.lang.String,java.lang.String,long) (I)java.util.List:contains(java.lang.Object)
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (M)java.lang.Class:newInstance()
M:org.apache.nutch.indexer.IndexingFiltersChecker:main(java.lang.String[]) (O)org.apache.nutch.indexer.IndexingFiltersChecker:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)java.util.HashSet:size()
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL) (M)java.util.regex.Matcher:matches()
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (M)org.apache.nutch.plugin.PluginDescriptor:getExtenstionPoints()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.hadoop.io.Text:equals(java.lang.Object)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilters:passScoreAfterParsing(org.apache.hadoop.io.Text,org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getQueueCount()
M:org.apache.nutch.scoring.ScoringFilters:indexerScore(org.apache.hadoop.io.Text,org.apache.nutch.indexer.NutchDocument,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.parse.Parse,org.apache.nutch.crawl.Inlinks,float) (I)org.apache.nutch.scoring.ScoringFilter:indexerScore(org.apache.hadoop.io.Text,org.apache.nutch.indexer.NutchDocument,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.parse.Parse,org.apache.nutch.crawl.Inlinks,float)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.crawl.CrawlDatum:setScore(float)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.Text:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$InputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int) (M)org.apache.hadoop.fs.FileStatus:getPath()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDatum:compareTo(java.lang.Object) (M)org.apache.nutch.crawl.CrawlDatum:compareTo(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.URLUtil:getHostSegments(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)java.lang.reflect.Constructor:newInstance(java.lang.Object[])
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:get(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.crawl.CrawlDbReader:openReaders(java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)org.apache.hadoop.fs.Path:toString()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.util.Set:iterator()
M:org.apache.nutch.tools.FreeGenerator:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)java.util.List:size()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.FloatWritable:set(float)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:set(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.segment.SegmentPart:parse(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Generator$SelectorEntry:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDbFilter:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.parse.ParseResult:filter() (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (I)java.util.Map:keySet()
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (S)java.lang.reflect.Modifier:isStatic(int)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)java.util.HashSet:contains(java.lang.Object)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (M)java.nio.charset.Charset:name()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.indexer.NutchDocument:toString() (I)java.util.Set:iterator()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Node:getNodeName()
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)org.apache.nutch.crawl.CrawlDatum:<init>(int,int)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:getOutputStream()
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parseXML(java.net.URL) (M)javax.xml.parsers.DocumentBuilder:parse(java.io.InputStream)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:calculateLastFetchTime(org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.protocol.Content:getUrl()
M:org.apache.nutch.util.LockUtil:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (M)org.apache.nutch.metadata.Metadata:write(java.io.DataOutput)
M:org.apache.nutch.segment.SegmentPart:parse(java.lang.String) (O)org.apache.nutch.segment.SegmentPart:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.CrawlDatum:metadataEquals(org.apache.hadoop.io.MapWritable) (M)java.util.HashSet:equals(java.lang.Object)
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setPageGoneSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.indexer.NutchDocument:add(java.lang.String,java.lang.Object) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.TextProfileSignature$Token:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.StringUtil:fromHexString(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (S)org.apache.nutch.fetcher.OldFetcher:access$508(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.crawl.Inlinks:write(java.io.DataOutput) (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$400(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:normalizeUrl(java.lang.String) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (M)org.apache.nutch.plugin.Extension:getClazz()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginRepository:shutDownActivatedPlugins() (M)org.apache.nutch.plugin.Plugin:shutDown()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.StringUtil:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.protocol.RobotRulesParser:<clinit>() (O)crawlercommons.robots.SimpleRobotRulesParser:<init>()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.metadata.MetaWrapper:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:configure(org.apache.hadoop.mapred.JobConf) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.segment.SegmentMerger$ObjectInputFormat$1:<init>(org.apache.nutch.segment.SegmentMerger$ObjectInputFormat,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.SequenceFileRecordReader,org.apache.hadoop.io.Writable,java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.Object) (O)org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.Object,long)
M:org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String,java.lang.String,java.lang.String) (O)org.apache.nutch.parse.ParseException:<init>(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseData:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.Generator$GeneratorOutputFormat:generateFileNameForKeyValue(org.apache.hadoop.io.FloatWritable,org.apache.nutch.crawl.Generator$SelectorEntry,java.lang.String) (M)org.apache.hadoop.io.IntWritable:toString()
M:org.apache.nutch.protocol.ProtocolStatus:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:toArray(java.lang.Object[])
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.tools.FreeGenerator:access$000()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.LinkedHashMap:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseData:getContentMeta()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getUrl()
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.Outlink:toString() (O)java.lang.StringBuffer:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseResult:put(java.lang.String,org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (M)java.util.ResourceBundle:getString(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$2:run() (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (O)org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)
M:org.apache.nutch.metadata.MetaWrapper:getMeta(java.lang.String) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.util.StringUtil:leftPad(java.lang.String,int) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String) (I)org.apache.nutch.net.URLNormalizer:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MD5Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.nutch.protocol.Content:getUrl()
M:org.apache.nutch.parse.ParseText:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:openReaders(java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (M)org.apache.nutch.util.SuffixStringMatcher:shortestMatch(java.lang.String)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.apache.nutch.tools.proxy.LogDebugHandler:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$102(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path)
M:org.apache.nutch.indexer.NutchDocument:add(java.lang.String,java.lang.Object) (O)org.apache.nutch.indexer.NutchField:<init>(java.lang.Object)
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (O)java.util.HashMap:<init>()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:<init>(org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.parse.Outlink:readFields(java.io.DataInput) (I)java.io.DataInput:readBoolean()
M:org.apache.nutch.crawl.MapWritable:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.util.zip.GZIPInputStream:read(byte[],int,int)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String,java.lang.String,long) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.nutch.metadata.MetaWrapper:getMeta(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (O)java.util.HashSet:<init>()
M:org.apache.nutch.crawl.Generator:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:addTiming(java.lang.String,java.lang.String,long) (O)java.util.HashMap:<init>()
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.lang.StringBuilder:length()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:setSignature(byte[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:read(char[],int,int) (S)org.apache.xerces.util.XMLChar:isValid(int)
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:configure(org.apache.hadoop.mapred.JobConf) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.Content:getContentType()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:close() (O)org.apache.nutch.tools.proxy.SegmentHandler$Segment:closeReaders(org.apache.hadoop.io.MapFile$Reader[])
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (I)java.io.DataInput:readFully(byte[])
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.Counters:findCounter(java.lang.String,java.lang.String)
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int) (O)java.io.ByteArrayOutputStream:<init>(int)
M:org.apache.nutch.parse.ParseOutputFormat$SimpleEntry:<init>(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parseManifestFile(java.lang.String) (M)java.io.File:toURI()
M:org.apache.nutch.net.URLFilterChecker:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getQueueSize()
M:org.apache.nutch.crawl.Inlinks:getAnchors() (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.tools.arc.ArcSegmentCreator:generateSegmentName()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (I)java.util.Collection:iterator()
M:org.apache.nutch.net.URLFilterException:<init>() (O)java.lang.Exception:<init>()
M:org.apache.nutch.indexer.IndexingFiltersChecker:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (I)java.util.List:iterator()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setReducerClass(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolStatus:getMessage()
M:org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.util.EncodingDetector$EncodingClue:<init>(org.apache.nutch.util.EncodingDetector,java.lang.String,java.lang.String) (O)org.apache.nutch.util.EncodingDetector$EncodingClue:<init>(org.apache.nutch.util.EncodingDetector,java.lang.String,java.lang.String,int)
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlinks:size()
M:org.apache.nutch.util.NutchConfiguration:create() (S)org.apache.nutch.util.NutchConfiguration:addNutchResources(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (I)java.util.List:iterator()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (S)java.util.Collections:emptyList()
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (M)org.apache.nutch.plugin.PluginDescriptor:getExtensions()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (M)java.lang.Exception:printStackTrace()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.protocol.Content:getMetadata()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (M)org.xml.sax.SAXParseException:toString()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtensionPoints(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:addExtensionPoint(org.apache.nutch.plugin.ExtensionPoint)
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (O)java.util.Vector:<init>(int)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)java.io.ByteArrayOutputStream:write(byte[])
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (M)org.apache.nutch.tools.ResolveUrls:resolveUrls()
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.Inlinks:readFields(java.io.DataInput) (M)java.util.HashSet:clear()
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (O)org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.plugin.PluginDescriptor:getExportedLibUrls() (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.nutch.indexer.IndexWriters:commit()
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)org.apache.nutch.indexer.IndexWriters:describe()
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:add(java.lang.Object)
M:org.apache.nutch.parse.ParserFactory:getExtensions(java.lang.String) (O)org.apache.nutch.parse.ParserFactory:findExtensions(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:getFieldValue(java.lang.String) (M)org.apache.nutch.indexer.NutchField:getValues()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)java.util.List:toArray(java.lang.Object[])
M:org.apache.nutch.protocol.ProtocolStatus:write(java.io.DataOutput) (I)java.io.DataOutput:writeByte(int)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text) (O)org.apache.nutch.crawl.Inlinks:<init>()
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (M)java.net.URL:getQuery()
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.Outlink:toString() (M)java.lang.StringBuffer:append(java.lang.Object)
M:org.apache.nutch.parse.ParseResult:isEmpty() (I)java.util.Map:isEmpty()
M:org.apache.nutch.parse.HTMLMetaTags:<init>() (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.LongWritable:get()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.protocol.Content:getMetadata()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.NodeDumper:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (O)org.apache.nutch.plugin.Extension:<init>(org.apache.nutch.plugin.PluginDescriptor,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setSpeculativeExecution(boolean)
M:org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String) (M)java.lang.String:lastIndexOf(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.Inlinks:iterator() (M)java.util.HashSet:iterator()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseStatus:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtensionPoints(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.NodeList:item(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (O)java.util.HashMap:<init>()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.bio.SocketConnector:setResolveNames(boolean)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (M)org.apache.nutch.plugin.Extension:getId()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$300(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)java.io.PrintStream:<init>(java.io.OutputStream)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.InlinkPriorityQueue:<init>(int) (O)org.apache.hadoop.util.PriorityQueue:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.util.HashSet:iterator()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.Object:getClass()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.Exception:printStackTrace()
M:org.apache.nutch.scoring.webgraph.Node:toString() (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)org.apache.nutch.protocol.Content:getUrl()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.nutch.scoring.webgraph.LinkDumper:getConf()
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)java.lang.StringBuffer:append(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.nutch.segment.SegmentReader:getConf()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.io.File:getAbsolutePath()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Route:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.util.SuffixStringMatcher:matches(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:error(org.xml.sax.SAXParseException) (M)org.xml.sax.SAXParseException:toString()
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)java.util.HashMap:<init>()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.ArrayList:<init>(int)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.Object:getClass()
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilterException:getMessage()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.parse.ParseStatus:getMessage()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParserFactory:escapeContentType(java.lang.String) (M)java.lang.String:replace(java.lang.CharSequence,java.lang.CharSequence)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump() (I)java.util.Map:keySet()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.util.HashMap:keySet()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)org.apache.nutch.crawl.CrawlDatum:<init>(int,int,float)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$6:run() (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.ParseStatus:toString() (S)java.lang.String:valueOf(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.fetcher.Fetcher$QueueFeeder:<init>(org.apache.hadoop.mapred.RecordReader,org.apache.nutch.fetcher.Fetcher$FetchItemQueues,int)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.Inlinks:<init>()
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginDescriptor:setPluginId(java.lang.String)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (M)org.apache.nutch.plugin.PluginRepository:getCachedClass(org.apache.nutch.plugin.PluginDescriptor,java.lang.String)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL) (S)org.apache.nutch.util.domain.DomainSuffixes:getInstance()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getTotalSize()
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.segment.SegmentPart:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDbFilter:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.tools.FreeGenerator:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.plugin.PluginDescriptor:addExtension(org.apache.nutch.plugin.Extension) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)org.apache.nutch.crawl.TextProfileSignature:getConf()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (M)java.lang.Integer:intValue()
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:iterator()
M:org.apache.nutch.scoring.webgraph.LinkDumper:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FSDataInputStream:close()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.BooleanWritable:get()
M:org.apache.nutch.crawl.CrawlDbReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.util.StringUtil:rightPad(java.lang.String,int) (M)java.lang.String:length()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getRetriesSinceFetch()
M:org.apache.nutch.indexer.CleaningJob$DBFilter:<init>() (O)org.apache.hadoop.io.ByteWritable:<init>(byte)
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.nutch.crawl.DeduplicationJob:getConf()
M:org.apache.nutch.util.CommandRunner:exec() (M)org.apache.nutch.util.CommandRunner$PusherThread:start()
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.IntWritable:<init>(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.nutch.scoring.webgraph.LinkRank:getConf()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (I)java.util.List:toArray(java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.util.MimeUtil:forName(java.lang.String) (M)org.apache.tika.mime.MimeTypeException:getMessage()
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (M)org.apache.nutch.plugin.ExtensionPoint:getId()
M:org.apache.nutch.indexer.IndexerOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.IndexWriters:close()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)java.util.Properties:get(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.hadoop.io.MapWritable:entrySet()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)java.util.List:iterator()
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.protocol.RobotRulesParser:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (I)java.util.Set:iterator()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (O)org.apache.nutch.util.SuffixStringMatcher:<init>(java.lang.String[])
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:<init>(java.io.DataOutputStream) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (S)org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (I)java.util.ListIterator:hasNext()
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.hadoop.io.MapWritable:containsKey(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$4:run() (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:characters(char[],int,int) (M)java.lang.StringBuffer:append(char[],int,int)
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:setNumInlinks(int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:get(int)
M:org.apache.nutch.crawl.CrawlDatum:toString() (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.util.concurrent.atomic.AtomicInteger:decrementAndGet()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (I)java.util.Collection:iterator()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean) (M)org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String)
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (M)java.lang.Byte:byteValue()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.ProtocolOutput:getStatus()
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.scoring.webgraph.Loops$Finalizer:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:reduce(org.apache.hadoop.io.ByteWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat:<init>() (O)org.apache.hadoop.mapred.FileOutputFormat:<init>()
M:org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.parse.ParseText:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:get()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.nutch.scoring.webgraph.NodeDumper:getConf()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (M)org.apache.nutch.util.GenericWritableConfigurable:get()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.FreeGenerator$FG:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.crawl.MapWritable:addIdEntry(byte,java.lang.Class) (O)org.apache.nutch.crawl.MapWritable$ClassIdEntry:<init>(org.apache.nutch.crawl.MapWritable,byte,java.lang.Class)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.util.TreeMap:get(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean)
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.parse.ParseOutputFormat:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$InputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int) (O)org.apache.hadoop.mapred.FileSplit:<init>(org.apache.hadoop.fs.Path,long,long,java.lang.String[])
M:org.apache.nutch.plugin.MissingDependencyException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.util.DeflateUtils:deflate(byte[]) (M)java.io.ByteArrayOutputStream:toByteArray()
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.nutch.crawl.LinkDbReader:init(org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.Generator$Selector:<init>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (O)org.apache.hadoop.mapred.lib.HashPartitioner:<init>()
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:set(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)org.apache.nutch.scoring.webgraph.LoopReader:getConf()
M:org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput) (O)org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:toString()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (O)org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.ProtocolNotFound:<init>(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.OldFetcher$FetcherThread:start()
M:org.apache.nutch.segment.SegmentPart:parse(java.lang.String) (M)java.lang.String:indexOf(int)
M:org.apache.nutch.plugin.PluginRepository:finalize() (O)org.apache.nutch.plugin.PluginRepository:shutDownActivatedPlugins()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (O)java.util.TreeMap:<init>()
M:org.apache.nutch.plugin.PluginDescriptor:getDependencyLibs() (O)org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.Long:toString(long)
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.fetcher.Fetcher:getConf()
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.SignatureFactory:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (O)org.apache.nutch.util.CommandRunner:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.parse.ParseResult:get(org.apache.hadoop.io.Text)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDatum:toString() (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.metadata.MetaWrapper:<init>(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (O)java.io.ByteArrayOutputStream:<init>(int)
M:org.apache.nutch.metadata.SpellCheckedMetadata:add(java.lang.String,java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.parse.ParseResult:filter() (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.util.SuffixStringMatcher:<init>(java.lang.String[]) (O)org.apache.nutch.util.TrieStringMatcher:<init>()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.URLUtil:fixEmbeddedParams(java.net.URL,java.lang.String) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseResult:iterator()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkTimelimit() (I)java.util.Map:size()
M:org.apache.nutch.protocol.ProtocolStatus:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.util.CommandRunner:exec() (M)java.lang.Process:getOutputStream()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (O)java.util.Date:<init>(long)
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (O)java.lang.String:<init>(java.lang.String)
M:org.apache.nutch.crawl.Inlink:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (I)org.xml.sax.Locator:getLineNumber()
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$400(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.nutch.segment.SegmentReader:usage()
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginRepository:displayStatus()
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (I)java.util.List:get(int)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)java.lang.Exception:getMessage()
M:org.apache.nutch.crawl.CrawlDatum:clone() (O)java.lang.Object:clone()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.CrawlDbMerger:main(java.lang.String[]) (O)org.apache.nutch.crawl.CrawlDbMerger:<init>()
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String) (M)java.lang.Object:getClass()
M:org.apache.nutch.parse.ParseText:write(java.io.DataOutput) (I)java.io.DataOutput:write(int)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (I)java.util.Map:putAll(java.util.Map)
M:org.apache.nutch.plugin.PluginManifestParser:parseRequires(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:addDependency(java.lang.String)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setCombinerClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilters:injectedScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.protocol.Content:write(java.io.DataOutput) (M)org.apache.nutch.metadata.Metadata:write(java.io.DataOutput)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.protocol.Content:getContentType(java.lang.String,java.lang.String,byte[]) (M)org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[])
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (S)org.apache.nutch.fetcher.Fetcher:isParsing(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.InlinkPriorityQueue:pop()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:configure(org.apache.hadoop.mapred.JobConf) (O)java.lang.IllegalArgumentException:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.MimeUtil:forName(java.lang.String) (M)org.apache.tika.mime.MimeTypes:forName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (M)java.net.URL:getHost()
M:org.apache.nutch.util.TimingUtil:elapsedTime(long,long) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getUrl()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (O)java.util.zip.GZIPInputStream:<init>(java.io.InputStream)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.net.protocols.HttpDateFormat:toLong(java.lang.String) (M)java.text.SimpleDateFormat:parse(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (O)org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArgs()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (S)org.apache.nutch.tools.arc.ArcRecordReader:isMagic(byte[])
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpURI:getScheme()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.parse.ParseData:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Route:write(java.io.DataOutput) (I)java.io.DataOutput:writeBoolean(boolean)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (I)org.slf4j.Logger:trace(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.plugin.PluginRepository:getCachedClass(org.apache.nutch.plugin.PluginDescriptor,java.lang.String) (M)org.apache.nutch.plugin.PluginDescriptor:getClassLoader()
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)java.util.Random:nextInt()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)java.lang.Math:round(double)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (M)java.util.HashMap:containsKey(java.lang.Object)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.tools.proxy.FakeHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (M)org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean)
M:org.apache.nutch.util.PrefixStringMatcher:shortestMatch(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.plugin.Extension:<init>(org.apache.nutch.plugin.PluginDescriptor,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository) (O)org.apache.nutch.plugin.Extension:setExtensionPoint(java.lang.String)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:handle(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,int) (I)javax.servlet.http.HttpServletResponse:addHeader(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (O)org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.plugin.PluginClassLoader:equals(java.lang.Object) (M)java.lang.Object:getClass()
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.FileStatus:isDir()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:getModifiedTime()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.parse.ParserChecker:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (O)java.io.InputStreamReader:<init>(java.io.InputStream,java.lang.String)
M:org.apache.nutch.util.DomUtil:getDom(java.io.InputStream) (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.HadoopFSUtil$2:accept(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:getFileStatus(org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.indexer.IndexerMapReduce:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.mapred.Counters$Counter:increment(long)
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.tools.FreeGenerator$FG:<init>() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDatum:<clinit>() (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbFilter:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (M)org.apache.nutch.crawl.MapWritable:clear()
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (I)java.io.DataInput:readFloat()
M:org.apache.nutch.util.NutchConfiguration:getUUID(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (I)org.apache.commons.logging.Log:info(java.lang.Object)
M:org.apache.nutch.metadata.Metadata:getValues(java.lang.String) (O)org.apache.nutch.metadata.Metadata:_getValues(java.lang.String)
M:org.apache.nutch.crawl.MD5Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.lang.String:getBytes()
M:org.apache.nutch.scoring.webgraph.WebGraph$NodeDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.Counters$Group:getCounter(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.util.NutchConfiguration:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (S)org.apache.nutch.fetcher.Fetcher:isParsing(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.metadata.MetaWrapper:addMeta(java.lang.String,java.lang.String) (M)org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$5:run() (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.NutchWritable:get()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$QueueFeeder:isAlive()
M:org.apache.nutch.crawl.Injector:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.util.DeflateUtils:deflate(byte[]) (M)java.util.zip.DeflaterOutputStream:close()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Injector$InjectReducer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Injector$InjectReducer:configure(org.apache.hadoop.mapred.JobConf) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlink:getAnchor()
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)java.io.PrintStream:print(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.indexer.NutchField:<init>(java.lang.Object,float) (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.parse.ParseException:<init>(java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Generator:main(java.lang.String[]) (O)org.apache.nutch.crawl.Generator:<init>()
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (M)java.io.ByteArrayOutputStream:write(byte[],int,int)
M:org.apache.nutch.util.SuffixStringMatcher:matches(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (O)org.apache.nutch.net.URLFilterChecker:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (O)org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean) (M)java.net.URL:getFile()
M:org.apache.nutch.fetcher.Fetcher:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (O)org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:<init>()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.crawl.MapWritable:putAll(org.apache.nutch.crawl.MapWritable) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.LinkDatum:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parseManifestFile(java.lang.String) (O)org.apache.nutch.plugin.PluginManifestParser:parseXML(java.net.URL)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getScore()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.lang.StringBuffer:setLength(int)
M:org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor) (O)org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexerOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.indexer.IndexerOutputFormat$1:<init>(org.apache.nutch.indexer.IndexerOutputFormat,org.apache.nutch.indexer.IndexWriters)
M:org.apache.nutch.indexer.IndexingJob:main(java.lang.String[]) (O)org.apache.nutch.indexer.IndexingJob:<init>()
M:org.apache.nutch.scoring.webgraph.Node:readFields(java.io.DataInput) (I)java.io.DataInput:readFloat()
M:org.apache.nutch.net.URLFilterChecker:checkAll() (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (O)org.apache.nutch.tools.DmozParser$RDFProcessor:<init>(org.apache.nutch.tools.DmozParser,org.xml.sax.XMLReader,int,boolean,int,java.util.regex.Pattern)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.DeduplicationJob:access$000()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (O)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:setEndTime(long)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.fetcher.Fetcher:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.scoring.ScoringFilterException:printStackTrace()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.crawl.LinkDb:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$008(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.indexer.NutchIndexAction:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.util.StringUtil:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (O)java.io.ByteArrayInputStream:<init>(byte[])
M:org.apache.nutch.crawl.CrawlDatum:metadataEquals(org.apache.hadoop.io.MapWritable) (O)java.util.HashSet:<init>(java.util.Collection)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.OldFetcher$InputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int) (M)org.apache.hadoop.fs.FileStatus:getLen()
M:org.apache.nutch.crawl.MapWritable:findEntryByKey(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Loops$Route,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.scoring.ScoringFilters:passScoreAfterParsing(org.apache.hadoop.io.Text,org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (I)org.apache.nutch.scoring.ScoringFilter:passScoreAfterParsing(org.apache.hadoop.io.Text,org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse)
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Math:min(int,int)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)java.util.HashSet:toArray(java.lang.Object[])
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)org.apache.nutch.util.CommandRunner:setStdOutputStream(java.io.OutputStream)
M:org.apache.nutch.util.SuffixStringMatcher:<init>(java.lang.String[]) (M)org.apache.nutch.util.SuffixStringMatcher:addPatternBackward(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginRepository:getCachedClass(org.apache.nutch.plugin.PluginDescriptor,java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher:addPatternForward(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.parse.ParseStatus:read(java.io.DataInput) (M)org.apache.nutch.parse.ParseStatus:readFields(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentPart:parse(java.lang.String) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.Parse) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.CrawlDbReader:get(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.mapred.lib.HashPartitioner:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List)
M:org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:listIterator()
M:org.apache.nutch.crawl.AbstractFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.scoring.webgraph.Node:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.LinkDbReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.StringUtil:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (M)org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.URLUtil:getPage(java.lang.String) (M)java.net.URL:getQuery()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.parse.ParseText:<init>(java.lang.String) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.MapWritable:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.fetcher.FetcherOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.TextProfileSignature$TokenComparator:compare(java.lang.Object,java.lang.Object) (M)org.apache.nutch.crawl.TextProfileSignature$TokenComparator:compare(org.apache.nutch.crawl.TextProfileSignature$Token,org.apache.nutch.crawl.TextProfileSignature$Token)
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.metadata.Metadata:equals(java.lang.Object) (M)org.apache.nutch.metadata.Metadata:names()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:<init>() (O)java.util.ArrayList:<init>()
M:org.apache.nutch.net.protocols.HttpDateFormat:toString(java.util.Calendar) (M)java.util.Calendar:getTime()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.net.protocols.HttpDateFormat:toString(long) (M)java.text.SimpleDateFormat:format(java.util.Date)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.crawl.CrawlDatum:<clinit>() (S)org.apache.hadoop.io.WritableComparator:define(java.lang.Class,org.apache.hadoop.io.WritableComparator)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.CrawlDatum:<init>(int,int,float)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.protocol.RobotRulesParser:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.util.ObjectCache:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:getNumMapTasks()
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (M)java.net.URL:getRef()
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.regex.Matcher:matches()
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.FileSystem:getFileStatus(org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentReader$4:run() (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReader:get(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getCrawlDatum(org.apache.hadoop.io.Text) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor) (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:size()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (O)org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:reduce(org.apache.hadoop.io.ByteWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.IndexWriters:delete(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)java.io.InputStreamReader:<init>(java.io.InputStream,java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.crawl.CrawlDb:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:access$300()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endElement(java.lang.String,java.lang.String,java.lang.String) (M)java.lang.StringBuffer:length()
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.segment.SegmentReader$3:run() (S)org.apache.nutch.segment.SegmentReader:access$100(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String)
M:org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.Math:floor(double)
M:org.apache.nutch.parse.ParseOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapFile$Writer:close()
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.net.URLNormalizerChecker:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.Benchmark:main(java.lang.String[]) (O)org.apache.nutch.tools.Benchmark:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.util.URLUtil:isSameDomainName(java.net.URL,java.net.URL) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.crawl.SignatureComparator:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.LinkDb:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)java.lang.Class:getConstructor(java.lang.Class[])
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (O)java.util.Random:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:<init>()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.CrawlDbMerger:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.URLPartitioner:getPartition(java.lang.Object,java.lang.Object,int) (M)org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:<init>(org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode[])
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (O)org.mortbay.jetty.servlet.ServletHandler:<init>()
M:org.apache.nutch.crawl.Generator$SelectorEntry:toString() (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)org.apache.nutch.util.URLUtil:toASCII(java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setPageRetrySchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (M)javax.xml.transform.Transformer:setOutputProperty(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.net.protocols.HttpDateFormat:toString(java.util.Date) (M)java.text.SimpleDateFormat:format(java.util.Date)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (I)org.apache.nutch.crawl.FetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.hadoop.fs.FileSystem:getFileStatus(org.apache.hadoop.fs.Path)
M:org.apache.nutch.indexer.NutchField:<init>(java.lang.Object,float) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseData:getOutlinks()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.metadata.SpellCheckedMetadata:remove(java.lang.String) (O)org.apache.nutch.metadata.Metadata:remove(java.lang.String)
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.apache.hadoop.mapred.RunningJob:getCounters()
M:org.apache.nutch.crawl.MapWritable:addIdEntry(byte,java.lang.Class) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$502(org.apache.nutch.crawl.MapWritable$ClassIdEntry,org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (S)org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.NodeDumper$AggrType:valueOf(java.lang.String) (S)java.lang.Enum:valueOf(java.lang.Class,java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:normalizeUrl(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)java.io.PrintStream:print(java.lang.Object)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.segment.SegmentReader$InputCompatMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.segment.SegmentReader$InputCompatMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (M)org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)java.util.Collections:sort(java.util.List,java.util.Comparator)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (I)org.slf4j.Logger:isDebugEnabled()
M:org.apache.nutch.util.SuffixStringMatcher:<init>(java.util.Collection) (O)org.apache.nutch.util.TrieStringMatcher:<init>()
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (M)java.lang.Long:longValue()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.util.URLUtil:getPage(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:clear()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.tools.proxy.SegmentHandler:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (O)org.apache.nutch.tools.proxy.AbstractTestbedHandler:<init>()
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (I)java.util.List:iterator()
M:org.apache.nutch.crawl.CrawlDatum:equals(java.lang.Object) (O)org.apache.nutch.crawl.CrawlDatum:metadataEquals(org.apache.hadoop.io.MapWritable)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDbMerger:getConf()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.metadata.Metadata:_getValues(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (O)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:setEndTime(long,boolean)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.apache.hadoop.mapred.RecordReader:next(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.util.URLUtil:getPage(java.lang.String) (M)java.lang.String:replace(java.lang.CharSequence,java.lang.CharSequence)
M:org.apache.nutch.util.EncodingDetector$EncodingClue:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseResult:isEmpty()
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (I)java.util.List:iterator()
M:org.apache.nutch.parse.Outlink:toString() (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.fetcher.Fetcher$InputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int) (M)org.apache.hadoop.fs.FileStatus:getPath()
M:org.apache.nutch.crawl.MapWritable:hashCode() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDatum:toString() (I)java.util.Iterator:next()
M:org.apache.nutch.metadata.MetaWrapper:<init>() (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.AbstractFetchSchedule:shouldFetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (I)org.apache.hadoop.mapred.RecordReader:next(java.lang.Object,java.lang.Object)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (I)java.io.DataInput:readLong()
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:normalizeUrl(java.lang.String) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getExtensionFromAlias(org.apache.nutch.plugin.Extension[],java.lang.String) (O)org.apache.nutch.parse.ParserFactory:getExtension(org.apache.nutch.plugin.Extension[],java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Route:readFields(java.io.DataInput) (I)java.io.DataInput:readBoolean()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.Outlink:getToUrl()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setInputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(float)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher$QueueFeeder:start()
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:get(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setMapRunnerClass(java.lang.Class)
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.String:split(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$102(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:access$100(org.apache.nutch.parse.ParseOutputFormat)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getNumInlinks()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseData:getContentMeta()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.crawl.CrawlDatum$Comparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.CrawlDatum$Comparator:readInt(byte[],int)
M:org.apache.nutch.util.StringUtil:isEmpty(java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.hadoop.io.MapWritable:putAll(java.util.Map)
M:org.apache.nutch.util.URLUtil:main(java.lang.String[]) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.tools.arc.ArcSegmentCreator:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)java.util.HashMap:containsKey(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (O)org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.plugin.PluginRepository:getPluginDescriptor(java.lang.String) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (I)org.apache.hadoop.io.Writable:readFields(java.io.DataInput)
M:org.apache.nutch.indexer.IndexingFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseData:equals(java.lang.Object) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (S)java.net.IDN:toUnicode(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolOutput:getStatus()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.parse.ParsePluginList:setPluginList(java.lang.String,java.util.List)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (O)org.apache.nutch.protocol.Content:<init>()
M:org.apache.nutch.util.URLUtil:getDomainName(java.lang.String) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.lang.String:substring(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (I)java.util.Iterator:remove()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Node:<init>() (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.util.StringUtil:fromHexString(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (I)org.apache.hadoop.mapred.Reporter:setStatus(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.metadata.Metadata:equals(java.lang.Object) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.ParserFactory:getExtensions(java.lang.String) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)com.ibm.icu.text.CharsetMatch:getName()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:getCounter(java.lang.String,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:addFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.metadata.Metadata:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.tools.FreeGenerator$FG:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.hadoop.mapred.MapReduceBase:configure(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)org.apache.nutch.scoring.webgraph.Node:getInlinkScore()
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)org.apache.nutch.fetcher.Fetcher:getConf()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.InlinkPriorityQueue:clear()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.util.ArrayList:size()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (O)java.util.HashMap:<init>()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.Benchmark:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.segment.SegmentReader$TextOutputFormat$1:<init>(org.apache.nutch.segment.SegmentReader$TextOutputFormat,java.io.PrintStream)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.mapred.JobClient:<init>(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (M)org.apache.nutch.indexer.IndexWriters:describe()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.tools.proxy.DelayHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseData:getParseMeta()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.protocol.ProtocolStatus:read(java.io.DataInput) (O)org.apache.nutch.protocol.ProtocolStatus:<init>()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (I)javax.servlet.http.HttpServletResponse:flushBuffer()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (O)java.util.Random:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArgs(int)
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:write(java.io.DataOutput) (I)java.io.DataOutput:writeUTF(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)org.apache.nutch.indexer.IndexingJob:getConf()
M:org.apache.nutch.tools.FreeGenerator:main(java.lang.String[]) (O)org.apache.nutch.tools.FreeGenerator:<init>()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (O)org.apache.hadoop.io.DataOutputBuffer:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.parse.ParseData:getMeta(java.lang.String) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (M)java.io.Writer:flush()
M:org.apache.nutch.scoring.webgraph.NodeDumper:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (O)java.lang.IllegalArgumentException:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getCachedClass(org.apache.nutch.plugin.PluginDescriptor,java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getQueueSize() (I)java.util.List:size()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:get(int)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbFilter:<init>() (O)org.apache.hadoop.io.Text:<init>()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasDbStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$2:run() (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)java.util.List:toArray(java.lang.Object[])
M:org.apache.nutch.metadata.Metadata:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.Injector$InjectMapper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getLong(java.lang.String,long)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.metadata.Metadata:size() (I)java.util.Map:size()
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (I)java.util.ListIterator:add(java.lang.Object)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getFloat(java.lang.String,float)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (O)java.util.concurrent.atomic.AtomicInteger:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:toString() (S)java.lang.String:valueOf(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.HashMap:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.metadata.MetaWrapper:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput)
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean) (M)org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int)
M:org.apache.nutch.crawl.CrawlDatum:read(java.io.DataInput) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.scoring.webgraph.Node:write(java.io.DataOutput) (I)java.io.DataOutput:writeFloat(float)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (O)java.lang.Object:<init>()
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (S)java.lang.Class:forName(java.lang.String,boolean,java.lang.ClassLoader)
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.NodeDumper$DumpType:<clinit>() (O)org.apache.nutch.scoring.webgraph.NodeDumper$DumpType:<init>(java.lang.String,int)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)org.apache.nutch.metadata.Metadata:names()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.Node:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.NodeWalker:nextNode() (I)org.w3c.dom.NodeList:getLength()
M:org.apache.nutch.crawl.CrawlDatum:read(java.io.DataInput) (M)org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)org.slf4j.Logger:isTraceEnabled()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.Outlink:readFields(java.io.DataInput) (M)org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getDomainName(java.lang.String)
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (M)org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (S)org.apache.nutch.net.protocols.HttpDateFormat:toString(java.util.Date)
M:org.apache.nutch.crawl.CrawlDatum:getStatusName(byte) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.NutchDocument:toString() (I)java.util.Iterator:next()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.fetcher.Fetcher:getConf()
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)org.apache.nutch.protocol.Content:getMetadata()
M:org.apache.nutch.parse.ParseStatus:readFields(java.io.DataInput) (I)java.io.DataInput:readShort()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.MapWritable$KeyValueEntry:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (M)java.lang.Class:getName()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (I)java.util.Iterator:next()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (S)javax.xml.parsers.SAXParserFactory:newInstance()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setLong(java.lang.String,long)
M:org.apache.nutch.util.URLUtil:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getDependencyCheckedPlugins(java.util.Map,java.util.Map) (M)org.apache.nutch.plugin.MissingDependencyException:getMessage()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.hadoop.io.SequenceFile$Reader:close()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.TreeMap:size()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:errorError(org.xml.sax.SAXParseException) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)java.util.ArrayList:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:<init>() (O)org.apache.hadoop.io.LongWritable:<init>()
M:org.apache.nutch.parse.Outlink:write(java.io.DataOutput) (M)org.apache.hadoop.io.MapWritable:size()
M:org.apache.nutch.parse.ParseData:equals(java.lang.Object) (S)java.util.Arrays:equals(java.lang.Object[],java.lang.Object[])
M:org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String) (M)java.lang.String:indexOf(int)
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (M)org.apache.nutch.protocol.Content:getContentType()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:read() (M)java.io.Reader:reset()
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (O)org.apache.nutch.plugin.PluginManifestParser:parseRequires(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.protocol.ProtocolException:<init>(java.lang.String,java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.HttpURI:getPort()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.tools.proxy.SegmentHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (M)org.apache.nutch.plugin.ExtensionPoint:getId()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)java.lang.Integer:valueOf(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (O)java.util.HashMap:<init>()
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.util.CommandRunner:exec() (M)org.apache.nutch.util.CommandRunner$PusherThread:interrupt()
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path)
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (O)java.util.Date:<init>(long)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:<init>(org.apache.hadoop.conf.Configuration,boolean,boolean,boolean,boolean,boolean,boolean) (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (O)java.io.FileInputStream:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (M)java.lang.Class:newInstance()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.metadata.Metadata:toString() (O)org.apache.nutch.metadata.Metadata:_getValues(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.util.NutchConfiguration:addNutchResources(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:addResource(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (M)org.apache.hadoop.fs.Path:getName()
M:org.apache.nutch.tools.Benchmark:createSeeds(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int) (M)java.io.OutputStream:write(byte[])
M:org.apache.nutch.util.GZIPUtils:zip(byte[]) (M)java.io.ByteArrayOutputStream:toByteArray()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.util.HadoopFSUtil$1:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (M)org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int)
M:org.apache.nutch.fetcher.FetcherOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:get(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)java.util.List:iterator()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (O)org.apache.nutch.parse.ParserFactory:escapeContentType(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)java.lang.Character:toLowerCase(char)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput) (I)java.io.DataInput:readFully(byte[])
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (O)java.lang.IllegalArgumentException:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (M)org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)org.apache.nutch.parse.Parse:isCanonical()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.ParserFactory:getExtension(org.apache.nutch.plugin.Extension[],java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.net.URL:getProtocol()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<init>() (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.parse.ParseStatus:getArgs()
M:org.apache.nutch.segment.SegmentReader$SegmentReaderStats:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.HtmlParseFilters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.util.SuffixStringMatcher:matches(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:isTerminal()
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.mapred.FileSplit:getLength()
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (M)java.util.LinkedList:add(java.lang.Object)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlinks:size()
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (O)org.apache.nutch.plugin.PluginRuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDbMerger:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (O)java.util.Vector:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.nutch.scoring.webgraph.LinkRank:getConf()
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.crawl.Inlinks:<init>()
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.net.MalformedURLException:toString()
M:org.apache.nutch.tools.Benchmark:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.parse.ParserFactory:match(org.apache.nutch.plugin.Extension,java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.Extension:getAttribute(java.lang.String)
M:org.apache.nutch.protocol.ProtocolNotFound:<init>(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)org.apache.nutch.scoring.webgraph.Node:getOutlinkScore()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.indexer.IndexingException:printStackTrace()
M:org.apache.nutch.tools.proxy.DelayHandler:<init>(int) (O)java.util.Random:<init>(long)
M:org.apache.nutch.crawl.CrawlDatum:metadataEquals(org.apache.hadoop.io.MapWritable) (M)org.apache.hadoop.io.MapWritable:entrySet()
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkRank$Initializer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (S)java.lang.Class:forName(java.lang.String)
M:org.apache.nutch.crawl.Inlinks:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.fetcher.FetcherOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (O)org.apache.hadoop.mapred.InvalidJobConfException:<init>(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (M)org.apache.tika.Tika:detect(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)org.apache.nutch.plugin.Extension:getClazz()
M:org.apache.nutch.crawl.Generator:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.LongWritable:set(long)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getNumOutlinks()
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.protocols.HttpDateFormat:toDate(java.lang.String) (M)java.text.SimpleDateFormat:parse(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.Class:getName()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.String:getBytes()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilters:indexerScore(org.apache.hadoop.io.Text,org.apache.nutch.indexer.NutchDocument,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.parse.Parse,org.apache.nutch.crawl.Inlinks,float)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:close()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.net.URL:getProtocol()
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(float)
M:org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseText:equals(java.lang.Object) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.ScoreUpdater:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.util.SuffixStringMatcher:main(java.lang.String[]) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.indexer.IndexingJob:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (M)java.lang.Class:isAssignableFrom(java.lang.Class)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.ParsePluginList:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:<init>() (O)org.apache.nutch.crawl.AbstractFetchSchedule:<init>()
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (I)java.util.Set:toArray(java.lang.Object[])
M:org.apache.nutch.crawl.Generator:<clinit>() (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseResult:filter() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.crawl.LinkDbMerger:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)org.xml.sax.SAXException:toString()
M:org.apache.nutch.segment.SegmentReader:<init>() (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean) (M)java.net.URL:getHost()
M:org.apache.nutch.segment.SegmentMerger:map(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.nutch.crawl.Injector:getConf()
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.util.URLUtil:getHost(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolStatus:getCode()
M:org.apache.nutch.crawl.MapWritable:hashCode() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.util.PrefixStringMatcher:<init>(java.util.Collection) (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.NodeDumper$DumpType:values() (M)org.apache.nutch.scoring.webgraph.NodeDumper$DumpType[]:clone()
M:org.apache.nutch.fetcher.Fetcher:isParsing(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.tools.arc.ArcRecordReader:createKey() (S)org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.util.PrefixStringMatcher:matches(java.lang.String) (M)java.lang.String:charAt(int)
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$400(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element)
M:org.apache.nutch.indexer.IndexingFiltersChecker:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.URLUtil:resolveURL(java.net.URL,java.lang.String) (O)java.net.URL:<init>(java.net.URL,java.lang.String)
M:org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (M)org.apache.hadoop.io.MapWritable:keySet()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (S)org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.parse.ParseResult:iterator()
M:org.apache.nutch.crawl.LinkDbReader:init(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.ProtocolStatus:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[]) (S)org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int)
M:org.apache.nutch.net.protocols.HttpDateFormat:<clinit>() (O)java.text.SimpleDateFormat:<init>(java.lang.String,java.util.Locale)
M:org.apache.nutch.util.StringUtil:leftPad(java.lang.String,int) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (M)java.net.URL:getPort()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (O)org.apache.nutch.segment.SegmentReader$SegmentReaderStats:<init>()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)java.io.DataOutputStream:writeByte(int)
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (O)org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.util.DeflateUtils:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.AbstractFetchSchedule:initializeSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String,int) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.protocol.Content:getMetadata()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexerOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.indexer.NutchIndexAction) (M)org.apache.nutch.indexer.IndexWriters:write(org.apache.nutch.indexer.NutchDocument)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.metadata.MetaWrapper:getMeta(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (O)java.util.HashSet:<init>()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.Generator$SelectorEntry:readFields(java.io.DataInput) (M)org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.util.PrefixStringMatcher:longestMatch(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:isTerminal()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItemQueue(java.lang.String) (O)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.segment.SegmentPart:toString()
M:org.apache.nutch.parse.ParserFactory:getExtensionFromAlias(org.apache.nutch.plugin.Extension[],java.lang.String) (M)org.apache.nutch.parse.ParsePluginList:getAliases()
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Generator$SelectorEntry:<init>() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.LinkedHashSet:<init>()
M:org.apache.nutch.tools.arc.ArcRecordReader:close() (M)org.apache.hadoop.fs.FSDataInputStream:close()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MapWritable:findEntryByKey(org.apache.hadoop.io.Writable) (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:<init>() (O)org.apache.nutch.crawl.AdaptiveFetchSchedule:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.parse.ParserFactory:findExtensions(java.lang.String) (M)org.apache.nutch.parse.ParsePluginList:getPluginList(java.lang.String)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:addMyHeader(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.MapWritable:toString() (M)java.lang.StringBuffer:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:readFields(java.io.DataInput) (M)org.apache.nutch.scoring.webgraph.Node:readFields(java.io.DataInput)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.lang.String:equalsIgnoreCase(java.lang.String)
M:org.apache.nutch.parse.HtmlParseFilters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)java.util.Random:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.apache.nutch.parse.Parse:isCanonical()
M:org.apache.nutch.parse.ParseResult:createParseResult(java.lang.String,org.apache.nutch.parse.Parse) (O)org.apache.nutch.parse.ParseResult:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (S)java.lang.reflect.Modifier:isPublic(int)
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.GZIPUtils:unzip(byte[]) (M)java.util.zip.GZIPInputStream:read(byte[])
M:org.apache.nutch.scoring.ScoringFilterException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.LinkDbMerger:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.metadata.SpellCheckedMetadata:get(java.lang.String) (O)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Node:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:clear()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.MapWritable:findEntryByKey(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:createKey() (M)org.apache.nutch.segment.ContentAsTextInputFormat$ContentAsTextRecordReader:createKey()
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (S)org.apache.nutch.net.protocols.HttpDateFormat:toString(long)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.tools.ResolveUrls:access$200()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.crawl.LinkDb:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:read(char[],int,int) (M)java.io.Reader:read(char[],int,int)
M:org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.Parse) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.parse.ParsePluginList:setPluginList(java.lang.String,java.util.List) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.metadata.SpellCheckedMetadata:set(java.lang.String,java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (O)java.util.zip.InflaterInputStream:<init>(java.io.InputStream)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.CrawlDatum$Comparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.CrawlDatum$Comparator:readFloat(byte[],int)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.crawl.CrawlDb:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseStatus:readFields(java.io.DataInput) (O)org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)java.util.StringTokenizer:hasMoreTokens()
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.io.File:toURI()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.util.Map:entrySet()
M:org.apache.nutch.scoring.webgraph.LinkDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readFloat()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (O)java.util.concurrent.atomic.AtomicInteger:<init>(int)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (M)java.lang.Exception:printStackTrace()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.FloatWritable:<init>(float)
M:org.apache.nutch.tools.proxy.LogDebugHandler:<init>() (O)org.apache.nutch.tools.proxy.AbstractTestbedHandler:<init>()
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (O)org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)java.lang.String:valueOf(float)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.fetcher.Fetcher:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)java.util.regex.Matcher:matches()
M:org.apache.nutch.segment.SegmentReader:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.CircularDependencyException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String) (O)java.net.URL:<init>(java.net.URL,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.nutch.scoring.webgraph.LinkRank:getConf()
M:org.apache.nutch.plugin.PluginManifestParser:parseLibraries(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getAttribute(java.lang.String)
M:org.apache.nutch.util.CommandRunner:exec() (M)java.lang.Runtime:exec(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.util.MimeUtil:getMimeType(java.io.File) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParseStatus:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (M)org.apache.nutch.util.GenericWritableConfigurable:set(org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.indexer.NutchIndexAction:<init>(org.apache.nutch.indexer.NutchDocument,byte)
M:org.apache.nutch.util.MimeUtil:forName(java.lang.String) (M)org.apache.tika.mime.MimeType:toString()
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (I)org.apache.hadoop.mapred.Reporter:setStatus(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.Outlink:getMetadata()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)java.lang.Class:getName()
M:org.apache.nutch.protocol.ProtocolNotFound:<init>(java.lang.String) (O)org.apache.nutch.protocol.ProtocolNotFound:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.plugin.PluginClassLoader:equals(java.lang.Object) (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (M)java.util.HashMap:values()
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration) (M)java.util.WeakHashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlinks:iterator()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.segment.ContentAsTextInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:setStatus(java.lang.String)
M:org.apache.nutch.crawl.AbstractFetchSchedule:initializeSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.parse.ParseStatus:getEmptyParse(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.ParseStatus$EmptyParseImpl:<init>(org.apache.nutch.parse.ParseStatus,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasFetchStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.scoring.webgraph.LinkDatum:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.String[],long) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader$5:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map) (O)java.lang.Thread:<init>()
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:setStatus(java.lang.String)
M:org.apache.nutch.fetcher.FetcherOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.protocol.ProtocolFactory:findExtension(java.lang.String) (M)org.apache.nutch.plugin.Extension:getAttribute(java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setSignature(byte[])
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.FetcherOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.Benchmark:<clinit>() (S)org.apache.commons.logging.LogFactory:getLog(java.lang.Class)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getInt(java.lang.String,int)
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (S)org.apache.nutch.net.protocols.HttpDateFormat:toLong(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.net.URLNormalizers:normalize(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (I)java.util.Iterator:next()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)org.apache.nutch.util.CommandRunner:setCommand(java.lang.String)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)org.w3c.dom.Document:getDocumentElement()
M:org.apache.nutch.crawl.LinkDb:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseStatus:getEmptyParse(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.DeduplicationJob:access$000()
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)java.util.ArrayList:size()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem) (O)org.apache.nutch.util.HadoopFSUtil$2:<init>(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.fs.FileStatus:getPath()
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (M)org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.util.ArrayList:size()
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)org.apache.hadoop.mapred.lib.HashPartitioner:<init>()
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:getNumOutlinks()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.plugin.PluginClassLoader:hashCode() (S)java.util.Arrays:hashCode(java.lang.Object[])
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)java.util.Iterator:next()
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (O)java.util.HashSet:<init>(java.util.Collection)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (I)java.util.Iterator:next()
M:org.apache.nutch.indexer.NutchDocument:toString() (I)java.util.Map$Entry:getKey()
M:org.apache.nutch.util.NodeWalker:<init>(org.w3c.dom.Node) (O)java.util.Stack:<init>()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:writeOutAsDuplicate(org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (O)org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.segment.SegmentReader:configure(org.apache.hadoop.mapred.JobConf) (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:<init>() (O)org.apache.hadoop.io.LongWritable:<init>(long)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.mapred.JobClient:<init>(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.util.HashSet:addAll(java.util.Collection)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.lang.StringBuilder:deleteCharAt(int)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)java.util.Random:nextInt(int)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)java.lang.reflect.Method:invoke(java.lang.Object,java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.Outlink:getToUrl()
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (M)org.apache.nutch.crawl.CrawlDatum:setRetriesSinceFetch(int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.scoring.ScoringFilters:initialScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (O)org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.metadata.MetaWrapper:<init>() (O)org.apache.nutch.crawl.NutchWritable:<init>()
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)java.lang.Boolean:valueOf(boolean)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:<init>() (O)org.apache.hadoop.mapred.SequenceFileInputFormat:<init>()
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:isFound()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (M)java.util.concurrent.atomic.AtomicInteger:get()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.PluginRepository:getExtensionPoint(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:main(java.lang.String[]) (O)org.apache.nutch.fetcher.Fetcher:<init>()
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (M)org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MapWritable:get(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.ParserFactory:findExtensions(java.lang.String) (O)org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (O)org.apache.nutch.parse.ParseText:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.LinkRank:<init>()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)java.util.regex.Pattern:matcher(java.lang.CharSequence)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:get()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)java.lang.Character:isLetterOrDigit(char)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.tools.proxy.SegmentHandler$Segment:getCrawlDatum(org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.Generator$Selector:getPartition(org.apache.hadoop.io.FloatWritable,org.apache.hadoop.io.Writable,int) (I)org.apache.hadoop.mapred.Partitioner:getPartition(java.lang.Object,java.lang.Object,int)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.net.URLNormalizerChecker:checkAll(java.lang.String) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.LinkDb:getHost(java.lang.String) (M)java.net.URL:getHost()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (I)org.slf4j.Logger:trace(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:isStoringContent(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.indexer.CleaningJob$DBFilter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.NutchConfiguration:getUUID(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (I)org.apache.nutch.parse.Parse:getText()
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.net.URLFilterChecker:checkAll() (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:hashCode() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.parse.ParseImpl:<init>(org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData,boolean)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:addInProgressFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:<init>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.util.CommandRunner:exec() (S)java.lang.Runtime:getRuntime()
M:org.apache.nutch.indexer.IndexWriters:commit() (I)org.apache.nutch.indexer.IndexWriter:commit()
M:org.apache.nutch.metadata.SpellCheckedMetadata:set(java.lang.String,java.lang.String) (O)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char) (M)java.util.LinkedList:size()
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (I)java.io.DataInput:readFully(byte[],int,int)
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.plugin.ExtensionPoint:<init>(java.lang.String,java.lang.String,java.lang.String) (O)org.apache.nutch.plugin.ExtensionPoint:setId(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)java.util.List:get(int)
M:org.apache.nutch.crawl.CrawlDbMerger:<init>() (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (I)java.util.Set:iterator()
M:org.apache.nutch.util.EncodingDetector$EncodingClue:<init>(org.apache.nutch.util.EncodingDetector,java.lang.String,java.lang.String,int) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)java.lang.String:valueOf(int)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:<clinit>() (O)java.lang.Byte:<init>(byte)
M:org.apache.nutch.scoring.webgraph.LinkDatum:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyComparatorClass(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (O)org.apache.hadoop.io.DataInputBuffer:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.util.MimeUtil:forName(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)
M:org.apache.nutch.util.URLUtil:getHost(java.lang.String) (M)java.lang.String:toLowerCase()
M:org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (O)org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.util.Vector:isEmpty()
M:org.apache.nutch.metadata.Metadata:names() (I)java.util.Set:size()
M:org.apache.nutch.crawl.Inlinks:add(org.apache.nutch.crawl.Inlinks) (M)java.util.HashSet:addAll(java.util.Collection)
M:org.apache.nutch.crawl.CrawlDatum:setSignature(byte[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (O)org.apache.nutch.net.URLFilterChecker:checkAll()
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:readUrl(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.crawl.CrawlDbReader:get(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.tools.FreeGenerator$FG:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (M)java.io.LineNumberReader:readLine()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:getUri()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.ScoringFilters:initialScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (I)org.apache.nutch.scoring.ScoringFilter:initialScore(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.FloatWritable:<init>(float)
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (M)java.io.ByteArrayOutputStream:write(byte[],int,int)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.mapred.JobConf:setReducerClass(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.parse.ParseStatus:getMessage()
M:org.apache.nutch.indexer.IndexerMapReduce:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.IndexerMapReduce:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (S)javax.xml.parsers.DocumentBuilderFactory:newInstance()
M:org.apache.nutch.crawl.CrawlDatum:setMetaData(org.apache.hadoop.io.MapWritable) (O)org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable)
M:org.apache.nutch.parse.ParseResult:isSuccess() (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (M)org.apache.hadoop.io.MD5Hash:hashCode()
M:org.apache.nutch.segment.SegmentPart:parse(java.lang.String) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.Outlink:getAnchor()
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (M)java.lang.Object:getClass()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.tools.arc.ArcSegmentCreator:getConf()
M:org.apache.nutch.crawl.CrawlDbReader:readUrl(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (M)org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.metadata.Metadata:names() (I)java.util.Set:toArray(java.lang.Object[])
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getInProgressSize() (I)java.util.Set:size()
M:org.apache.nutch.protocol.ProtocolStatus:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.String:valueOf(long)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.metadata.Metadata:get(java.lang.String)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.indexer.NutchIndexAction:readFields(java.io.DataInput) (M)org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getAnchor()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.lang.String:trim()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDbFilter:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.segment.SegmentPart:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (O)org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.fetcher.Fetcher:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.util.StringUtil:fromHexString(java.lang.String) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.SuffixStringMatcher:longestMatch(java.lang.String) (M)java.lang.String:substring(int)
M:org.apache.nutch.plugin.PluginRepository:installExtensions(java.util.List) (M)org.apache.nutch.plugin.Extension:getTargetPoint()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.crawl.AbstractFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (M)org.apache.nutch.crawl.CrawlDatum:setModifiedTime(long)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.String:length()
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (M)java.util.Date:getTime()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.Object)
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.Integer:intValue()
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean)
M:org.apache.nutch.parse.Parser:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.nutch.scoring.webgraph.ScoreUpdater:getConf()
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (O)java.net.URI:<init>(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (O)java.util.Random:<init>()
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.segment.SegmentReader:append(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.io.PrintWriter,int) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.crawl.MapWritable:equals(java.lang.Object) (M)java.util.HashSet:add(java.lang.Object)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.parse.ParserNotFound:<init>(java.lang.String) (O)org.apache.nutch.parse.ParseException:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:getFetchTime(org.apache.nutch.parse.ParseData)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator$SelectorEntry:readFields(java.io.DataInput) (M)org.apache.hadoop.io.Text:readFields(java.io.DataInput)
M:org.apache.nutch.crawl.Injector$InjectReducer:configure(org.apache.hadoop.mapred.JobConf) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.parse.Outlink:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.crawl.Generator:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (O)java.util.Vector:<init>(int)
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseSegment:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat$1:write(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable) (M)java.io.PrintStream:println(java.lang.Object)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (M)org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean)
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (M)org.apache.nutch.protocol.Content:getUrl()
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:isErrorEnabled()
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.DeflateUtils:deflate(byte[]) (I)org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.ObjectWritable:get()
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput) (M)org.apache.hadoop.io.MapWritable:size()
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.Outlink:readFields(java.io.DataInput) (O)org.apache.hadoop.io.MapWritable:<init>()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParserFactory:findExtensions(java.lang.String) (M)org.apache.nutch.plugin.ExtensionPoint:getExtensions()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (I)org.apache.hadoop.conf.Configurable:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.Outlink:equals(java.lang.Object) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:close() (O)org.apache.nutch.crawl.CrawlDbReader:closeReaders()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (O)java.io.BufferedInputStream:<init>(java.io.InputStream)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endElement(java.lang.String,java.lang.String,java.lang.String) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map:keySet()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.apache.nutch.metadata.Metadata:getValues(java.lang.String)
M:org.apache.nutch.net.protocols.HttpDateFormat:<clinit>() (S)java.util.TimeZone:getTimeZone(java.lang.String)
M:org.apache.nutch.util.StringUtil:cleanField(java.lang.String) (M)java.lang.String:replaceAll(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.crawl.Injector:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.ObjectWritable:<init>()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.util.PrefixStringMatcher:<init>(java.util.Collection) (M)org.apache.nutch.util.PrefixStringMatcher:addPatternForward(java.lang.String)
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)org.apache.nutch.crawl.Inlink:getAnchor()
M:org.apache.nutch.crawl.Inlinks:size() (M)java.util.HashSet:size()
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (M)java.util.StringTokenizer:nextToken()
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Loops$Route,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Loops$Route:getLookingFor()
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (S)org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String)
M:org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String) (M)java.lang.String:substring(int)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (I)java.util.concurrent.Future:cancel(boolean)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader$2:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map) (O)java.lang.Thread:<init>()
M:org.apache.nutch.fetcher.OldFetcher:access$300(org.apache.nutch.fetcher.OldFetcher,int) (O)org.apache.nutch.fetcher.OldFetcher:updateStatus(int)
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.Inlinks:add(org.apache.nutch.crawl.Inlink)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getScore()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:progress()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.LongWritable:set(long)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:<init>() (O)org.apache.hadoop.mapred.MapReduceBase:<init>()
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)org.apache.nutch.crawl.Inlink:toString()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (O)java.io.FileInputStream:<init>(java.io.File)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:iterator()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.protocol.ProtocolOutput:getContent()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:getDependencies()
M:org.apache.nutch.tools.proxy.SegmentHandler:<clinit>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat$1:close(org.apache.hadoop.mapred.Reporter) (M)java.io.PrintStream:close()
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (O)java.util.Random:<init>()
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (M)org.apache.nutch.parse.ParsePluginList:getSupportedMimeTypes()
M:org.apache.nutch.parse.ParseResult:put(java.lang.String,org.apache.nutch.parse.ParseText,org.apache.nutch.parse.ParseData) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:<init>(java.io.Reader) (O)java.io.FilterReader:<init>(java.io.Reader)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.util.GenericWritableConfigurable:readFields(java.io.DataInput) (M)java.lang.Class:newInstance()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobClient:getFs()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.Outlink:skip(java.io.DataInput) (O)org.apache.hadoop.io.MapWritable:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setMapperClass(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.plugin.PluginDescriptor:addExportedLibRelative(java.lang.String) (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.parse.ParseSegment:<init>() (O)org.apache.nutch.parse.ParseSegment:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.plugin.PluginManifestParser:parseXML(java.net.URL) (M)java.net.URL:openStream()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:configure(org.apache.hadoop.mapred.JobConf) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.indexer.IndexingFilters:filter(org.apache.nutch.indexer.NutchDocument,org.apache.nutch.parse.Parse,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.Inlinks) (I)org.apache.nutch.indexer.IndexingFilter:filter(org.apache.nutch.indexer.NutchDocument,org.apache.nutch.parse.Parse,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.Inlinks)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (I)java.util.Map:clear()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDatum:setMetaData(org.apache.hadoop.io.MapWritable)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (M)java.net.URL:getProtocol()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String) (M)java.net.URI:toURL()
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (M)org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)
M:org.apache.nutch.crawl.Inlinks:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (I)java.util.Map:values()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.metadata.Metadata:write(java.io.DataOutput) (M)org.apache.nutch.metadata.Metadata:size()
M:org.apache.nutch.indexer.NutchField:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.arc.ArcRecordReader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.FileSplit) (M)org.apache.hadoop.mapred.FileSplit:getStart()
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginDescriptor:setProvidername(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.Inlinks:getAnchors() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.parse.ParsePluginList:getSupportedMimeTypes() (I)java.util.Set:toArray(java.lang.Object[])
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (I)org.apache.nutch.parse.Parser:getParse(org.apache.nutch.protocol.Content)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDatum:toString() (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.util.MimeUtil:forName(java.lang.String) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (M)java.net.URL:getProtocol()
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (I)org.apache.hadoop.conf.Configurable:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.util.Vector:addElement(java.lang.Object)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.tika.mime.MimeTypes:getDefaultMimeTypes()
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setLong(java.lang.String,long)
M:org.apache.nutch.crawl.Injector$InjectReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getDomainName(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbFilter:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (O)java.lang.Byte:<init>(byte)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.plugin.Extension:<init>(org.apache.nutch.plugin.PluginDescriptor,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.nutch.plugin.PluginRepository) (M)org.apache.nutch.plugin.Extension:setDescriptor(org.apache.nutch.plugin.PluginDescriptor)
M:org.apache.nutch.protocol.Content:equals(java.lang.Object) (M)org.apache.nutch.metadata.Metadata:equals(java.lang.Object)
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (M)org.apache.tika.mime.MimeType:getName()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:main(java.lang.String[]) (O)org.apache.nutch.scoring.webgraph.ScoreUpdater:<init>()
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.scoring.webgraph.NodeDumper$AggrType:<init>(java.lang.String,int) (O)java.lang.Enum:<init>(java.lang.String,int)
M:org.apache.nutch.crawl.CrawlDatum:toString() (O)java.util.Date:<init>(long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)java.util.Date:toString()
M:org.apache.nutch.crawl.Generator$SelectorEntry:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.HTMLMetaTags:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger:setConf(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (O)org.apache.nutch.crawl.TextProfileSignature$TokenComparator:<init>(org.apache.nutch.crawl.TextProfileSignature$1)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (I)org.xml.sax.Attributes:getValue(java.lang.String)
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.MapWritable$ClassIdEntry:<init>(org.apache.nutch.crawl.MapWritable,byte,java.lang.Class) (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$900(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (M)org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.lang.String) (S)org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (O)org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (O)java.util.Date:<init>(long)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (I)java.util.List:get(int)
M:org.apache.nutch.util.TimingUtil:elapsedTime(long,long) (O)java.lang.StringBuffer:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getNumReduceTasks()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (M)org.apache.commons.cli.HelpFormatter:printHelp(java.lang.String,org.apache.commons.cli.Options)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$600(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:<init>(java.lang.String,org.apache.nutch.scoring.webgraph.Node) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.scoring.ScoringFilters:passScoreBeforeParsing(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content)
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (I)org.w3c.dom.Document:getDocumentElement()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.net.URL) (M)java.lang.String:substring(int)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.Class:getSimpleName()
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Generator$Selector:<init>() (O)org.apache.nutch.crawl.URLPartitioner:<init>()
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (M)java.net.MalformedURLException:toString()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbMerger:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.crawl.CrawlDbMerger:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (M)org.apache.nutch.parse.ParsePluginList:getPluginList(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (M)java.io.File:isDirectory()
M:org.apache.nutch.util.ObjectCache:<init>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.parse.ParsePluginsReader:getAliases(org.w3c.dom.Element) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.TreeMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.util.StringUtil:rightPad(java.lang.String,int) (O)java.lang.StringBuffer:<init>(java.lang.String)
M:org.apache.nutch.protocol.Content:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.nutch.util.FSUtils:closeReaders(org.apache.hadoop.io.MapFile$Reader[])
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:<init>() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.Class:newInstance()
M:org.apache.nutch.util.URLUtil:getProtocol(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (O)java.lang.String:<init>(java.lang.String)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getContent(org.apache.hadoop.io.Text) (O)org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.indexer.NutchField:reset() (I)java.util.List:clear()
M:org.apache.nutch.crawl.Inlinks:write(java.io.DataOutput) (M)java.util.HashSet:size()
M:org.apache.nutch.plugin.PluginDescriptor:getExtensions() (M)java.util.ArrayList:toArray(java.lang.Object[])
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)org.apache.hadoop.util.GenericOptionsParser:getRemainingArgs()
M:org.apache.nutch.plugin.PluginManifestParser:parsePlugin(org.w3c.dom.Document,java.lang.String) (O)org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.CommandRunner:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (S)java.lang.System:arraycopy(java.lang.Object,int,java.lang.Object,int,int)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.NodeDumper$NameType:values() (M)org.apache.nutch.scoring.webgraph.NodeDumper$NameType[]:clone()
M:org.apache.nutch.fetcher.FetcherOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer)
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setScore(float)
M:org.apache.nutch.scoring.ScoringFilters:distributeScoreToOutlinks(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,java.util.Collection,org.apache.nutch.crawl.CrawlDatum,int) (I)org.apache.nutch.scoring.ScoringFilter:distributeScoreToOutlinks(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,java.util.Collection,org.apache.nutch.crawl.CrawlDatum,int)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.util.StringTokenizer:hasMoreTokens()
M:org.apache.nutch.util.EncodingDetector$EncodingClue:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (O)java.util.zip.Inflater:<init>(boolean)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem) (M)java.util.concurrent.atomic.AtomicInteger:incrementAndGet()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem() (I)java.util.Set:add(java.lang.Object)
M:org.apache.nutch.crawl.DefaultFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (O)org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (M)org.apache.nutch.protocol.ProtocolOutput:getContent()
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:openReaders(java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.HTMLMetaTags:<init>() (O)java.util.Properties:<init>()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readLong()
M:org.apache.nutch.crawl.DeduplicationJob:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (I)org.apache.hadoop.mapred.Partitioner:configure(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParsePluginList:getSupportedMimeTypes() (I)java.util.Map:keySet()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator$FG:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (M)java.util.Vector:addElement(java.lang.Object)
M:org.apache.nutch.util.PrefixStringMatcher:<init>(java.lang.String[]) (O)org.apache.nutch.util.TrieStringMatcher:<init>()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getStrings(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.ScoringFilterException:getMessage()
M:org.apache.nutch.crawl.Generator$GeneratorOutputFormat:<init>() (O)org.apache.hadoop.mapred.lib.MultipleSequenceFileOutputFormat:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)org.apache.nutch.crawl.CrawlDatum:getRetriesSinceFetch()
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (M)org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String)
M:org.apache.nutch.tools.proxy.DelayHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.String:valueOf(int)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.crawl.MapWritable$KeyValueEntry:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int) (M)java.util.zip.InflaterInputStream:read(byte[])
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:iterator()
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (M)java.io.ByteArrayOutputStream:toByteArray()
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (M)java.io.OutputStream:flush()
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Loops$Route,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.FloatWritable:<init>(float)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.parse.ParseData:getOutlinks()
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (M)org.apache.nutch.indexer.CleaningJob:getConf()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (O)java.util.HashSet:<init>()
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.ObjectWritable:<init>()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginManifestParser:parsePluginFolder(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)org.apache.hadoop.io.ArrayFile$Reader:close()
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.crawl.Generator$PartitionReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDbMerger:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getPluginDescriptor(java.lang.String) (I)java.util.Iterator:next()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues() (I)java.util.Iterator:next()
M:org.apache.nutch.util.PrefixStringMatcher:shortestMatch(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:isTerminal()
M:org.apache.nutch.segment.SegmentPart:get(java.lang.String) (M)java.lang.String:lastIndexOf(int)
M:org.apache.nutch.crawl.LinkDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.Inlinks,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.LongWritable:<init>(long)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkTimelimit() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:emptyQueues()
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (I)java.io.DataInput:readInt()
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexingFilters:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String)
M:org.apache.nutch.protocol.ProtocolStatus:readFields(java.io.DataInput) (I)java.io.DataInput:readByte()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseData:equals(java.lang.Object) (M)org.apache.nutch.metadata.Metadata:equals(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (M)org.apache.nutch.plugin.PluginDescriptor:getClassLoader()
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.PrefixStringMatcher:shortestMatch(java.lang.String) (M)org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.nutch.crawl.CrawlDb:getConf()
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.FloatWritable:set(float)
M:org.apache.nutch.indexer.CleaningJob:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.indexer.IndexingFilters:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.NutchWritable:<init>(org.apache.hadoop.io.Writable) (M)org.apache.nutch.crawl.NutchWritable:set(org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:<init>(org.apache.nutch.fetcher.OldFetcher,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readFully(byte[])
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:getNumMapTasks()
M:org.apache.nutch.indexer.IndexingFiltersChecker:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.tools.arc.ArcSegmentCreator:<clinit>() (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDatum:write(java.io.DataOutput) (I)java.io.DataOutput:writeLong(long)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (M)java.net.URL:getHost()
M:org.apache.nutch.parse.ParseStatus$EmptyParseImpl:<init>(org.apache.nutch.parse.ParseStatus,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)org.apache.nutch.util.ObjectCache:setObject(java.lang.String,java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (M)org.apache.hadoop.mapred.JobConf:setInt(java.lang.String,int)
M:org.apache.nutch.parse.ParserFactory:match(org.apache.nutch.plugin.Extension,java.lang.String,java.lang.String) (O)org.apache.nutch.parse.ParserFactory:escapeContentType(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Node:write(java.io.DataOutput) (M)org.apache.nutch.metadata.Metadata:write(java.io.DataOutput)
M:org.apache.nutch.crawl.CrawlDatum:setSignature(byte[]) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:<init>(org.apache.hadoop.conf.Configuration) (O)java.util.concurrent.atomic.AtomicLong:<init>(long)
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:<init>() (O)java.util.HashMap:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.CrawlDatum:set(org.apache.nutch.crawl.CrawlDatum) (O)org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable)
M:org.apache.nutch.parse.ParseData:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.util.DeflateUtils:inflate(byte[]) (O)java.io.ByteArrayInputStream:<init>(byte[])
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (M)org.mortbay.jetty.Server:start()
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (O)org.apache.nutch.protocol.ProtocolFactory:findExtension(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.Inlinks:clear() (M)java.util.HashSet:clear()
M:org.apache.nutch.plugin.PluginRepository:displayStatus() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.parse.ParseOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (I)org.slf4j.Logger:debug(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(int)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.parse.ParseSegment:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.List:size()
M:org.apache.nutch.parse.ParseStatus:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.segment.SegmentReader:createJobConf() (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.plugin.PluginRepository:getPluginDescriptor(java.lang.String) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.plugin.PluginRepository:getPluginCheckedDependencies(org.apache.nutch.plugin.PluginDescriptor,java.util.Map,java.util.Map,java.util.Map) (M)org.apache.nutch.plugin.PluginDescriptor:getPluginId()
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (M)com.ibm.icu.text.CharsetDetector:detectAll()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:close() (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.parse.Outlink:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (M)org.apache.hadoop.mapred.JobConf:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (I)java.util.Collection:iterator()
M:org.apache.nutch.tools.proxy.LogDebugHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)org.mortbay.jetty.Request:getMethod()
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher:<init>() (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.parse.ParseImpl:readFields(java.io.DataInput) (I)java.io.DataInput:readBoolean()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.indexer.IndexingFilters:filter(org.apache.nutch.indexer.NutchDocument,org.apache.nutch.parse.Parse,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.Inlinks)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.FreeGenerator$FG:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.HashMap:entrySet()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (I)org.apache.commons.cli.CommandLineParser:parse(org.apache.commons.cli.Options,java.lang.String[])
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)java.lang.StringBuilder:append(boolean)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuffer:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Counter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.LongWritable:<init>(long)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.String:isEmpty()
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:toString()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long) (M)org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int)
M:org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileStatus:isDir()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getQueueSize()
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.LinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.arc.ArcInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:setStatus(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (M)java.io.BufferedReader:readLine()
M:org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.indexer.NutchDocument:getFieldNames()
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:configure(org.apache.hadoop.mapred.JobConf) (O)java.lang.RuntimeException:<init>(java.lang.Throwable)
M:org.apache.nutch.scoring.ScoringFilterException:<init>(java.lang.String,java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:toString() (I)java.util.Set:iterator()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyComparatorClass(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.Fetcher:checkConfiguration() (O)java.util.ArrayList:<init>()
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.MapWritable:putAll(org.apache.nutch.crawl.MapWritable) (M)org.apache.nutch.crawl.MapWritable:get(org.apache.hadoop.io.Writable)
M:org.apache.nutch.util.PrefixStringMatcher:shortestMatch(java.lang.String) (M)java.lang.String:substring(int,int)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.util.URLUtil:getPage(java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.mapred.JobConf:setPartitionerClass(java.lang.Class)
M:org.apache.nutch.crawl.Generator:generateSegmentName() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getLong(java.lang.String,long)
M:org.apache.nutch.util.SuffixStringMatcher:shortestMatch(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (I)org.apache.hadoop.mapred.RunningJob:getCounters()
M:org.apache.nutch.util.DeflateUtils:inflate(byte[]) (M)java.io.ByteArrayOutputStream:close()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.lang.Integer:intValue()
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:handle(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,int) (M)org.mortbay.jetty.HttpConnection:getRequest()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.util.Vector:get(int)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:warn(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (M)org.apache.nutch.plugin.Extension:getDescriptor()
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.LongWritable:get()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:startsWith(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:toString() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.DmozParser$RDFProcessor:endDocument() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.segment.SegmentReader:createJobConf() (O)org.apache.nutch.util.NutchJob:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.util.TreeMap:entrySet()
M:org.apache.nutch.parse.ParseUtil:parse(org.apache.nutch.protocol.Content) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.Inlinks:getAnchors() (I)java.util.Iterator:next()
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.LinkDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String,java.lang.String,long)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginDescriptor:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.plugin.PluginDescriptor:setVersion(java.lang.String)
M:org.apache.nutch.parse.ParseStatus:write(java.io.DataOutput) (I)java.io.DataOutput:writeShort(int)
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)java.lang.StringBuilder:append(java.lang.Object)
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (I)java.util.List:add(java.lang.Object)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (M)org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)
M:org.apache.nutch.tools.proxy.SegmentHandler:<clinit>() (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:toString() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (O)org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (O)org.apache.nutch.crawl.TextProfileSignature:<init>()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:getOptionValue(java.lang.String)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (M)org.apache.nutch.util.PrefixStringMatcher:shortestMatch(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (O)org.apache.nutch.segment.SegmentReader$6:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (M)java.lang.Object:hashCode()
M:org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginRepository:getPluginDescriptor(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob$DeleterReducer:close() (M)org.apache.nutch.indexer.IndexWriters:commit()
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getFloat(java.lang.String,float)
M:org.apache.nutch.plugin.PluginRepository:shutDownActivatedPlugins() (I)java.util.Iterator:hasNext()
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.crawl.AbstractFetchSchedule:shouldFetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long) (M)org.apache.nutch.crawl.CrawlDatum:setFetchInterval(float)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.Class:getSimpleName()
M:org.apache.nutch.crawl.TextProfileSignature$Token:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (M)java.lang.String:matches(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.net.protocols.ProtocolException:<init>(java.lang.String,java.lang.Throwable) (O)java.lang.Exception:<init>(java.lang.String,java.lang.Throwable)
M:org.apache.nutch.parse.Outlink:<init>(java.lang.String,java.lang.String) (O)java.lang.Object:<init>()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (M)org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseResult:isSuccess() (M)org.apache.nutch.parse.ParseStatus:isSuccess()
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (I)java.util.Set:iterator()
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.parse.ParseResult:get(org.apache.hadoop.io.Text) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (M)java.lang.Object:toString()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)
M:org.apache.nutch.crawl.CrawlDbFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.net.URLFilters:filter(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReducer:<init>() (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:setStatus(int)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:setMapOutputValueClass(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.nutch.indexer.IndexWriters:open(org.apache.hadoop.mapred.JobConf,java.lang.String)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtension(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setMapOutputKeyClass(java.lang.Class)
M:org.apache.nutch.protocol.ProtocolStatus:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (O)org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.parse.ParseStatus:write(java.io.DataOutput) (I)java.io.DataOutput:writeByte(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (M)java.util.concurrent.atomic.AtomicLong:get()
M:org.apache.nutch.indexer.CleaningJob:main(java.lang.String[]) (O)org.apache.nutch.indexer.CleaningJob:<init>()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (M)org.apache.commons.cli.Options:addOption(org.apache.commons.cli.Option)
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Injector$InjectMapper:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)java.lang.Object:getClass()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.Object:getClass()
M:org.apache.nutch.util.TrieStringMatcher:addPatternForward(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:isInfoEnabled()
M:org.apache.nutch.tools.proxy.FakeHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:<init>(org.apache.hadoop.io.Text,java.net.URL,org.apache.nutch.crawl.CrawlDatum,java.lang.String) (O)org.apache.nutch.fetcher.Fetcher$FetchItem:<init>(org.apache.hadoop.io.Text,java.net.URL,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (S)java.lang.reflect.Modifier:isFinal(int)
M:org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String) (S)java.lang.System:arraycopy(java.lang.Object,int,java.lang.Object,int,int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.plugin.PluginRepository:getPluginInstance(org.apache.nutch.plugin.PluginDescriptor) (M)java.util.HashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (M)java.lang.Class:getFields()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:<init>(org.apache.hadoop.mapred.RecordReader,org.apache.nutch.fetcher.Fetcher$FetchItemQueues,int) (O)java.lang.Thread:<init>()
M:org.apache.nutch.parse.ParseData:<init>() (O)org.apache.nutch.metadata.Metadata:<init>()
M:org.apache.nutch.plugin.PluginRepository:getPluginDescriptors() (I)java.util.List:size()
M:org.apache.nutch.plugin.PluginManifestParser:parseLibraries(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (M)org.apache.nutch.plugin.PluginDescriptor:addNotExportedLibRelative(java.lang.String)
M:org.apache.nutch.net.URLNormalizer:<clinit>() (M)java.lang.Class:getName()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:setModifiedTime(long)
M:org.apache.nutch.parse.ParseResult:isSuccess() (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (M)java.lang.Object:getClass()
M:org.apache.nutch.crawl.AbstractFetchSchedule:<init>() (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (M)java.lang.Class:getMethod(java.lang.String,java.lang.Class[])
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:<init>() (O)org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseResult:get(java.lang.String) (M)org.apache.nutch.parse.ParseResult:get(org.apache.hadoop.io.Text)
M:org.apache.nutch.metadata.Metadata:setAll(java.util.Properties) (M)java.util.Properties:propertyNames()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (M)org.apache.nutch.util.NutchJob:setOutputFormat(java.lang.Class)
M:org.apache.nutch.indexer.IndexerOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Inlinks:getAnchors() (M)java.util.HashSet:iterator()
M:org.apache.nutch.segment.SegmentReader:list(java.util.List,java.io.Writer) (I)java.util.List:get(int)
M:org.apache.nutch.crawl.DeduplicationJob$StatusUpdateReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.apache.oro.text.regex.PatternCompiler:compile(java.lang.String,int)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkExceptionThreshold(java.lang.String) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:incrementExceptionCounter()
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexerMapReduce:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader$4:run() (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (O)org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParseResult:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (M)java.text.SimpleDateFormat:format(java.lang.Object)
M:org.apache.nutch.util.NodeWalker:nextNode() (M)java.util.Stack:pop()
M:org.apache.nutch.crawl.CrawlDatum:hashCode() (I)java.util.Set:hashCode()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)org.apache.nutch.metadata.Metadata:set(java.lang.String,java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.indexer.IndexerMapReduce:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.nutch.indexer.IndexerMapReduce:getConf()
M:org.apache.nutch.plugin.PluginDescriptor:getClassLoader() (M)java.lang.Class:getClassLoader()
M:org.apache.nutch.indexer.IndexerMapReduce:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.Text:set(java.lang.String)
M:org.apache.nutch.metadata.Metadata:get(java.lang.String) (I)java.util.Map:get(java.lang.Object)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (O)java.io.File:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.parse.ParseData:getStatus()
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputFormat(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (O)org.apache.nutch.indexer.NutchDocument:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:next()
M:org.apache.nutch.crawl.MapWritable$KeyValueEntry:toString() (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (M)java.util.concurrent.CyclicBarrier:await()
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getStatus()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:indexOf(java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (O)org.apache.nutch.indexer.NutchField:<init>()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.util.PrefixStringMatcher:<init>(java.util.Collection) (O)org.apache.nutch.util.TrieStringMatcher:<init>()
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseSegment:run(java.lang.String[]) (M)org.apache.nutch.parse.ParseSegment:getConf()
M:org.apache.nutch.indexer.NutchDocument:toString() (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.tools.FreeGenerator:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (O)javax.xml.transform.dom.DOMSource:<init>(org.w3c.dom.Node)
M:org.apache.nutch.plugin.PluginManifestParser:parseExtensionPoints(org.w3c.dom.Element,org.apache.nutch.plugin.PluginDescriptor) (I)org.w3c.dom.Element:getAttribute(java.lang.String)
M:org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String)
M:org.apache.nutch.parse.ParseStatus:write(java.io.DataOutput) (S)org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.tools.FreeGenerator:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (I)java.util.Map:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.protocol.RobotRulesParser:setConf(org.apache.hadoop.conf.Configuration) (M)java.util.StringTokenizer:nextToken()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.plugin.PluginManifestParser:parseManifestFile(java.lang.String) (M)java.io.File:getParent()
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (S)org.apache.nutch.util.NutchConfiguration:addNutchResources(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$Selector:<init>() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (M)org.apache.commons.cli.CommandLine:hasOption(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.metadata.MetaWrapper:set(org.apache.hadoop.io.Writable)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map$Entry:getValue()
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (M)org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (O)java.io.IOException:<init>(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:access$400(org.apache.nutch.fetcher.Fetcher,int) (O)org.apache.nutch.fetcher.Fetcher:updateStatus(int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (M)org.apache.nutch.scoring.ScoringFilters:distributeScoreToOutlinks(org.apache.hadoop.io.Text,org.apache.nutch.parse.ParseData,java.util.Collection,org.apache.nutch.crawl.CrawlDatum,int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)org.apache.nutch.parse.ParseOutputFormat$SimpleEntry:<init>(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$300(org.apache.nutch.fetcher.OldFetcher,int)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)java.lang.Float:parseFloat(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (M)java.util.WeakHashMap:put(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.crawl.AdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.nutch.indexer.NutchDocument:<init>()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (O)org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (M)org.apache.nutch.plugin.Extension:getDescriptor()
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (O)org.apache.commons.cli.HelpFormatter:<init>()
M:org.apache.nutch.segment.SegmentMergeFilters:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection) (I)org.apache.nutch.segment.SegmentMergeFilter:filter(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.parse.ParseData,org.apache.nutch.parse.ParseText,java.util.Collection)
M:org.apache.nutch.plugin.PluginRepository:installExtensionPoints(java.util.List) (I)java.util.List:iterator()
M:org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.net.URL) (M)org.apache.nutch.util.domain.DomainSuffix:toString()
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (S)org.apache.commons.lang.StringUtils:lowerCase(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Initializer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (O)java.lang.Object:<init>()
M:org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String) (M)java.lang.String:length()
M:org.apache.nutch.util.CommandRunner$PumperThread:<init>(org.apache.nutch.util.CommandRunner,java.lang.String,java.io.InputStream,java.io.OutputStream,boolean) (O)java.lang.Thread:<init>(java.lang.String)
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (O)org.apache.nutch.segment.SegmentReader$3:<init>(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.util.Map)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (O)org.apache.hadoop.fs.Path:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (M)org.apache.nutch.plugin.PluginRuntimeException:toString()
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.parse.ParseUtil:parseByExtensionId(java.lang.String,org.apache.nutch.protocol.Content) (O)org.apache.nutch.parse.ParseStatus:<init>(java.lang.Throwable)
M:org.apache.nutch.util.PrefixStringMatcher:main(java.lang.String[]) (M)org.apache.nutch.util.PrefixStringMatcher:longestMatch(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)java.util.List:toArray(java.lang.Object[])
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:run(java.lang.String[]) (M)org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:<init>(org.apache.hadoop.conf.Configuration) (O)java.lang.Object:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper$LinkNodes:readFields(java.io.DataInput) (M)org.apache.nutch.scoring.webgraph.LinkDumper$LinkNode:readFields(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (O)org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.protocol.ProtocolStatus:toString() (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasOptionalArg()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)org.slf4j.Logger:info(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (M)org.apache.nutch.crawl.CrawlDatum:setFetchTime(long)
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (I)org.apache.nutch.crawl.FetchSchedule:setConf(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.Extension:getAttribute(java.lang.String) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (M)java.lang.StringBuilder:append(long)
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (M)org.apache.tika.Tika:detect(byte[])
M:org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration) (O)org.apache.nutch.util.ObjectCache:<init>()
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.Loops:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.tools.proxy.NotFoundHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.util.SuffixStringMatcher:shortestMatch(java.lang.String) (M)java.lang.String:substring(int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (O)org.apache.hadoop.io.LongWritable:<init>()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (M)java.io.IOException:toString()
M:org.apache.nutch.crawl.CrawlDatum:toString() (M)org.apache.nutch.crawl.CrawlDatum:getSignature()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (O)org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setBoolean(java.lang.String,boolean)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.metadata.HttpHeaders:<clinit>() (O)org.apache.hadoop.io.Text:<init>(java.lang.String)
M:org.apache.nutch.parse.ParseText:read(java.io.DataInput) (O)org.apache.nutch.parse.ParseText:<init>()
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (M)java.lang.String:substring(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)java.lang.StringBuilder:append(int)
M:org.apache.nutch.protocol.ProtocolStatus:toString() (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (O)org.apache.nutch.crawl.CrawlDatum:<init>()
M:org.apache.nutch.scoring.webgraph.LinkDumper:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.mapred.Counters$Counter:increment(long)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.protocol.ProtocolException:<init>(java.lang.String) (O)java.lang.Exception:<init>(java.lang.String)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.crawl.Inlinks:toString() (M)java.lang.StringBuffer:toString()
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.protocol.ProtocolOutput:<init>(org.apache.nutch.protocol.Content) (O)java.lang.Object:<init>()
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (M)org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.util.ArrayList:get(int)
M:org.apache.nutch.net.URLFilterChecker:checkAll() (M)java.io.PrintStream:print(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem() (I)java.util.List:remove(int)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (M)org.apache.hadoop.util.GenericOptionsParser:getRemainingArgs()
M:org.apache.nutch.segment.SegmentReader$2:run() (S)org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (M)java.io.PrintStream:println(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$010(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (M)org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (I)java.util.Map:size()
M:org.apache.nutch.parse.Outlink:toString() (I)java.util.Iterator:next()
M:org.apache.nutch.segment.SegmentReader:<init>(org.apache.hadoop.conf.Configuration,boolean,boolean,boolean,boolean,boolean,boolean) (O)java.text.SimpleDateFormat:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.DefaultFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (M)org.apache.nutch.crawl.CrawlDatum:setModifiedTime(long)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:error(org.xml.sax.SAXParseException) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (I)java.io.DataOutput:writeFloat(float)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (O)org.apache.commons.cli.Options:<init>()
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (M)org.apache.hadoop.mapred.JobConf:setMapperClass(java.lang.Class)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (M)java.util.WeakHashMap:get(java.lang.Object)
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (M)org.apache.nutch.crawl.CrawlDatum:getFetchInterval()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (I)java.io.DataInput:readBoolean()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.Node:setInlinkScore(float)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.fetcher.Fetcher$InputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int) (O)org.apache.hadoop.mapred.FileSplit:<init>(org.apache.hadoop.fs.Path,long,long,java.lang.String[])
M:org.apache.nutch.crawl.Generator$CrawlDbUpdater:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.OutputCollector:collect(java.lang.Object,java.lang.Object)
M:org.apache.nutch.crawl.Generator$SelectorInverseMapper:<init>() (O)org.apache.hadoop.mapred.MapReduceBase:<init>()
M:org.apache.nutch.crawl.CrawlDbMerger:<init>(org.apache.hadoop.conf.Configuration) (O)org.apache.hadoop.conf.Configured:<init>()
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)org.apache.nutch.parse.Parse:getData()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (M)org.apache.nutch.segment.SegmentReader:getConf()
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNMapper:configure(org.apache.hadoop.mapred.JobConf) (M)org.apache.hadoop.mapred.JobConf:getLong(java.lang.String,long)
M:org.apache.nutch.parse.ParseUtil:runParser(org.apache.nutch.parse.Parser,org.apache.nutch.protocol.Content) (M)org.apache.nutch.protocol.Content:getUrl()
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)java.lang.StringBuilder:toString()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:<init>(org.apache.nutch.fetcher.Fetcher,org.apache.hadoop.conf.Configuration) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (O)org.apache.commons.cli.GnuParser:<init>()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setOutputValueClass(java.lang.Class)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (M)org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.crawl.DefaultFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (O)org.apache.nutch.crawl.AbstractFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (I)org.apache.hadoop.mapred.Reporter:incrCounter(java.lang.String,java.lang.String,long)
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getMetaData()
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (M)java.lang.Object:equals(java.lang.Object)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)java.util.Iterator:next()
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (O)org.apache.nutch.util.EncodingDetector:<init>(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (O)java.io.BufferedReader:<init>(java.io.Reader)
M:org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput)
M:org.apache.nutch.parse.ParseData:read(java.io.DataInput) (M)org.apache.nutch.parse.ParseData:readFields(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.indexer.IndexerMapReduce:configure(org.apache.hadoop.mapred.JobConf) (O)org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
M:org.apache.nutch.segment.SegmentReader:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.parse.ParseData:toString()
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.crawl.CrawlDatum:getFetchTime()
M:org.apache.nutch.indexer.IndexingFilters:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (M)org.apache.hadoop.mapred.JobConf:setInputFormat(java.lang.Class)
M:org.apache.nutch.crawl.Inlinks:getAnchors() (O)java.util.ArrayList:<init>()
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (O)java.net.URL:<init>(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.StringBuilder:append(java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (M)org.apache.nutch.indexer.IndexingFilters:filter(org.apache.nutch.indexer.NutchDocument,org.apache.nutch.parse.Parse,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.crawl.Inlinks)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (M)java.io.File:isDirectory()
M:org.apache.nutch.scoring.webgraph.Node:readFields(java.io.DataInput) (M)org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (M)org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (O)java.lang.RuntimeException:<init>(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops$Finalizer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Iterator:hasNext()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.scoring.webgraph.LinkDatum:getTimestamp()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:getFetchItem() (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getInProgressSize()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (M)java.util.HashMap:get(java.lang.Object)
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (M)org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String)
M:org.apache.nutch.indexer.NutchDocument:add(java.lang.String,java.lang.Object) (M)org.apache.nutch.indexer.NutchField:add(java.lang.Object)
M:org.apache.nutch.crawl.SignatureComparator:compare(java.lang.Object,java.lang.Object) (S)org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object)
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (M)java.util.Locale:toString()
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (M)org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
M:org.apache.nutch.crawl.Inlinks:write(java.io.DataOutput) (I)java.io.DataOutput:writeInt(int)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Map:keySet()
M:org.apache.nutch.parse.ParserFactory:matchExtensions(java.util.List,org.apache.nutch.plugin.Extension[],java.lang.String) (I)java.util.List:add(int,java.lang.Object)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)org.apache.nutch.fetcher.Fetcher$FetchItemQueues:dump()
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getHost(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (M)org.apache.hadoop.mapred.JobConf:setJobName(java.lang.String)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (I)org.slf4j.Logger:error(java.lang.String)
M:org.apache.nutch.indexer.IndexerMapReduce:normalizeUrl(java.lang.String) (M)java.lang.String:trim()
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.slf4j.Logger:isWarnEnabled()
M:org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean) (M)org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (M)java.lang.String:equals(java.lang.Object)
M:org.apache.nutch.plugin.PluginRepository:filter(java.util.regex.Pattern,java.util.regex.Pattern,java.util.Map) (O)java.util.HashMap:<init>()
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (M)org.apache.hadoop.mapred.JobConf:setOutputKeyClass(java.lang.Class)
M:org.apache.nutch.parse.ParseStatus:<init>(int,int,java.lang.String) (O)org.apache.nutch.parse.ParseStatus:<init>(int,int,java.lang.String[])
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.fetcher.FetcherOutputFormat:<init>() (O)java.lang.Object:<init>()
M:org.apache.nutch.protocol.ProtocolStatus:<clinit>() (O)java.lang.Integer:<init>(int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (M)java.util.concurrent.atomic.AtomicLong:set(long)
M:org.apache.nutch.crawl.CrawlDb:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.EncodingDetector:<init>(org.apache.hadoop.conf.Configuration) (O)java.util.ArrayList:<init>()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (O)java.lang.StringBuilder:<init>()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)java.util.Set:iterator()
M:org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String) (M)org.apache.nutch.util.ObjectCache:getObject(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (I)org.apache.hadoop.mapred.Reporter:getCounter(java.lang.String,java.lang.String)
