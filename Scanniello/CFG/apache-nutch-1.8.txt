M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.metadata.MetaWrapper) (S)org.apache.nutch.segment.SegmentPart:parse(java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (S)org.apache.nutch.util.URLUtil:toASCII(java.lang.String)
M:org.apache.nutch.util.StringUtil:toHexString(byte[]) (S)org.apache.nutch.util.StringUtil:toHexString(byte[],java.lang.String,int)
M:org.apache.nutch.plugin.PluginDescriptor:collectLibs(java.util.ArrayList,org.apache.nutch.plugin.PluginDescriptor) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (S)javax.xml.parsers.DocumentBuilderFactory:newInstance()
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.MapWritable:findEntryByKey(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.Injector:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$500(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.MapWritable:<clinit>() (S)org.apache.nutch.crawl.MapWritable:addToMap(java.lang.Class,java.lang.Byte)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getOutputCompressionType(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.indexer.IndexingJob:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.LinkDbMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Math:min(int,int)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (S)java.lang.Character:isLetter(char)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.Generator:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.metadata.Metadata:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)java.lang.Math:min(int,int)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.LinkRank$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.DeduplicationJob:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.OldFetcher:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParseSegment:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getPage(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.Math:min(int,int)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.segment.SegmentReader:get(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text,java.io.Writer,java.util.Map) (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.crawl.LinkDb:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.String:valueOf(long)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean)
M:org.apache.nutch.metadata.SpellCheckedMetadata:get(java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.util.URLUtil:getHostSegments(java.lang.String) (S)org.apache.nutch.util.URLUtil:getHostSegments(java.net.URL)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (S)org.apache.nutch.fetcher.OldFetcher:access$400(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (S)com.google.common.io.Files:toByteArray(java.io.File)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader$TextOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Injector:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)java.util.regex.Pattern:compile(java.lang.String)
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$602(org.apache.nutch.crawl.MapWritable$KeyValueEntry,byte)
M:org.apache.nutch.parse.ParseText:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.CommandRunner:exec() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDbReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.LinkDbMerger:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)java.lang.Class:forName(java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content)
M:org.apache.nutch.fetcher.FetcherOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.WebGraph:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:access$200(org.apache.nutch.parse.ParseOutputFormat)
M:org.apache.nutch.fetcher.Fetcher:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:generateSegmentName() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.AbstractFetchSchedule:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.fetcher.OldFetcher:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.CrawlDatum:<clinit>() (S)org.apache.hadoop.io.WritableComparator:define(java.lang.Class,org.apache.hadoop.io.WritableComparator)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (S)org.apache.nutch.fetcher.Fetcher:isStoringContent(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (S)org.apache.tika.config.TikaConfig:getDefaultConfig()
M:org.apache.nutch.crawl.LinkDbReader:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.util.ObjectCache:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.tools.DmozParser:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkRank:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcRecordReader:getProgress() (S)java.lang.Math:min(float,float)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (S)java.lang.reflect.Array:newInstance(java.lang.Class,int)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.Outlink:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.tools.Benchmark:run(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum:hashCode() (S)java.lang.Float:floatToIntBits(float)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Float:toString(float)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:access$000(org.apache.nutch.parse.ParseOutputFormat)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:access$300()
M:org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String) (S)java.lang.Character:toLowerCase(char)
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (S)java.net.InetAddress:getByName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.DeduplicationJob:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.plugin.PluginClassLoader:equals(java.lang.Object) (S)java.util.Arrays:equals(java.lang.Object[],java.lang.Object[])
M:org.apache.nutch.segment.SegmentReader$3:run() (S)org.apache.nutch.segment.SegmentReader:access$100(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.fetcher.OldFetcher:access$800(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.fetcher.OldFetcher:access$700(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.plugin.PluginRepository:<init>(org.apache.hadoop.conf.Configuration) (S)java.util.regex.Pattern:compile(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (S)org.apache.nutch.crawl.Generator:generateSegmentName()
M:org.apache.nutch.net.URLFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.Math:floor(double)
M:org.apache.nutch.util.URLUtil:<clinit>() (S)java.util.regex.Pattern:compile(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.plugin.PluginDescriptor:getResourceString(java.lang.String,java.util.Locale) (S)java.util.ResourceBundle:getBundle(java.lang.String,java.util.Locale,java.lang.ClassLoader)
M:org.apache.nutch.parse.ParseStatus:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:run(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseStatus:getEmptyParseResult(java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.parse.ParseResult:createParseResult(java.lang.String,org.apache.nutch.parse.Parse)
M:org.apache.nutch.tools.FreeGenerator:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasFetchStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginList:getSupportedMimeTypes() (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$600(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)java.net.InetAddress:getByName(java.lang.String)
M:org.apache.nutch.protocol.Content:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.URLPartitioner:getPartition(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,int) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.tools.arc.ArcRecordReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbMerger:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.Long:toString(long)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.parse.Outlink:skip(java.io.DataInput) (S)org.apache.hadoop.io.Text:skip(java.io.DataInput)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.net.protocols.HttpDateFormat:<clinit>() (S)java.util.TimeZone:getTimeZone(java.lang.String)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)org.apache.nutch.util.URLUtil:toASCII(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.Injector:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.net.URLNormalizers:findExtensions(java.lang.String) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.tools.ResolveUrls:access$000()
M:org.apache.nutch.parse.ParsePluginsReader:parse(org.apache.hadoop.conf.Configuration) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:configure(org.apache.hadoop.mapred.JobConf) (S)java.util.regex.Pattern:compile(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (S)org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (S)org.apache.nutch.util.EncodingDetector$EncodingClue:access$100(org.apache.nutch.util.EncodingDetector$EncodingClue)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:dump() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasOptionalArg()
M:org.apache.nutch.crawl.MapWritable:<init>(org.apache.nutch.crawl.MapWritable) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDb:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.MapWritable:addIdEntry(byte,java.lang.Class) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$502(org.apache.nutch.crawl.MapWritable$ClassIdEntry,org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.parse.ParseOutputFormat:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.MimeUtil:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (S)org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean)
M:org.apache.nutch.scoring.webgraph.NodeDumper$AggrType:valueOf(java.lang.String) (S)java.lang.Enum:valueOf(java.lang.Class,java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.plugin.PluginManifestParser:<clinit>() (S)java.lang.System:getProperty(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.util.EncodingDetector:addClue(java.lang.String,java.lang.String,int) (S)org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (S)org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.protocol.ProtocolStatus:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:configure(org.apache.hadoop.mapred.JobConf) (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.net.URLFilterChecker:checkOne(java.lang.String) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexingFiltersChecker:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.metadata.SpellCheckedMetadata:getValues(java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.parse.ParserChecker:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$400(org.apache.nutch.fetcher.Fetcher,int)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)java.util.Collections:sort(java.util.List,java.util.Comparator)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.util.DomUtil:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.Generator$HashComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable) (S)org.apache.nutch.crawl.Generator$HashComparator:hash(byte[],int,int)
M:org.apache.nutch.indexer.IndexerMapReduce:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:getFetchTime(org.apache.nutch.parse.ParseData) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MapWritable:hashCode() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getOutputCompressionType(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.tools.ResolveUrls:access$100()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.Character:isUpperCase(char)
M:org.apache.nutch.crawl.Generator$Selector:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.tools.ResolveUrls:resolveUrls() (S)java.util.concurrent.Executors:newFixedThreadPool(int)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:checkTimelimit() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.FreeGenerator:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:logError(org.apache.hadoop.io.Text,java.lang.Throwable) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.indexer.IndexingJob:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.fetcher.OldFetcher:<init>(org.apache.hadoop.conf.Configuration) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.fetcher.OldFetcher:isParsing(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$200(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.MapWritable:keySet() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (S)java.lang.reflect.Modifier:isStatic(int)
M:org.apache.nutch.crawl.CrawlDatum:toString() (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.protocol.ProtocolFactory:getProtocol(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.tika.mime.MimeTypesFactory:create(java.io.InputStream)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$102(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:access$100(org.apache.nutch.parse.ParseOutputFormat)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object) (S)org.apache.nutch.crawl.SignatureComparator:_compare(byte[],int,int,byte[],int,int)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDatum$Comparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.CrawlDatum$Comparator:readInt(byte[],int)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.DmozParser:addTopicsFromFile(java.lang.String,java.util.Vector) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.URLUtil:main(java.lang.String[]) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.CrawlDbMerger:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.MapWritable:values() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDatum:toString() (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.protocol.ProtocolFactory:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.tika.mime.MimeTypes:getDefaultMimeTypes()
M:org.apache.nutch.indexer.IndexingFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (S)org.apache.nutch.fetcher.OldFetcher:access$508(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.util.URLUtil:toUNICODE(java.lang.String) (S)java.net.IDN:toUnicode(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$400(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.indexer.NutchField:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.URLUtil:getDomainName(java.lang.String) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers)
M:org.apache.nutch.tools.proxy.TestbedProxy:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.NutchConfiguration:setUUID(org.apache.hadoop.conf.Configuration) (S)java.util.UUID:randomUUID()
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getDomainName(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (S)org.apache.hadoop.io.MD5Hash:digest(java.lang.String)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDatum:getStatusName(byte) (S)java.lang.Byte:valueOf(byte)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.NodeDumper$NameType:valueOf(java.lang.String) (S)java.lang.Enum:valueOf(java.lang.Class,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$702(org.apache.nutch.crawl.MapWritable$KeyValueEntry,byte)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:setConf(org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.plugin.PluginManifestParser:parseXML(java.net.URL) (S)javax.xml.parsers.DocumentBuilderFactory:newInstance()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.Inlink:skip(java.io.DataInput) (S)org.apache.hadoop.io.Text:skip(java.io.DataInput)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph$InlinkDb:configure(org.apache.hadoop.mapred.JobConf) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getReaders(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.MapWritable:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.protocol.RobotRulesParser:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.tools.FreeGenerator:access$000()
M:org.apache.nutch.indexer.CleaningJob:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Long:toString(long)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$900(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentPart:get(org.apache.hadoop.mapred.FileSplit) (S)org.apache.nutch.segment.SegmentPart:get(java.lang.String)
M:org.apache.nutch.tools.Benchmark:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$100(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.util.TimingUtil:elapsedTime(long,long) (S)java.text.NumberFormat:getInstance()
M:org.apache.nutch.tools.arc.ArcRecordReader:createValue() (S)org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.lang.String) (S)org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$600(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.parse.OutlinkExtractor:getOutlinks(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[]) (S)org.apache.nutch.util.DeflateUtils:inflateBestEffort(byte[],int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:finishFetchItem(org.apache.nutch.fetcher.Fetcher$FetchItem,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDbReducer:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.plugin.PluginManifestParser:getPluginFolder(java.lang.String) (S)java.net.URLDecoder:decode(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.CrawlDatum$Comparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.SignatureComparator:_compare(byte[],int,int,byte[],int,int)
M:org.apache.nutch.crawl.CrawlDbReader:openReaders(java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArgs(int)
M:org.apache.nutch.crawl.AbstractFetchSchedule:forceRefetch(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$102(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.nutch.util.FSUtils:closeReaders(org.apache.hadoop.io.MapFile$Reader[])
M:org.apache.nutch.tools.proxy.SegmentHandler:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.scoring.webgraph.Loops$Looper:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (S)org.apache.nutch.parse.Outlink:read(java.io.DataInput)
M:org.apache.nutch.metadata.SpellCheckedMetadata:remove(java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.lang.String) (S)org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.net.URL)
M:org.apache.nutch.crawl.Inlink:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.net.URLNormalizers:getURLNormalizers(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.parse.ParserChecker:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (S)java.lang.System:arraycopy(java.lang.Object,int,java.lang.Object,int,int)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getHost(java.lang.String)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$700(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.fetcher.OldFetcher:access$600(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.fetcher.OldFetcher:access$900(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.crawl.LinkDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.util.CommandRunner:exec() (S)java.lang.Thread:interrupted()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasDbStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.parse.Outlink:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:getFetchItem() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDbMerger:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:read(char[],int,int) (S)org.apache.xerces.util.XMLChar:isValid(int)
M:org.apache.nutch.crawl.CrawlDbReader:openReaders(java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Injector$InjectMapper:configure(org.apache.hadoop.mapred.JobConf) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Generator:generateSegmentName() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.Loops$Looper:map(org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMergeFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.DeduplicationJob:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.NodeDumper:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.URLUtil:getProtocol(java.lang.String) (S)org.apache.nutch.util.URLUtil:getProtocol(java.net.URL)
M:org.apache.nutch.fetcher.Fetcher:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.fetcher.Fetcher:isStoringContent(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDatum:setFetchInterval(float) (S)java.lang.Math:round(float)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.indexer.CleaningJob:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$300(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.util.StringUtil:fromHexString(java.lang.String)
M:org.apache.nutch.util.URLUtil:resolveURL(java.net.URL,java.lang.String) (S)org.apache.nutch.util.URLUtil:fixPureQueryTargets(java.net.URL,java.lang.String)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.protocol.ProtocolStatus:toString() (S)java.lang.String:valueOf(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MapWritable:containsValue(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.tools.arc.ArcSegmentCreator:generateSegmentName()
M:org.apache.nutch.protocol.Content:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.proxy.DelayHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.String:valueOf(int)
M:org.apache.nutch.segment.SegmentMerger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.segment.SegmentPart:parse(java.lang.String)
M:org.apache.nutch.indexer.IndexingFiltersChecker:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.NodeDumper$DumpType:valueOf(java.lang.String) (S)java.lang.Enum:valueOf(java.lang.Class,java.lang.String)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$500(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$202(org.apache.nutch.fetcher.OldFetcher,long)
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.plugin.PluginRepository:main(java.lang.String[]) (S)java.lang.Class:forName(java.lang.String,boolean,java.lang.ClassLoader)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)org.apache.hadoop.io.MD5Hash:digest(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.segment.SegmentReader$6:run() (S)org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.Benchmark:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.util.NutchConfiguration:create() (S)org.apache.nutch.util.NutchConfiguration:addNutchResources(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMergeFilters:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.Fetcher:reportStatus(int,int) (S)java.lang.Math:round(float)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getDomainName(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:getInlinks(org.apache.hadoop.io.Text) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration) (S)java.lang.Class:forName(java.lang.String)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseUtil:<init>(org.apache.hadoop.conf.Configuration) (S)java.util.concurrent.Executors:newCachedThreadPool(java.util.concurrent.ThreadFactory)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (S)org.apache.nutch.net.protocols.HttpDateFormat:toString(java.util.Date)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.FreeGenerator$FG:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDbMerger:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (S)java.util.Collections:emptyList()
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReducer:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (S)java.lang.Math:round(float)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.CrawlDbMerger$Merger:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.OldFetcher:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (S)javax.xml.parsers.SAXParserFactory:newInstance()
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.util.DomUtil:saveDom(java.io.OutputStream,org.w3c.dom.Element) (S)javax.xml.transform.TransformerFactory:newInstance()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentMerger:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.indexer.CleaningJob:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.parse.ParseData:equals(java.lang.Object) (S)java.util.Arrays:equals(java.lang.Object[],java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.Loops:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.MimeUtil:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParsePluginsReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.protocol.Content:equals(java.lang.Object) (S)java.util.Arrays:equals(byte[],byte[])
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$000(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.parse.ParseText:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.indexer.IndexingFiltersChecker:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.NodeDumper:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.LinkDb:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$700(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$800(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)java.lang.Integer:valueOf(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Sorter:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParseStatus:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.crawl.LinkDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.plugin.PluginRepository:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$300(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (S)java.net.InetAddress:getByName(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)java.lang.Character:toLowerCase(char)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.nutch.util.FSUtils:closeReaders(org.apache.hadoop.io.MapFile$Reader[])
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (S)java.util.Collections:emptyList()
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:setFetchSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,long,long,long,long,int) (S)java.lang.Math:round(double)
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.parse.ParserFactory:getExtensions(java.lang.String) (S)org.apache.nutch.util.MimeUtil:cleanMimeType(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (S)java.nio.charset.Charset:isSupported(java.lang.String)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.net.URLNormalizerChecker:checkOne(java.lang.String,java.lang.String) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.segment.SegmentPart:get(org.apache.hadoop.mapred.FileSplit)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.CrawlDatum$Comparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.CrawlDatum$Comparator:readLong(byte[],int)
M:org.apache.nutch.scoring.webgraph.Loops$Route:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)java.util.UUID:randomUUID()
M:org.apache.nutch.crawl.URLPartitioner:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.Math:floor(double)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (S)org.apache.nutch.util.StringUtil:isEmpty(java.lang.String)
M:org.apache.nutch.protocol.Content:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.crawl.CrawlDatum:compareTo(org.apache.nutch.crawl.CrawlDatum) (S)org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$400(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChild(char) (S)java.util.Arrays:sort(java.lang.Object[])
M:org.apache.nutch.parse.ParserFactory:getParserById(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexingJob:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.Benchmark:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Reader:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserFactory:getExtensions(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (S)org.apache.nutch.util.CommandRunner:access$002(org.apache.nutch.util.CommandRunner,java.lang.Throwable)
M:org.apache.nutch.crawl.MapWritable:equals(java.lang.Object) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.nutch.crawl.Generator:generateSegmentName()
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDbFilter:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.parse.ParseStatus:toString() (S)java.lang.String:valueOf(java.lang.Object)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasDbStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration) (S)java.lang.Class:forName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getReaders(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.URLUtil:isSameDomainName(java.lang.String,java.lang.String) (S)org.apache.nutch.util.URLUtil:isSameDomainName(java.net.URL,java.net.URL)
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL) (S)org.apache.nutch.util.domain.DomainSuffixes:getInstance()
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseData:readFields(java.io.DataInput) (S)org.apache.nutch.parse.ParseStatus:read(java.io.DataInput)
M:org.apache.nutch.segment.SegmentReader:getSeqRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getReaders(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.Generator:generateSegmentName() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.crawl.CrawlDatum:<clinit>() (S)java.lang.Byte:valueOf(byte)
M:org.apache.nutch.crawl.CrawlDbReader:get(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.parse.OutlinkExtractor:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDb:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (S)java.util.Collections:synchronizedSet(java.util.Set)
M:org.apache.nutch.crawl.MapWritable:toString() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.protocol.ProtocolStatus:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getDomainName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexingJob:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.CrawlDbReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.metadata.Metadata:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.protocol.Content:readFieldsCompressed(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (S)org.apache.commons.lang.StringUtils:isNotBlank(java.lang.String)
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.segment.SegmentReader$5:run() (S)org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.parse.ParseText:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.LinkDbMerger:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.Injector:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasOptionalArg()
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.Generator$HashComparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.Generator$HashComparator:hash(byte[],int,int)
M:org.apache.nutch.crawl.LinkDb:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (S)java.lang.Byte:valueOf(byte)
M:org.apache.nutch.parse.ParseSegment:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.util.URLUtil:getHost(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.MapWritable:hashCode() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.util.CommandRunner:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.tools.arc.ArcRecordReader:createKey() (S)org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher:<init>() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.util.EncodingDetector:autoDetectClues(org.apache.nutch.protocol.Content,boolean) (S)org.apache.nutch.util.EncodingDetector:parseCharacterEncoding(java.lang.String)
M:org.apache.nutch.util.StringUtil:fromHexString(java.lang.String) (S)org.apache.nutch.util.StringUtil:charToNibble(char)
M:org.apache.nutch.util.NutchConfiguration:create() (S)org.apache.nutch.util.NutchConfiguration:setUUID(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:getClass(byte) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$400(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.tools.proxy.SegmentHandler:<clinit>() (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.crawl.CrawlDbReader:processDumpJob(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.Integer) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.indexer.IndexingFiltersChecker:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.net.URLNormalizers:getExtensions(java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.util.EncodingDetector:guessEncoding(org.apache.nutch.protocol.Content,java.lang.String) (S)org.apache.nutch.util.EncodingDetector$EncodingClue:access$200(org.apache.nutch.util.EncodingDetector$EncodingClue)
M:org.apache.nutch.parse.ParserFactory:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.DmozParser$RDFProcessor:startElement(java.lang.String,java.lang.String,java.lang.String,org.xml.sax.Attributes) (S)java.lang.Math:abs(int)
M:org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$002(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.hadoop.io.Writable)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (S)org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (S)org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content)
M:org.apache.nutch.util.URLUtil:toASCII(java.lang.String) (S)java.net.IDN:toASCII(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:reportStatus() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:init(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[]) (S)org.apache.nutch.util.GZIPUtils:unzipBestEffort(byte[],int)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.LinkDbMerger:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getPage(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDbDumpMapper:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.tools.ResolveUrls:access$300()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.DeflateUtils:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.ProtocolStatus:getName() (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.crawl.MapWritable:getKeyValueEntry(byte,byte) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.fetcher.Fetcher:<init>(org.apache.hadoop.conf.Configuration) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)org.apache.nutch.parse.ParseOutputFormat:filterNormalize(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.nutch.net.URLFilters,org.apache.nutch.net.URLNormalizers)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.tools.proxy.SegmentHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.Character:isLetter(char)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$QueueFeeder:run() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.tools.proxy.TestbedProxy:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (S)org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)
M:org.apache.nutch.parse.ParserFactory:getParsers(java.lang.String,java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbMerger:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.crawl.Generator:partitionSegment(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (S)org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (S)java.lang.reflect.Modifier:isFinal(int)
M:org.apache.nutch.scoring.webgraph.LinkDatum:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)org.apache.nutch.util.StringUtil:toHexString(byte[])
M:org.apache.nutch.scoring.webgraph.Loops$Route:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.CrawlDatum:<init>() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.metadata.Metadata:add(java.lang.String,java.lang.String) (S)java.lang.System:arraycopy(java.lang.Object,int,java.lang.Object,int,int)
M:org.apache.nutch.tools.proxy.DelayHandler:handle(org.mortbay.jetty.Request,javax.servlet.http.HttpServletResponse,java.lang.String,int) (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.scoring.ScoringFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.fetcher.Fetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.scoring.webgraph.LinkDumper:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withArgName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.parse.ParseSegment:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.crawl.CrawlDatum:equals(java.lang.Object) (S)org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.crawl.MapWritable:createInternalIdClassEntries() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.NodeDumper:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.fetcher.OldFetcher:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.fetcher.OldFetcher:isStoringContent(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.metadata.SpellCheckedMetadata:<clinit>() (S)java.lang.reflect.Modifier:isPublic(int)
M:org.apache.nutch.indexer.NutchDocument:readFields(java.io.DataInput) (S)org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput)
M:org.apache.nutch.parse.ParseUtil:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:containsValue(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:writeOutAsDuplicate(org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.DeduplicationJob:access$000()
M:org.apache.nutch.parse.HtmlParseFilters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.LinkDbMerger:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:handleRedirect(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,java.lang.String,boolean,java.lang.String) (S)org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean)
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (S)org.apache.nutch.util.NutchConfiguration:setUUID(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.WebGraph$OutlinkDb:getFetchTime(org.apache.nutch.parse.ParseData) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:findEntryByKey(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.HadoopFSUtil:getPassAllFilter()
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (S)org.apache.nutch.net.protocols.HttpDateFormat:toString(long)
M:org.apache.nutch.util.URLUtil:chooseRepr(java.lang.String,java.lang.String,boolean) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.crawl.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$102(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.parse.ParseResult:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.ResolveUrls$ResolverThread:run() (S)org.apache.nutch.tools.ResolveUrls:access$200()
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.nutch.crawl.Generator:generateSegmentName()
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)java.lang.Long:toString(long)
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.metadata.SpellCheckedMetadata:set(java.lang.String,java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.parse.ParseSegment:map(org.apache.hadoop.io.WritableComparable,org.apache.nutch.protocol.Content,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.parse.ParseSegment:isTruncated(org.apache.nutch.protocol.Content)
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.SignatureFactory:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureSequenceFile(java.lang.String,java.lang.String) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.CrawlDatum$Comparator:compare(byte[],int,int,byte[],int,int) (S)org.apache.nutch.crawl.CrawlDatum$Comparator:readFloat(byte[],int)
M:org.apache.nutch.tools.ResolveUrls:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.indexer.NutchDocument:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.metadata.SpellCheckedMetadata:add(java.lang.String,java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.crawl.CrawlDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.tools.FreeGenerator:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.crawl.CrawlDatum:readFields(java.io.DataInput) (S)java.lang.Math:round(float)
M:org.apache.nutch.scoring.webgraph.NodeDumper:dumpNodes(org.apache.hadoop.fs.Path,org.apache.nutch.scoring.webgraph.NodeDumper$DumpType,long,org.apache.hadoop.fs.Path,boolean,org.apache.nutch.scoring.webgraph.NodeDumper$NameType,org.apache.nutch.scoring.webgraph.NodeDumper$AggrType,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.crawl.MapWritable:keySet() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$200(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)java.lang.String:valueOf(float)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentReader:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (S)org.apache.nutch.util.EncodingDetector$EncodingClue:access$300(org.apache.nutch.util.EncodingDetector$EncodingClue)
M:org.apache.nutch.util.URLUtil:getTopLevelDomainName(java.net.URL) (S)org.apache.nutch.util.URLUtil:getDomainSuffix(java.net.URL)
M:org.apache.nutch.indexer.IndexingFiltersChecker:run(java.lang.String[]) (S)java.lang.Math:min(int,int)
M:org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String) (S)org.apache.nutch.metadata.SpellCheckedMetadata:normalize(java.lang.String)
M:org.apache.nutch.parse.ParseStatus:write(java.io.DataOutput) (S)org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])
M:org.apache.nutch.tools.FreeGenerator:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.parse.ParseOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getOutputCompressionType(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,long) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher:access$300(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$400(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.segment.SegmentReader$4:run() (S)org.apache.nutch.segment.SegmentReader:access$100(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.util.NutchConfiguration:create(boolean,java.util.Properties) (S)org.apache.nutch.util.NutchConfiguration:addNutchResources(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Generator$Selector:<init>() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.plugin.PluginDescriptor:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.nutch.util.FSUtils:replace(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:generateSegmentName() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.scoring.webgraph.LinkRank$Initializer:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.segment.SegmentReader:main(java.lang.String[]) (S)org.apache.nutch.segment.SegmentReader:usage()
M:org.apache.nutch.metadata.SpellCheckedMetadata:getNormalizedName(java.lang.String) (S)org.apache.commons.lang.StringUtils:getLevenshteinDistance(java.lang.String,java.lang.String)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.net.URLNormalizers:<init>(org.apache.hadoop.conf.Configuration,java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexerMapReduce:initMRJob(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Collection,org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParsePluginsReader:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPaths(org.apache.hadoop.fs.FileStatus[])
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.parse.ParserChecker:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.indexer.IndexingJob:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.Injector$InjectMapper:map(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Text,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Float:parseFloat(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$300(org.apache.nutch.fetcher.OldFetcher,int)
M:org.apache.nutch.crawl.AbstractFetchSchedule:initializeSchedule(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)java.lang.Float:parseFloat(java.lang.String)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.AdaptiveFetchSchedule:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.indexer.IndexerMapReduce:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasFetchStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.tools.Benchmark:getDate() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.tools.DmozParser:parseDmozFile(java.io.File,int,boolean,int,java.util.regex.Pattern) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.CommandRunner$PumperThread:run() (S)org.apache.nutch.util.CommandRunner:access$100(org.apache.nutch.util.CommandRunner)
M:org.apache.nutch.scoring.webgraph.LinkDumper:dumpLinks(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:createSegments(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LinkDumper:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.segment.SegmentReader:<init>(org.apache.hadoop.conf.Configuration,boolean,boolean,boolean,boolean,boolean,boolean) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.tools.Benchmark:benchmark(int,int,int,int,long,boolean,java.lang.String) (S)org.apache.nutch.fetcher.Fetcher:isParsing(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.IndexWriters:<init>(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.Generator:generate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,long,long,boolean,boolean,boolean,int) (S)org.apache.nutch.util.LockUtil:createLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
M:org.apache.nutch.indexer.CleaningJob:run(java.lang.String[]) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:readMimeFile(java.io.Reader) (S)org.apache.commons.lang.StringUtils:lowerCase(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.fetcher.FetcherOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.Generator:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDb:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$500(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)java.lang.Long:toString(long)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String) (S)org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String,int)
M:org.apache.nutch.protocol.ProtocolFactory:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.Benchmark:<clinit>() (S)org.apache.commons.logging.LogFactory:getLog(java.lang.Class)
M:org.apache.nutch.crawl.MapWritable:getClassId(java.lang.Class) (S)org.apache.nutch.crawl.MapWritable$ClassIdEntry:access$300(org.apache.nutch.crawl.MapWritable$ClassIdEntry)
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbMerger:run(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.CleaningJob:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.nutch.crawl.CrawlDb:install(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.net.protocols.HttpDateFormat:main(java.lang.String[]) (S)org.apache.nutch.net.protocols.HttpDateFormat:toLong(java.lang.String)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArgs()
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.nutch.util.FSUtils:closeReaders(org.apache.hadoop.io.MapFile$Reader[])
M:org.apache.nutch.scoring.webgraph.NodeReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasOptionalArg()
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (S)org.apache.nutch.tools.arc.ArcRecordReader:isMagic(byte[])
M:org.apache.nutch.net.URLNormalizerChecker:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.segment.SegmentMerger$ObjectInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.protocol.ProtocolStatus:<init>(int,java.lang.Object,long) (S)java.lang.String:valueOf(java.lang.Object)
M:org.apache.nutch.crawl.MapWritable:values() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.Loops:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.parse.ParserChecker:run(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.util.EncodingDetector$EncodingClue:meetsThreshold() (S)org.apache.nutch.util.EncodingDetector:access$000(org.apache.nutch.util.EncodingDetector)
M:org.apache.nutch.net.URLFilterChecker:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueues:addFetchItem(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String)
M:org.apache.nutch.parse.ParseData:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getReaders(java.lang.String) (S)java.util.Arrays:sort(java.lang.Object[])
M:org.apache.nutch.tools.DmozParser:main(java.lang.String[]) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReader:processTopNJob(java.lang.String,long,float,java.lang.String,org.apache.hadoop.conf.Configuration) (S)java.lang.Math:round(double)
M:org.apache.nutch.crawl.DeduplicationJob$DBFilter:map(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.DeduplicationJob:access$000()
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LoopReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.CrawlDb:run(java.lang.String[]) (S)org.apache.nutch.util.HadoopFSUtil:getPassDirectoriesFilter(org.apache.hadoop.fs.FileSystem)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInitializer(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.arc.ArcSegmentCreator:output(org.apache.hadoop.mapred.OutputCollector,java.lang.String,org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int) (S)org.apache.nutch.crawl.SignatureFactory:getSignature(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String) (S)java.nio.charset.Charset:forName(java.lang.String)
M:org.apache.nutch.scoring.webgraph.NodeReader:dumpUrl(org.apache.hadoop.fs.Path,java.lang.String) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.tools.arc.ArcRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.BytesWritable) (S)java.lang.System:arraycopy(java.lang.Object,int,java.lang.Object,int,int)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Merger:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.tools.proxy.AbstractTestbedHandler:handle(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,int) (S)org.mortbay.jetty.HttpConnection:getCurrentConnection()
M:org.apache.nutch.plugin.PluginClassLoader:hashCode() (S)java.util.Arrays:hashCode(java.lang.Object[])
M:org.apache.nutch.scoring.webgraph.LinkDumper:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.tools.DmozParser$XMLCharFilter:read() (S)org.apache.xerces.util.XMLChar:isValid(int)
M:org.apache.nutch.parse.ParseOutputFormat$1:write(org.apache.hadoop.io.Text,org.apache.nutch.parse.Parse) (S)java.lang.Integer:valueOf(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.crawl.LinkDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.util.MimeUtil:autoResolveContentType(java.lang.String,java.lang.String,byte[]) (S)org.apache.nutch.util.MimeUtil:cleanMimeType(java.lang.String)
M:org.apache.nutch.util.URLUtil:isSameDomainName(java.net.URL,java.net.URL) (S)org.apache.nutch.util.URLUtil:getDomainName(java.net.URL)
M:org.apache.nutch.fetcher.Fetcher:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.fetcher.Fetcher:isParsing(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)java.lang.Boolean:valueOf(boolean)
M:org.apache.nutch.fetcher.Fetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.util.URLUtil:getHost(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.parse.ParserChecker:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDb:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean,boolean) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.indexer.IndexWriters:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.segment.SegmentReader$2:run() (S)org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$010(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.scoring.webgraph.Loops:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.scoring.webgraph.Loops:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.Generator:run(java.lang.String[]) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.protocol.ProtocolStatus:write(java.io.DataOutput) (S)org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)java.lang.Math:round(float)
M:org.apache.nutch.scoring.webgraph.LinkRank:runInverter(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.crawl.MapWritable:get(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.NodeDumper:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.CrawlDb:createJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)java.lang.Integer:toString(int)
M:org.apache.nutch.segment.SegmentReader:dump(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.util.EncodingDetector:findDisagreements(java.lang.String,java.util.List) (S)org.apache.nutch.util.EncodingDetector$EncodingClue:access$100(org.apache.nutch.util.EncodingDetector$EncodingClue)
M:org.apache.nutch.crawl.DeduplicationJob:run(java.lang.String[]) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.parse.ParseData:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.scoring.webgraph.LinkRank:analyze(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String,java.lang.String) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Inlinks:readFields(java.io.DataInput) (S)org.apache.nutch.crawl.Inlink:read(java.io.DataInput)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.crawl.TextProfileSignature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)java.lang.Character:isLetterOrDigit(char)
M:org.apache.nutch.crawl.CrawlDbReader:main(java.lang.String[]) (S)java.lang.Integer:valueOf(int)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)java.lang.Float:toString(float)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:create(java.lang.String)
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbMerger:createMergeJob(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,boolean) (S)org.apache.hadoop.mapred.FileOutputFormat:setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.parse.ParseSegment:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.tools.proxy.LogDebugHandler:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.indexer.CleaningJob:delete(java.lang.String,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.TextProfileSignature:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.fetcher.OldFetcher:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph:createWebGraph(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.MapWritable:remove(org.apache.hadoop.io.Writable) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$100(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.tools.FreeGenerator:run(java.lang.String[]) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.LoopReader:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.fetcher.FetcherOutputFormat$1:<init>(org.apache.nutch.fetcher.FetcherOutputFormat,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable,java.lang.String,org.apache.hadoop.io.MapFile$Writer) (S)org.apache.nutch.fetcher.Fetcher:isParsing(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.CrawlDbReducer:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.CrawlDatum:hasFetchStatus(org.apache.nutch.crawl.CrawlDatum)
M:org.apache.nutch.crawl.Injector:inject(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.segment.SegmentReader:getStats(org.apache.hadoop.fs.Path,org.apache.nutch.segment.SegmentReader$SegmentReaderStats) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MimeAdaptiveFetchSchedule:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.FetcherOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable) (S)org.apache.hadoop.mapred.SequenceFileOutputFormat:getOutputCompressionType(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration) (S)org.apache.nutch.util.NutchConfiguration:getUUID(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.scoring.webgraph.LinkDatum:<init>(java.lang.String) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.Injector:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.parse.ParserFactory:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)org.apache.nutch.util.LockUtil:removeLockFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
M:org.apache.nutch.tools.Benchmark$BenchmarkResults:addTiming(java.lang.String,java.lang.String,long) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.indexer.IndexingFilters:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.plugin.PluginRepository:getOrderedPlugins(java.lang.Class,java.lang.String,java.lang.String) (S)org.apache.nutch.util.ObjectCache:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:hashCode() (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$000(org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.indexer.IndexingJob:index(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.List,boolean,boolean,java.lang.String,boolean,boolean) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.scoring.webgraph.LinkRank:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArg()
M:org.apache.nutch.crawl.Generator$Selector:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.nutch.crawl.FetchScheduleFactory:getFetchSchedule(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.DeduplicationJob:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$100(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.segment.SegmentMerger$SegmentOutputFormat$1:ensureMapFile(java.lang.String,java.lang.String,java.lang.Class) (S)org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.segment.SegmentReader:getMapRecords(org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.util.CommandRunner:exec() (S)java.lang.Runtime:getRuntime()
M:org.apache.nutch.scoring.webgraph.Loops:findLoops(org.apache.hadoop.fs.Path) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.crawl.CrawlDbReader$CrawlDatumCsvOutputFormat$LineRecordWriter:write(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum) (S)org.apache.nutch.crawl.CrawlDatum:getStatusName(byte)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:output(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,org.apache.nutch.protocol.Content,org.apache.nutch.protocol.ProtocolStatus,int,int) (S)org.apache.nutch.fetcher.Fetcher:access$300(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.util.GZIPUtils:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.LinkDumper$Inverter:reduce(org.apache.hadoop.io.Text,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.plugin.Extension:getExtensionInstance() (S)org.apache.nutch.plugin.PluginRepository:get(org.apache.hadoop.conf.Configuration)
M:org.apache.nutch.crawl.MapWritable:readFields(java.io.DataInput) (S)org.apache.nutch.crawl.MapWritable$KeyValueEntry:access$102(org.apache.nutch.crawl.MapWritable$KeyValueEntry,org.apache.nutch.crawl.MapWritable$KeyValueEntry)
M:org.apache.nutch.fetcher.OldFetcher:<init>() (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.SignatureComparator:compare(java.lang.Object,java.lang.Object) (S)org.apache.nutch.crawl.SignatureComparator:_compare(java.lang.Object,java.lang.Object)
M:org.apache.nutch.protocol.RobotRulesParser:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runAnalysis(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,int,float) (S)java.lang.String:valueOf(int)
M:org.apache.nutch.segment.SegmentReader$1:run() (S)org.apache.nutch.segment.SegmentReader:access$000(org.apache.nutch.segment.SegmentReader,org.apache.hadoop.fs.Path,org.apache.hadoop.io.Text)
M:org.apache.nutch.fetcher.OldFetcher:fetch(org.apache.hadoop.fs.Path,int) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.tools.proxy.SegmentHandler$Segment:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Writable) (S)org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
M:org.apache.nutch.scoring.webgraph.LinkRank$Analyzer:configure(org.apache.hadoop.mapred.JobConf) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.scoring.webgraph.NodeDumper$Dumper:map(org.apache.hadoop.io.Text,org.apache.nutch.scoring.webgraph.Node,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.util.URLUtil:getHost(java.lang.String)
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)org.apache.nutch.util.TimingUtil:elapsedTime(long,long)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.tools.ResolveUrls:main(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:withDescription(java.lang.String)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)java.lang.Integer:valueOf(java.lang.String)
M:org.apache.nutch.fetcher.OldFetcher:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.crawl.CrawlDbReader:processStatJob(java.lang.String,org.apache.hadoop.conf.Configuration,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.crawl.LinkDb:invert(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean,boolean) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.fetcher.Fetcher:main(java.lang.String[]) (S)org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])
M:org.apache.nutch.scoring.webgraph.WebGraph:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.CommandRunner:exec() (S)java.lang.Thread:sleep(long)
M:org.apache.nutch.util.TrieStringMatcher$TrieNode:getChildAddIfNotPresent(char,boolean) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.tools.proxy.SegmentHandler:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.util.StringUtil:main(java.lang.String[]) (S)org.apache.nutch.util.EncodingDetector:resolveEncodingAlias(java.lang.String)
M:org.apache.nutch.crawl.FetchScheduleFactory:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:logError(org.apache.hadoop.io.Text,java.lang.String) (S)org.apache.nutch.fetcher.Fetcher:access$500(org.apache.nutch.fetcher.Fetcher)
M:org.apache.nutch.scoring.webgraph.LinkDatum:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.segment.SegmentMerger:main(java.lang.String[]) (S)java.lang.Long:parseLong(java.lang.String)
M:org.apache.nutch.scoring.webgraph.ScoreUpdater:update(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.FileInputFormat:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
M:org.apache.nutch.crawl.Inlink:readFields(java.io.DataInput) (S)org.apache.hadoop.io.Text:readString(java.io.DataInput)
M:org.apache.nutch.crawl.LinkDb:run(java.lang.String[]) (S)java.util.Arrays:asList(java.lang.Object[])
M:org.apache.nutch.parse.ParseSegment:parse(org.apache.hadoop.fs.Path) (S)java.lang.System:currentTimeMillis()
M:org.apache.nutch.util.EncodingDetector:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.crawl.CrawlDb:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.DeduplicationJob$DedupReducer:reduce(org.apache.hadoop.io.BytesWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.nutch.crawl.DeduplicationJob:access$000()
M:org.apache.nutch.fetcher.Fetcher$FetchItemQueue:<init>(org.apache.hadoop.conf.Configuration,int,long,long) (S)java.util.Collections:synchronizedList(java.util.List)
M:org.apache.nutch.indexer.NutchField:readFields(java.io.DataInput) (S)java.lang.Float:valueOf(float)
M:org.apache.nutch.scoring.webgraph.Loops$LoopSet:write(java.io.DataOutput) (S)org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)
M:org.apache.nutch.util.URLUtil:getDomainName(java.net.URL) (S)org.apache.nutch.util.domain.DomainSuffixes:getInstance()
M:org.apache.nutch.scoring.webgraph.LinkRank:runCounter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.scoring.webgraph.Loops:main(java.lang.String[]) (S)org.apache.nutch.util.NutchConfiguration:create()
M:org.apache.nutch.crawl.LinkDbReader:processDumpJob(java.lang.String,java.lang.String) (S)java.lang.Long:valueOf(long)
M:org.apache.nutch.net.URLNormalizers:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.scoring.webgraph.WebGraph:run(java.lang.String[]) (S)org.apache.commons.cli.OptionBuilder:hasArgs()
M:org.apache.nutch.parse.ParseText:main(java.lang.String[]) (S)java.lang.Integer:parseInt(java.lang.String)
M:org.apache.nutch.crawl.Generator$Selector:reduce(org.apache.hadoop.io.FloatWritable,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter) (S)org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)
M:org.apache.nutch.crawl.CrawlDbFilter:<clinit>() (S)org.slf4j.LoggerFactory:getLogger(java.lang.Class)
M:org.apache.nutch.fetcher.Fetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.Fetcher$FetchItem:create(org.apache.hadoop.io.Text,org.apache.nutch.crawl.CrawlDatum,java.lang.String)
M:org.apache.nutch.crawl.Generator:main(java.lang.String[]) (S)java.lang.System:exit(int)
M:org.apache.nutch.fetcher.OldFetcher$FetcherThread:run() (S)org.apache.nutch.fetcher.OldFetcher:access$008(org.apache.nutch.fetcher.OldFetcher)
M:org.apache.nutch.crawl.LinkDbMerger:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[],boolean,boolean) (S)org.apache.hadoop.mapred.JobClient:runJob(org.apache.hadoop.mapred.JobConf)
M:org.apache.nutch.crawl.MD5Signature:calculate(org.apache.nutch.protocol.Content,org.apache.nutch.parse.Parse) (S)org.apache.hadoop.io.MD5Hash:digest(byte[])
